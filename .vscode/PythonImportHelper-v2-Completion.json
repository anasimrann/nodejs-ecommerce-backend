[
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "platform",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "platform",
        "description": "platform",
        "detail": "platform",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "Annotator",
        "importPath": "ultralytics.utils.plotting",
        "description": "ultralytics.utils.plotting",
        "isExtraImport": true,
        "detail": "ultralytics.utils.plotting",
        "documentation": {}
    },
    {
        "label": "Annotator",
        "importPath": "ultralytics.utils.plotting",
        "description": "ultralytics.utils.plotting",
        "isExtraImport": true,
        "detail": "ultralytics.utils.plotting",
        "documentation": {}
    },
    {
        "label": "colors",
        "importPath": "ultralytics.utils.plotting",
        "description": "ultralytics.utils.plotting",
        "isExtraImport": true,
        "detail": "ultralytics.utils.plotting",
        "documentation": {}
    },
    {
        "label": "save_one_box",
        "importPath": "ultralytics.utils.plotting",
        "description": "ultralytics.utils.plotting",
        "isExtraImport": true,
        "detail": "ultralytics.utils.plotting",
        "documentation": {}
    },
    {
        "label": "Annotator",
        "importPath": "ultralytics.utils.plotting",
        "description": "ultralytics.utils.plotting",
        "isExtraImport": true,
        "detail": "ultralytics.utils.plotting",
        "documentation": {}
    },
    {
        "label": "colors",
        "importPath": "ultralytics.utils.plotting",
        "description": "ultralytics.utils.plotting",
        "isExtraImport": true,
        "detail": "ultralytics.utils.plotting",
        "documentation": {}
    },
    {
        "label": "save_one_box",
        "importPath": "ultralytics.utils.plotting",
        "description": "ultralytics.utils.plotting",
        "isExtraImport": true,
        "detail": "ultralytics.utils.plotting",
        "documentation": {}
    },
    {
        "label": "Annotator",
        "importPath": "ultralytics.utils.plotting",
        "description": "ultralytics.utils.plotting",
        "isExtraImport": true,
        "detail": "ultralytics.utils.plotting",
        "documentation": {}
    },
    {
        "label": "colors",
        "importPath": "ultralytics.utils.plotting",
        "description": "ultralytics.utils.plotting",
        "isExtraImport": true,
        "detail": "ultralytics.utils.plotting",
        "documentation": {}
    },
    {
        "label": "Annotator",
        "importPath": "ultralytics.utils.plotting",
        "description": "ultralytics.utils.plotting",
        "isExtraImport": true,
        "detail": "ultralytics.utils.plotting",
        "documentation": {}
    },
    {
        "label": "Annotator",
        "importPath": "ultralytics.utils.plotting",
        "description": "ultralytics.utils.plotting",
        "isExtraImport": true,
        "detail": "ultralytics.utils.plotting",
        "documentation": {}
    },
    {
        "label": "colors",
        "importPath": "ultralytics.utils.plotting",
        "description": "ultralytics.utils.plotting",
        "isExtraImport": true,
        "detail": "ultralytics.utils.plotting",
        "documentation": {}
    },
    {
        "label": "save_one_box",
        "importPath": "ultralytics.utils.plotting",
        "description": "ultralytics.utils.plotting",
        "isExtraImport": true,
        "detail": "ultralytics.utils.plotting",
        "documentation": {}
    },
    {
        "label": "DetectMultiBackend",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "DetectMultiBackend",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "C3",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "SPP",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "SPPF",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "Bottleneck",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "BottleneckCSP",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "C3x",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "Concat",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "Conv",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "CrossConv",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "DWConv",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "DWConvTranspose2d",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "Focus",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "autopad",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "DetectMultiBackend",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "DetectMultiBackend",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "DetectMultiBackend",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "DetectMultiBackend",
        "importPath": "models.common",
        "description": "models.common",
        "isExtraImport": true,
        "detail": "models.common",
        "documentation": {}
    },
    {
        "label": "classify_transforms",
        "importPath": "utils.augmentations",
        "description": "utils.augmentations",
        "isExtraImport": true,
        "detail": "utils.augmentations",
        "documentation": {}
    },
    {
        "label": "Albumentations",
        "importPath": "utils.augmentations",
        "description": "utils.augmentations",
        "isExtraImport": true,
        "detail": "utils.augmentations",
        "documentation": {}
    },
    {
        "label": "augment_hsv",
        "importPath": "utils.augmentations",
        "description": "utils.augmentations",
        "isExtraImport": true,
        "detail": "utils.augmentations",
        "documentation": {}
    },
    {
        "label": "classify_albumentations",
        "importPath": "utils.augmentations",
        "description": "utils.augmentations",
        "isExtraImport": true,
        "detail": "utils.augmentations",
        "documentation": {}
    },
    {
        "label": "classify_transforms",
        "importPath": "utils.augmentations",
        "description": "utils.augmentations",
        "isExtraImport": true,
        "detail": "utils.augmentations",
        "documentation": {}
    },
    {
        "label": "copy_paste",
        "importPath": "utils.augmentations",
        "description": "utils.augmentations",
        "isExtraImport": true,
        "detail": "utils.augmentations",
        "documentation": {}
    },
    {
        "label": "letterbox",
        "importPath": "utils.augmentations",
        "description": "utils.augmentations",
        "isExtraImport": true,
        "detail": "utils.augmentations",
        "documentation": {}
    },
    {
        "label": "mixup",
        "importPath": "utils.augmentations",
        "description": "utils.augmentations",
        "isExtraImport": true,
        "detail": "utils.augmentations",
        "documentation": {}
    },
    {
        "label": "random_perspective",
        "importPath": "utils.augmentations",
        "description": "utils.augmentations",
        "isExtraImport": true,
        "detail": "utils.augmentations",
        "documentation": {}
    },
    {
        "label": "IMG_FORMATS",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "VID_FORMATS",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "LoadImages",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "LoadScreenshots",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "LoadStreams",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "create_classification_dataloader",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "create_classification_dataloader",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "exif_transpose",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "letterbox",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "IMG_FORMATS",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "VID_FORMATS",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "LoadImages",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "LoadScreenshots",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "LoadStreams",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "IMG_FORMATS",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "VID_FORMATS",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "LoadImages",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "LoadScreenshots",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "LoadStreams",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "LoadImages",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "create_dataloader",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "create_dataloader",
        "importPath": "utils.dataloaders",
        "description": "utils.dataloaders",
        "isExtraImport": true,
        "detail": "utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "Profile",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_file",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_img_size",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_imshow",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_requirements",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "cv2",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "increment_path",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "print_args",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "strip_optimizer",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "DATASETS_DIR",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "TQDM_BAR_FORMAT",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "WorkingDirectory",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_git_info",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_git_status",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_requirements",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "download",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "increment_path",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "init_seeds",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "print_args",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "yaml_save",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "TQDM_BAR_FORMAT",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "Profile",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_img_size",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_requirements",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "increment_path",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "print_args",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "Profile",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_requirements",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_suffix",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_version",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "increment_path",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "is_jupyter",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "make_divisible",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "non_max_suppression",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "scale_boxes",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "xywh2xyxy",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "xyxy2xywh",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "yaml_load",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "make_divisible",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "print_args",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_version",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_yaml",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "make_divisible",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "print_args",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "Profile",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_file",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_img_size",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_imshow",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_requirements",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "cv2",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "increment_path",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "non_max_suppression",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "print_args",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "scale_boxes",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "scale_segments",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "strip_optimizer",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "TQDM_BAR_FORMAT",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_amp",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_dataset",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_file",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_git_info",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_git_status",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_img_size",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_requirements",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_suffix",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_yaml",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "get_latest_run",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "increment_path",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "init_seeds",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "intersect_dicts",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "labels_to_class_weights",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "labels_to_image_weights",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "one_cycle",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "print_args",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "print_mutation",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "strip_optimizer",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "yaml_save",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "NUM_THREADS",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "TQDM_BAR_FORMAT",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "Profile",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_dataset",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_img_size",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_requirements",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_yaml",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "coco80_to_coco91_class",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "increment_path",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "non_max_suppression",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "print_args",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "scale_boxes",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "xywh2xyxy",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "xyxy2xywh",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "increment_path",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_version",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "resample_segments",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "segment2box",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "xywhn2xyxy",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "TQDM_BAR_FORMAT",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "DATASETS_DIR",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "NUM_THREADS",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "TQDM_BAR_FORMAT",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_dataset",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_requirements",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_yaml",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "clean_str",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "cv2",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "is_colab",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "is_kaggle",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "segments2boxes",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "unzip_file",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "xyn2xy",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "xywh2xyxy",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "xywhn2xyxy",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "xyxy2xywhn",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "clip_boxes",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "increment_path",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "xywh2xyxy",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "xyxy2xywh",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_version",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "file_date",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "git_describe",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_yaml",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "file_size",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "print_args",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "Profile",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_file",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_img_size",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_imshow",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_requirements",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "cv2",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "increment_path",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "non_max_suppression",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "print_args",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "scale_boxes",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "strip_optimizer",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "xyxy2xywh",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "Profile",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_dataset",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_img_size",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_requirements",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_version",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_yaml",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "file_size",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "get_default_args",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "print_args",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "url2file",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "yaml_save",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "TQDM_BAR_FORMAT",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_amp",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_dataset",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_file",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_git_info",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_git_status",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_img_size",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_requirements",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_suffix",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_yaml",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "get_latest_run",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "increment_path",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "init_seeds",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "intersect_dicts",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "labels_to_class_weights",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "labels_to_image_weights",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "methods",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "one_cycle",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "print_args",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "print_mutation",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "strip_optimizer",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "yaml_save",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "TQDM_BAR_FORMAT",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "Profile",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_dataset",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_img_size",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_requirements",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "check_yaml",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "coco80_to_coco91_class",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "increment_path",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "non_max_suppression",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "print_args",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "scale_boxes",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "xywh2xyxy",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "xyxy2xywh",
        "importPath": "utils.general",
        "description": "utils.general",
        "isExtraImport": true,
        "detail": "utils.general",
        "documentation": {}
    },
    {
        "label": "select_device",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_inference_mode",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "ModelEMA",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "de_parallel",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "model_info",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "reshape_classifier_output",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "select_device",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_DDP",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_optimizer",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smartCrossEntropyLoss",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "torch_distributed_zero_first",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "select_device",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_inference_mode",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "copy_attr",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_inference_mode",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "fuse_conv_and_bn",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "initialize_weights",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "model_info",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "profile",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "scale_img",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "select_device",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "time_sync",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "select_device",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_inference_mode",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "EarlyStopping",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "ModelEMA",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "de_parallel",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "select_device",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_DDP",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_optimizer",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_resume",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "torch_distributed_zero_first",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "de_parallel",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "select_device",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_inference_mode",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "select_device",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "profile",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "torch_distributed_zero_first",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "de_parallel",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "select_device",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "select_device",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_inference_mode",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "select_device",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_inference_mode",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "EarlyStopping",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "ModelEMA",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "de_parallel",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "select_device",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_DDP",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_optimizer",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_resume",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "torch_distributed_zero_first",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "select_device",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_inference_mode",
        "importPath": "utils.torch_utils",
        "description": "utils.torch_utils",
        "isExtraImport": true,
        "detail": "utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_output",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "copy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "copy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "torch.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "torch.hub",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.hub",
        "description": "torch.hub",
        "detail": "torch.hub",
        "documentation": {}
    },
    {
        "label": "torch.optim.lr_scheduler",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.optim.lr_scheduler",
        "description": "torch.optim.lr_scheduler",
        "detail": "torch.optim.lr_scheduler",
        "documentation": {}
    },
    {
        "label": "torchvision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision",
        "description": "torchvision",
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "amp",
        "importPath": "torch.cuda",
        "description": "torch.cuda",
        "isExtraImport": true,
        "detail": "torch.cuda",
        "documentation": {}
    },
    {
        "label": "amp",
        "importPath": "torch.cuda",
        "description": "torch.cuda",
        "isExtraImport": true,
        "detail": "torch.cuda",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "val",
        "importPath": "classify",
        "description": "classify",
        "isExtraImport": true,
        "detail": "classify",
        "documentation": {}
    },
    {
        "label": "attempt_load",
        "importPath": "models.experimental",
        "description": "models.experimental",
        "isExtraImport": true,
        "detail": "models.experimental",
        "documentation": {}
    },
    {
        "label": "MixConv2d",
        "importPath": "models.experimental",
        "description": "models.experimental",
        "isExtraImport": true,
        "detail": "models.experimental",
        "documentation": {}
    },
    {
        "label": "attempt_load",
        "importPath": "models.experimental",
        "description": "models.experimental",
        "isExtraImport": true,
        "detail": "models.experimental",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "models.experimental",
        "description": "models.experimental",
        "isExtraImport": true,
        "detail": "models.experimental",
        "documentation": {}
    },
    {
        "label": "attempt_load",
        "importPath": "models.experimental",
        "description": "models.experimental",
        "isExtraImport": true,
        "detail": "models.experimental",
        "documentation": {}
    },
    {
        "label": "attempt_load",
        "importPath": "models.experimental",
        "description": "models.experimental",
        "isExtraImport": true,
        "detail": "models.experimental",
        "documentation": {}
    },
    {
        "label": "attempt_load",
        "importPath": "models.experimental",
        "description": "models.experimental",
        "isExtraImport": true,
        "detail": "models.experimental",
        "documentation": {}
    },
    {
        "label": "attempt_load",
        "importPath": "models.experimental",
        "description": "models.experimental",
        "isExtraImport": true,
        "detail": "models.experimental",
        "documentation": {}
    },
    {
        "label": "attempt_load",
        "importPath": "models.experimental",
        "description": "models.experimental",
        "isExtraImport": true,
        "detail": "models.experimental",
        "documentation": {}
    },
    {
        "label": "ClassificationModel",
        "importPath": "models.yolo",
        "description": "models.yolo",
        "isExtraImport": true,
        "detail": "models.yolo",
        "documentation": {}
    },
    {
        "label": "DetectionModel",
        "importPath": "models.yolo",
        "description": "models.yolo",
        "isExtraImport": true,
        "detail": "models.yolo",
        "documentation": {}
    },
    {
        "label": "Detect",
        "importPath": "models.yolo",
        "description": "models.yolo",
        "isExtraImport": true,
        "detail": "models.yolo",
        "documentation": {}
    },
    {
        "label": "Segment",
        "importPath": "models.yolo",
        "description": "models.yolo",
        "isExtraImport": true,
        "detail": "models.yolo",
        "documentation": {}
    },
    {
        "label": "SegmentationModel",
        "importPath": "models.yolo",
        "description": "models.yolo",
        "isExtraImport": true,
        "detail": "models.yolo",
        "documentation": {}
    },
    {
        "label": "SegmentationModel",
        "importPath": "models.yolo",
        "description": "models.yolo",
        "isExtraImport": true,
        "detail": "models.yolo",
        "documentation": {}
    },
    {
        "label": "SegmentationModel",
        "importPath": "models.yolo",
        "description": "models.yolo",
        "isExtraImport": true,
        "detail": "models.yolo",
        "documentation": {}
    },
    {
        "label": "ClassificationModel",
        "importPath": "models.yolo",
        "description": "models.yolo",
        "isExtraImport": true,
        "detail": "models.yolo",
        "documentation": {}
    },
    {
        "label": "Detect",
        "importPath": "models.yolo",
        "description": "models.yolo",
        "isExtraImport": true,
        "detail": "models.yolo",
        "documentation": {}
    },
    {
        "label": "DetectionModel",
        "importPath": "models.yolo",
        "description": "models.yolo",
        "isExtraImport": true,
        "detail": "models.yolo",
        "documentation": {}
    },
    {
        "label": "SegmentationModel",
        "importPath": "models.yolo",
        "description": "models.yolo",
        "isExtraImport": true,
        "detail": "models.yolo",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "models.yolo",
        "description": "models.yolo",
        "isExtraImport": true,
        "detail": "models.yolo",
        "documentation": {}
    },
    {
        "label": "GenericLogger",
        "importPath": "utils.loggers",
        "description": "utils.loggers",
        "isExtraImport": true,
        "detail": "utils.loggers",
        "documentation": {}
    },
    {
        "label": "GenericLogger",
        "importPath": "utils.loggers",
        "description": "utils.loggers",
        "isExtraImport": true,
        "detail": "utils.loggers",
        "documentation": {}
    },
    {
        "label": "Loggers",
        "importPath": "utils.loggers",
        "description": "utils.loggers",
        "isExtraImport": true,
        "detail": "utils.loggers",
        "documentation": {}
    },
    {
        "label": "imshow_cls",
        "importPath": "utils.plots",
        "description": "utils.plots",
        "isExtraImport": true,
        "detail": "utils.plots",
        "documentation": {}
    },
    {
        "label": "feature_visualization",
        "importPath": "utils.plots",
        "description": "utils.plots",
        "isExtraImport": true,
        "detail": "utils.plots",
        "documentation": {}
    },
    {
        "label": "plot_evolve",
        "importPath": "utils.plots",
        "description": "utils.plots",
        "isExtraImport": true,
        "detail": "utils.plots",
        "documentation": {}
    },
    {
        "label": "plot_labels",
        "importPath": "utils.plots",
        "description": "utils.plots",
        "isExtraImport": true,
        "detail": "utils.plots",
        "documentation": {}
    },
    {
        "label": "output_to_target",
        "importPath": "utils.plots",
        "description": "utils.plots",
        "isExtraImport": true,
        "detail": "utils.plots",
        "documentation": {}
    },
    {
        "label": "plot_val_study",
        "importPath": "utils.plots",
        "description": "utils.plots",
        "isExtraImport": true,
        "detail": "utils.plots",
        "documentation": {}
    },
    {
        "label": "plot_evolve",
        "importPath": "utils.plots",
        "description": "utils.plots",
        "isExtraImport": true,
        "detail": "utils.plots",
        "documentation": {}
    },
    {
        "label": "output_to_target",
        "importPath": "utils.plots",
        "description": "utils.plots",
        "isExtraImport": true,
        "detail": "utils.plots",
        "documentation": {}
    },
    {
        "label": "plot_images",
        "importPath": "utils.plots",
        "description": "utils.plots",
        "isExtraImport": true,
        "detail": "utils.plots",
        "documentation": {}
    },
    {
        "label": "plot_val_study",
        "importPath": "utils.plots",
        "description": "utils.plots",
        "isExtraImport": true,
        "detail": "utils.plots",
        "documentation": {}
    },
    {
        "label": "ast",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ast",
        "description": "ast",
        "detail": "ast",
        "documentation": {}
    },
    {
        "label": "contextlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "contextlib",
        "description": "contextlib",
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "zipfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "zipfile",
        "description": "zipfile",
        "detail": "zipfile",
        "documentation": {}
    },
    {
        "label": "ZipFile",
        "importPath": "zipfile",
        "description": "zipfile",
        "isExtraImport": true,
        "detail": "zipfile",
        "documentation": {}
    },
    {
        "label": "is_zipfile",
        "importPath": "zipfile",
        "description": "zipfile",
        "isExtraImport": true,
        "detail": "zipfile",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ExifTags",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageOps",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageDraw",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "TryExcept",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "TryExcept",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "TryExcept",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "emojis",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "TryExcept",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "threaded",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "TryExcept",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "threaded",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "notebook_init",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "attempt_download",
        "importPath": "utils.downloads",
        "description": "utils.downloads",
        "isExtraImport": true,
        "detail": "utils.downloads",
        "documentation": {}
    },
    {
        "label": "attempt_download",
        "importPath": "utils.downloads",
        "description": "utils.downloads",
        "isExtraImport": true,
        "detail": "utils.downloads",
        "documentation": {}
    },
    {
        "label": "is_url",
        "importPath": "utils.downloads",
        "description": "utils.downloads",
        "isExtraImport": true,
        "detail": "utils.downloads",
        "documentation": {}
    },
    {
        "label": "curl_download",
        "importPath": "utils.downloads",
        "description": "utils.downloads",
        "isExtraImport": true,
        "detail": "utils.downloads",
        "documentation": {}
    },
    {
        "label": "gsutil_getsize",
        "importPath": "utils.downloads",
        "description": "utils.downloads",
        "isExtraImport": true,
        "detail": "utils.downloads",
        "documentation": {}
    },
    {
        "label": "attempt_download",
        "importPath": "utils.downloads",
        "description": "utils.downloads",
        "isExtraImport": true,
        "detail": "utils.downloads",
        "documentation": {}
    },
    {
        "label": "is_url",
        "importPath": "utils.downloads",
        "description": "utils.downloads",
        "isExtraImport": true,
        "detail": "utils.downloads",
        "documentation": {}
    },
    {
        "label": "tensorflow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow",
        "description": "tensorflow",
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "keras",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "SiLU",
        "importPath": "utils.activations",
        "description": "utils.activations",
        "isExtraImport": true,
        "detail": "utils.activations",
        "documentation": {}
    },
    {
        "label": "check_anchor_order",
        "importPath": "utils.autoanchor",
        "description": "utils.autoanchor",
        "isExtraImport": true,
        "detail": "utils.autoanchor",
        "documentation": {}
    },
    {
        "label": "check_anchors",
        "importPath": "utils.autoanchor",
        "description": "utils.autoanchor",
        "isExtraImport": true,
        "detail": "utils.autoanchor",
        "documentation": {}
    },
    {
        "label": "check_anchors",
        "importPath": "utils.autoanchor",
        "description": "utils.autoanchor",
        "isExtraImport": true,
        "detail": "utils.autoanchor",
        "documentation": {}
    },
    {
        "label": "masks2segments",
        "importPath": "utils.segment.general",
        "description": "utils.segment.general",
        "isExtraImport": true,
        "detail": "utils.segment.general",
        "documentation": {}
    },
    {
        "label": "process_mask",
        "importPath": "utils.segment.general",
        "description": "utils.segment.general",
        "isExtraImport": true,
        "detail": "utils.segment.general",
        "documentation": {}
    },
    {
        "label": "process_mask_native",
        "importPath": "utils.segment.general",
        "description": "utils.segment.general",
        "isExtraImport": true,
        "detail": "utils.segment.general",
        "documentation": {}
    },
    {
        "label": "mask_iou",
        "importPath": "utils.segment.general",
        "description": "utils.segment.general",
        "isExtraImport": true,
        "detail": "utils.segment.general",
        "documentation": {}
    },
    {
        "label": "process_mask",
        "importPath": "utils.segment.general",
        "description": "utils.segment.general",
        "isExtraImport": true,
        "detail": "utils.segment.general",
        "documentation": {}
    },
    {
        "label": "process_mask_native",
        "importPath": "utils.segment.general",
        "description": "utils.segment.general",
        "isExtraImport": true,
        "detail": "utils.segment.general",
        "documentation": {}
    },
    {
        "label": "scale_image",
        "importPath": "utils.segment.general",
        "description": "utils.segment.general",
        "isExtraImport": true,
        "detail": "utils.segment.general",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "lr_scheduler",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "lr_scheduler",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "segment.val",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "segment.val",
        "description": "segment.val",
        "detail": "segment.val",
        "documentation": {}
    },
    {
        "label": "run",
        "importPath": "segment.val",
        "description": "segment.val",
        "isExtraImport": true,
        "detail": "segment.val",
        "documentation": {}
    },
    {
        "label": "check_train_batch_size",
        "importPath": "utils.autobatch",
        "description": "utils.autobatch",
        "isExtraImport": true,
        "detail": "utils.autobatch",
        "documentation": {}
    },
    {
        "label": "check_train_batch_size",
        "importPath": "utils.autobatch",
        "description": "utils.autobatch",
        "isExtraImport": true,
        "detail": "utils.autobatch",
        "documentation": {}
    },
    {
        "label": "Callbacks",
        "importPath": "utils.callbacks",
        "description": "utils.callbacks",
        "isExtraImport": true,
        "detail": "utils.callbacks",
        "documentation": {}
    },
    {
        "label": "Callbacks",
        "importPath": "utils.callbacks",
        "description": "utils.callbacks",
        "isExtraImport": true,
        "detail": "utils.callbacks",
        "documentation": {}
    },
    {
        "label": "Callbacks",
        "importPath": "utils.callbacks",
        "description": "utils.callbacks",
        "isExtraImport": true,
        "detail": "utils.callbacks",
        "documentation": {}
    },
    {
        "label": "Callbacks",
        "importPath": "utils.callbacks",
        "description": "utils.callbacks",
        "isExtraImport": true,
        "detail": "utils.callbacks",
        "documentation": {}
    },
    {
        "label": "Callbacks",
        "importPath": "utils.callbacks",
        "description": "utils.callbacks",
        "isExtraImport": true,
        "detail": "utils.callbacks",
        "documentation": {}
    },
    {
        "label": "create_dataloader",
        "importPath": "utils.segment.dataloaders",
        "description": "utils.segment.dataloaders",
        "isExtraImport": true,
        "detail": "utils.segment.dataloaders",
        "documentation": {}
    },
    {
        "label": "create_dataloader",
        "importPath": "utils.segment.dataloaders",
        "description": "utils.segment.dataloaders",
        "isExtraImport": true,
        "detail": "utils.segment.dataloaders",
        "documentation": {}
    },
    {
        "label": "ComputeLoss",
        "importPath": "utils.segment.loss",
        "description": "utils.segment.loss",
        "isExtraImport": true,
        "detail": "utils.segment.loss",
        "documentation": {}
    },
    {
        "label": "KEYS",
        "importPath": "utils.segment.metrics",
        "description": "utils.segment.metrics",
        "isExtraImport": true,
        "detail": "utils.segment.metrics",
        "documentation": {}
    },
    {
        "label": "fitness",
        "importPath": "utils.segment.metrics",
        "description": "utils.segment.metrics",
        "isExtraImport": true,
        "detail": "utils.segment.metrics",
        "documentation": {}
    },
    {
        "label": "Metrics",
        "importPath": "utils.segment.metrics",
        "description": "utils.segment.metrics",
        "isExtraImport": true,
        "detail": "utils.segment.metrics",
        "documentation": {}
    },
    {
        "label": "ap_per_class_box_and_mask",
        "importPath": "utils.segment.metrics",
        "description": "utils.segment.metrics",
        "isExtraImport": true,
        "detail": "utils.segment.metrics",
        "documentation": {}
    },
    {
        "label": "plot_images_and_masks",
        "importPath": "utils.segment.plots",
        "description": "utils.segment.plots",
        "isExtraImport": true,
        "detail": "utils.segment.plots",
        "documentation": {}
    },
    {
        "label": "plot_results_with_masks",
        "importPath": "utils.segment.plots",
        "description": "utils.segment.plots",
        "isExtraImport": true,
        "detail": "utils.segment.plots",
        "documentation": {}
    },
    {
        "label": "plot_images_and_masks",
        "importPath": "utils.segment.plots",
        "description": "utils.segment.plots",
        "isExtraImport": true,
        "detail": "utils.segment.plots",
        "documentation": {}
    },
    {
        "label": "ThreadPool",
        "importPath": "multiprocessing.pool",
        "description": "multiprocessing.pool",
        "isExtraImport": true,
        "detail": "multiprocessing.pool",
        "documentation": {}
    },
    {
        "label": "Pool",
        "importPath": "multiprocessing.pool",
        "description": "multiprocessing.pool",
        "isExtraImport": true,
        "detail": "multiprocessing.pool",
        "documentation": {}
    },
    {
        "label": "ThreadPool",
        "importPath": "multiprocessing.pool",
        "description": "multiprocessing.pool",
        "isExtraImport": true,
        "detail": "multiprocessing.pool",
        "documentation": {}
    },
    {
        "label": "ThreadPool",
        "importPath": "multiprocessing.pool",
        "description": "multiprocessing.pool",
        "isExtraImport": true,
        "detail": "multiprocessing.pool",
        "documentation": {}
    },
    {
        "label": "ConfusionMatrix",
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "isExtraImport": true,
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "box_iou",
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "isExtraImport": true,
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "bbox_ioa",
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "isExtraImport": true,
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "box_iou",
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "isExtraImport": true,
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "fitness",
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "isExtraImport": true,
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "bbox_iou",
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "isExtraImport": true,
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "fitness",
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "isExtraImport": true,
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "fitness",
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "isExtraImport": true,
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "ConfusionMatrix",
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "isExtraImport": true,
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "ap_per_class",
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "isExtraImport": true,
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "box_iou",
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "isExtraImport": true,
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "pprint",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pprint",
        "description": "pprint",
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "Task",
        "importPath": "clearml",
        "description": "clearml",
        "isExtraImport": true,
        "detail": "clearml",
        "documentation": {}
    },
    {
        "label": "HyperParameterOptimizer",
        "importPath": "clearml.automation",
        "description": "clearml.automation",
        "isExtraImport": true,
        "detail": "clearml.automation",
        "documentation": {}
    },
    {
        "label": "UniformParameterRange",
        "importPath": "clearml.automation",
        "description": "clearml.automation",
        "isExtraImport": true,
        "detail": "clearml.automation",
        "documentation": {}
    },
    {
        "label": "OptimizerOptuna",
        "importPath": "clearml.automation.optuna",
        "description": "clearml.automation.optuna",
        "isExtraImport": true,
        "detail": "clearml.automation.optuna",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "comet_ml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "comet_ml",
        "description": "comet_ml",
        "detail": "comet_ml",
        "documentation": {}
    },
    {
        "label": "train",
        "importPath": "train",
        "description": "train",
        "isExtraImport": true,
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "distributed",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "distributed",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "Thread",
        "importPath": "threading",
        "description": "threading",
        "isExtraImport": true,
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "psutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "psutil",
        "description": "psutil",
        "detail": "psutil",
        "documentation": {}
    },
    {
        "label": "urllib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib",
        "description": "urllib",
        "detail": "urllib",
        "documentation": {}
    },
    {
        "label": "inspect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inspect",
        "description": "inspect",
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "logging.config",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging.config",
        "description": "logging.config",
        "detail": "logging.config",
        "documentation": {}
    },
    {
        "label": "signal",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "signal",
        "description": "signal",
        "detail": "signal",
        "documentation": {}
    },
    {
        "label": "is_tarfile",
        "importPath": "tarfile",
        "description": "tarfile",
        "isExtraImport": true,
        "detail": "tarfile",
        "documentation": {}
    },
    {
        "label": "typing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "typing",
        "description": "typing",
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "pkg_resources",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pkg_resources",
        "description": "pkg_resources",
        "detail": "pkg_resources",
        "documentation": {}
    },
    {
        "label": "check_requirements",
        "importPath": "ultralytics.utils.checks",
        "description": "ultralytics.utils.checks",
        "isExtraImport": true,
        "detail": "ultralytics.utils.checks",
        "documentation": {}
    },
    {
        "label": "matplotlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib",
        "description": "matplotlib",
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "gaussian_filter1d",
        "importPath": "scipy.ndimage.filters",
        "description": "scipy.ndimage.filters",
        "isExtraImport": true,
        "detail": "scipy.ndimage.filters",
        "documentation": {}
    },
    {
        "label": "DistributedDataParallel",
        "importPath": "torch.nn.parallel",
        "description": "torch.nn.parallel",
        "isExtraImport": true,
        "detail": "torch.nn.parallel",
        "documentation": {}
    },
    {
        "label": "export",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "export",
        "description": "export",
        "detail": "export",
        "documentation": {}
    },
    {
        "label": "val",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "val",
        "description": "val",
        "detail": "val",
        "documentation": {}
    },
    {
        "label": "run",
        "importPath": "val",
        "description": "val",
        "isExtraImport": true,
        "detail": "val",
        "documentation": {}
    },
    {
        "label": "optimize_for_mobile",
        "importPath": "torch.utils.mobile_optimizer",
        "description": "torch.utils.mobile_optimizer",
        "isExtraImport": true,
        "detail": "torch.utils.mobile_optimizer",
        "documentation": {}
    },
    {
        "label": "check_comet_resume",
        "importPath": "utils.loggers.comet.comet_utils",
        "description": "utils.loggers.comet.comet_utils",
        "isExtraImport": true,
        "detail": "utils.loggers.comet.comet_utils",
        "documentation": {}
    },
    {
        "label": "ComputeLoss",
        "importPath": "utils.loss",
        "description": "utils.loss",
        "isExtraImport": true,
        "detail": "utils.loss",
        "documentation": {}
    },
    {
        "label": "load_model",
        "importPath": "tensorflow.keras.models",
        "description": "tensorflow.keras.models",
        "isExtraImport": true,
        "detail": "tensorflow.keras.models",
        "documentation": {}
    },
    {
        "label": "load_model",
        "importPath": "tensorflow.keras.models",
        "description": "tensorflow.keras.models",
        "isExtraImport": true,
        "detail": "tensorflow.keras.models",
        "documentation": {}
    },
    {
        "label": "resnet",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "resnet",
        "importPath": "tensorflow.keras.applications",
        "description": "tensorflow.keras.applications",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications",
        "documentation": {}
    },
    {
        "label": "io",
        "importPath": "skimage",
        "description": "skimage",
        "isExtraImport": true,
        "detail": "skimage",
        "documentation": {}
    },
    {
        "label": "io",
        "importPath": "skimage",
        "description": "skimage",
        "isExtraImport": true,
        "detail": "skimage",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.classify.predict",
        "description": "recommendation_assets.yolov5.classify.predict",
        "peekOfCode": "def run(\n        weights=ROOT / 'yolov5s-cls.pt',  # model.pt path(s)\n        source=ROOT / 'data/images',  # file/dir/URL/glob/screen/0(webcam)\n        data=ROOT / 'data/coco128.yaml',  # dataset.yaml path\n        imgsz=(224, 224),  # inference size (height, width)\n        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n        view_img=False,  # show results\n        save_txt=False,  # save results to *.txt\n        nosave=False,  # do not save images/videos\n        augment=False,  # augmented inference",
        "detail": "recommendation_assets.yolov5.classify.predict",
        "documentation": {}
    },
    {
        "label": "parse_opt",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.classify.predict",
        "description": "recommendation_assets.yolov5.classify.predict",
        "peekOfCode": "def parse_opt():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s-cls.pt', help='model path(s)')\n    parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob/screen/0(webcam)')\n    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) dataset.yaml path')\n    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[224], help='inference size h,w')\n    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n    parser.add_argument('--view-img', action='store_true', help='show results')\n    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')",
        "detail": "recommendation_assets.yolov5.classify.predict",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.classify.predict",
        "description": "recommendation_assets.yolov5.classify.predict",
        "peekOfCode": "def main(opt):\n    check_requirements(ROOT / 'requirements.txt', exclude=('tensorboard', 'thop'))\n    run(**vars(opt))\nif __name__ == '__main__':\n    opt = parse_opt()\n    main(opt)",
        "detail": "recommendation_assets.yolov5.classify.predict",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.classify.predict",
        "description": "recommendation_assets.yolov5.classify.predict",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom ultralytics.utils.plotting import Annotator\nfrom models.common import DetectMultiBackend\nfrom utils.augmentations import classify_transforms\nfrom utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\nfrom utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,",
        "detail": "recommendation_assets.yolov5.classify.predict",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.classify.predict",
        "description": "recommendation_assets.yolov5.classify.predict",
        "peekOfCode": "ROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom ultralytics.utils.plotting import Annotator\nfrom models.common import DetectMultiBackend\nfrom utils.augmentations import classify_transforms\nfrom utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\nfrom utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n                           increment_path, print_args, strip_optimizer)",
        "detail": "recommendation_assets.yolov5.classify.predict",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.classify.predict",
        "description": "recommendation_assets.yolov5.classify.predict",
        "peekOfCode": "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom ultralytics.utils.plotting import Annotator\nfrom models.common import DetectMultiBackend\nfrom utils.augmentations import classify_transforms\nfrom utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\nfrom utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n                           increment_path, print_args, strip_optimizer)\nfrom utils.torch_utils import select_device, smart_inference_mode\n@smart_inference_mode()\ndef run(",
        "detail": "recommendation_assets.yolov5.classify.predict",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.classify.train",
        "description": "recommendation_assets.yolov5.classify.train",
        "peekOfCode": "def train(opt, device):\n    init_seeds(opt.seed + 1 + RANK, deterministic=True)\n    save_dir, data, bs, epochs, nw, imgsz, pretrained = \\\n        opt.save_dir, Path(opt.data), opt.batch_size, opt.epochs, min(os.cpu_count() - 1, opt.workers), \\\n        opt.imgsz, str(opt.pretrained).lower() == 'true'\n    cuda = device.type != 'cpu'\n    # Directories\n    wdir = save_dir / 'weights'\n    wdir.mkdir(parents=True, exist_ok=True)  # make dir\n    last, best = wdir / 'last.pt', wdir / 'best.pt'",
        "detail": "recommendation_assets.yolov5.classify.train",
        "documentation": {}
    },
    {
        "label": "parse_opt",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.classify.train",
        "description": "recommendation_assets.yolov5.classify.train",
        "peekOfCode": "def parse_opt(known=False):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', type=str, default='yolov5s-cls.pt', help='initial weights path')\n    parser.add_argument('--data', type=str, default='imagenette160', help='cifar10, cifar100, mnist, imagenet, ...')\n    parser.add_argument('--epochs', type=int, default=10, help='total training epochs')\n    parser.add_argument('--batch-size', type=int, default=64, help='total batch size for all GPUs')\n    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=224, help='train, val image size (pixels)')\n    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')\n    parser.add_argument('--cache', type=str, nargs='?', const='ram', help='--cache images in \"ram\" (default) or \"disk\"')\n    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')",
        "detail": "recommendation_assets.yolov5.classify.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.classify.train",
        "description": "recommendation_assets.yolov5.classify.train",
        "peekOfCode": "def main(opt):\n    # Checks\n    if RANK in {-1, 0}:\n        print_args(vars(opt))\n        check_git_status()\n        check_requirements(ROOT / 'requirements.txt')\n    # DDP mode\n    device = select_device(opt.device, batch_size=opt.batch_size)\n    if LOCAL_RANK != -1:\n        assert opt.batch_size != -1, 'AutoBatch is coming soon for classification, please pass a valid --batch-size'",
        "detail": "recommendation_assets.yolov5.classify.train",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.classify.train",
        "description": "recommendation_assets.yolov5.classify.train",
        "peekOfCode": "def run(**kwargs):\n    # Usage: from yolov5 import classify; classify.train.run(data=mnist, imgsz=320, model='yolov5m')\n    opt = parse_opt(True)\n    for k, v in kwargs.items():\n        setattr(opt, k, v)\n    main(opt)\n    return opt\nif __name__ == '__main__':\n    opt = parse_opt()\n    main(opt)",
        "detail": "recommendation_assets.yolov5.classify.train",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.classify.train",
        "description": "recommendation_assets.yolov5.classify.train",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom classify import val as validate\nfrom models.experimental import attempt_load\nfrom models.yolo import ClassificationModel, DetectionModel\nfrom utils.dataloaders import create_classification_dataloader\nfrom utils.general import (DATASETS_DIR, LOGGER, TQDM_BAR_FORMAT, WorkingDirectory, check_git_info, check_git_status,",
        "detail": "recommendation_assets.yolov5.classify.train",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.classify.train",
        "description": "recommendation_assets.yolov5.classify.train",
        "peekOfCode": "ROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom classify import val as validate\nfrom models.experimental import attempt_load\nfrom models.yolo import ClassificationModel, DetectionModel\nfrom utils.dataloaders import create_classification_dataloader\nfrom utils.general import (DATASETS_DIR, LOGGER, TQDM_BAR_FORMAT, WorkingDirectory, check_git_info, check_git_status,\n                           check_requirements, colorstr, download, increment_path, init_seeds, print_args, yaml_save)",
        "detail": "recommendation_assets.yolov5.classify.train",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.classify.train",
        "description": "recommendation_assets.yolov5.classify.train",
        "peekOfCode": "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom classify import val as validate\nfrom models.experimental import attempt_load\nfrom models.yolo import ClassificationModel, DetectionModel\nfrom utils.dataloaders import create_classification_dataloader\nfrom utils.general import (DATASETS_DIR, LOGGER, TQDM_BAR_FORMAT, WorkingDirectory, check_git_info, check_git_status,\n                           check_requirements, colorstr, download, increment_path, init_seeds, print_args, yaml_save)\nfrom utils.loggers import GenericLogger\nfrom utils.plots import imshow_cls\nfrom utils.torch_utils import (ModelEMA, de_parallel, model_info, reshape_classifier_output, select_device, smart_DDP,",
        "detail": "recommendation_assets.yolov5.classify.train",
        "documentation": {}
    },
    {
        "label": "LOCAL_RANK",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.classify.train",
        "description": "recommendation_assets.yolov5.classify.train",
        "peekOfCode": "LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html\nRANK = int(os.getenv('RANK', -1))\nWORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))\nGIT_INFO = check_git_info()\ndef train(opt, device):\n    init_seeds(opt.seed + 1 + RANK, deterministic=True)\n    save_dir, data, bs, epochs, nw, imgsz, pretrained = \\\n        opt.save_dir, Path(opt.data), opt.batch_size, opt.epochs, min(os.cpu_count() - 1, opt.workers), \\\n        opt.imgsz, str(opt.pretrained).lower() == 'true'\n    cuda = device.type != 'cpu'",
        "detail": "recommendation_assets.yolov5.classify.train",
        "documentation": {}
    },
    {
        "label": "RANK",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.classify.train",
        "description": "recommendation_assets.yolov5.classify.train",
        "peekOfCode": "RANK = int(os.getenv('RANK', -1))\nWORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))\nGIT_INFO = check_git_info()\ndef train(opt, device):\n    init_seeds(opt.seed + 1 + RANK, deterministic=True)\n    save_dir, data, bs, epochs, nw, imgsz, pretrained = \\\n        opt.save_dir, Path(opt.data), opt.batch_size, opt.epochs, min(os.cpu_count() - 1, opt.workers), \\\n        opt.imgsz, str(opt.pretrained).lower() == 'true'\n    cuda = device.type != 'cpu'\n    # Directories",
        "detail": "recommendation_assets.yolov5.classify.train",
        "documentation": {}
    },
    {
        "label": "WORLD_SIZE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.classify.train",
        "description": "recommendation_assets.yolov5.classify.train",
        "peekOfCode": "WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))\nGIT_INFO = check_git_info()\ndef train(opt, device):\n    init_seeds(opt.seed + 1 + RANK, deterministic=True)\n    save_dir, data, bs, epochs, nw, imgsz, pretrained = \\\n        opt.save_dir, Path(opt.data), opt.batch_size, opt.epochs, min(os.cpu_count() - 1, opt.workers), \\\n        opt.imgsz, str(opt.pretrained).lower() == 'true'\n    cuda = device.type != 'cpu'\n    # Directories\n    wdir = save_dir / 'weights'",
        "detail": "recommendation_assets.yolov5.classify.train",
        "documentation": {}
    },
    {
        "label": "GIT_INFO",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.classify.train",
        "description": "recommendation_assets.yolov5.classify.train",
        "peekOfCode": "GIT_INFO = check_git_info()\ndef train(opt, device):\n    init_seeds(opt.seed + 1 + RANK, deterministic=True)\n    save_dir, data, bs, epochs, nw, imgsz, pretrained = \\\n        opt.save_dir, Path(opt.data), opt.batch_size, opt.epochs, min(os.cpu_count() - 1, opt.workers), \\\n        opt.imgsz, str(opt.pretrained).lower() == 'true'\n    cuda = device.type != 'cpu'\n    # Directories\n    wdir = save_dir / 'weights'\n    wdir.mkdir(parents=True, exist_ok=True)  # make dir",
        "detail": "recommendation_assets.yolov5.classify.train",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.classify.val",
        "description": "recommendation_assets.yolov5.classify.val",
        "peekOfCode": "def run(\n    data=ROOT / '../datasets/mnist',  # dataset dir\n    weights=ROOT / 'yolov5s-cls.pt',  # model.pt path(s)\n    batch_size=128,  # batch size\n    imgsz=224,  # inference size (pixels)\n    device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n    workers=8,  # max dataloader workers (per RANK in DDP mode)\n    verbose=False,  # verbose output\n    project=ROOT / 'runs/val-cls',  # save to project/name\n    name='exp',  # save to project/name",
        "detail": "recommendation_assets.yolov5.classify.val",
        "documentation": {}
    },
    {
        "label": "parse_opt",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.classify.val",
        "description": "recommendation_assets.yolov5.classify.val",
        "peekOfCode": "def parse_opt():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data', type=str, default=ROOT / '../datasets/mnist', help='dataset path')\n    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s-cls.pt', help='model.pt path(s)')\n    parser.add_argument('--batch-size', type=int, default=128, help='batch size')\n    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=224, help='inference size (pixels)')\n    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n    parser.add_argument('--workers', type=int, default=8, help='max dataloader workers (per RANK in DDP mode)')\n    parser.add_argument('--verbose', nargs='?', const=True, default=True, help='verbose output')\n    parser.add_argument('--project', default=ROOT / 'runs/val-cls', help='save to project/name')",
        "detail": "recommendation_assets.yolov5.classify.val",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.classify.val",
        "description": "recommendation_assets.yolov5.classify.val",
        "peekOfCode": "def main(opt):\n    check_requirements(ROOT / 'requirements.txt', exclude=('tensorboard', 'thop'))\n    run(**vars(opt))\nif __name__ == '__main__':\n    opt = parse_opt()\n    main(opt)",
        "detail": "recommendation_assets.yolov5.classify.val",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.classify.val",
        "description": "recommendation_assets.yolov5.classify.val",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom models.common import DetectMultiBackend\nfrom utils.dataloaders import create_classification_dataloader\nfrom utils.general import (LOGGER, TQDM_BAR_FORMAT, Profile, check_img_size, check_requirements, colorstr,\n                           increment_path, print_args)\nfrom utils.torch_utils import select_device, smart_inference_mode",
        "detail": "recommendation_assets.yolov5.classify.val",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.classify.val",
        "description": "recommendation_assets.yolov5.classify.val",
        "peekOfCode": "ROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom models.common import DetectMultiBackend\nfrom utils.dataloaders import create_classification_dataloader\nfrom utils.general import (LOGGER, TQDM_BAR_FORMAT, Profile, check_img_size, check_requirements, colorstr,\n                           increment_path, print_args)\nfrom utils.torch_utils import select_device, smart_inference_mode\n@smart_inference_mode()",
        "detail": "recommendation_assets.yolov5.classify.val",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.classify.val",
        "description": "recommendation_assets.yolov5.classify.val",
        "peekOfCode": "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom models.common import DetectMultiBackend\nfrom utils.dataloaders import create_classification_dataloader\nfrom utils.general import (LOGGER, TQDM_BAR_FORMAT, Profile, check_img_size, check_requirements, colorstr,\n                           increment_path, print_args)\nfrom utils.torch_utils import select_device, smart_inference_mode\n@smart_inference_mode()\ndef run(\n    data=ROOT / '../datasets/mnist',  # dataset dir\n    weights=ROOT / 'yolov5s-cls.pt',  # model.pt path(s)",
        "detail": "recommendation_assets.yolov5.classify.val",
        "documentation": {}
    },
    {
        "label": "Conv",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class Conv(nn.Module):\n    # Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)\n    default_act = nn.SiLU()  # default activation\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n        super().__init__()\n        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n        self.bn = nn.BatchNorm2d(c2)\n        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n    def forward(self, x):\n        return self.act(self.bn(self.conv(x)))",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "DWConv",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class DWConv(Conv):\n    # Depth-wise convolution\n    def __init__(self, c1, c2, k=1, s=1, d=1, act=True):  # ch_in, ch_out, kernel, stride, dilation, activation\n        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), d=d, act=act)\nclass DWConvTranspose2d(nn.ConvTranspose2d):\n    # Depth-wise transpose convolution\n    def __init__(self, c1, c2, k=1, s=1, p1=0, p2=0):  # ch_in, ch_out, kernel, stride, padding, padding_out\n        super().__init__(c1, c2, k, s, p1, p2, groups=math.gcd(c1, c2))\nclass TransformerLayer(nn.Module):\n    # Transformer layer https://arxiv.org/abs/2010.11929 (LayerNorm layers removed for better performance)",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "DWConvTranspose2d",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class DWConvTranspose2d(nn.ConvTranspose2d):\n    # Depth-wise transpose convolution\n    def __init__(self, c1, c2, k=1, s=1, p1=0, p2=0):  # ch_in, ch_out, kernel, stride, padding, padding_out\n        super().__init__(c1, c2, k, s, p1, p2, groups=math.gcd(c1, c2))\nclass TransformerLayer(nn.Module):\n    # Transformer layer https://arxiv.org/abs/2010.11929 (LayerNorm layers removed for better performance)\n    def __init__(self, c, num_heads):\n        super().__init__()\n        self.q = nn.Linear(c, c, bias=False)\n        self.k = nn.Linear(c, c, bias=False)",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "TransformerLayer",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class TransformerLayer(nn.Module):\n    # Transformer layer https://arxiv.org/abs/2010.11929 (LayerNorm layers removed for better performance)\n    def __init__(self, c, num_heads):\n        super().__init__()\n        self.q = nn.Linear(c, c, bias=False)\n        self.k = nn.Linear(c, c, bias=False)\n        self.v = nn.Linear(c, c, bias=False)\n        self.ma = nn.MultiheadAttention(embed_dim=c, num_heads=num_heads)\n        self.fc1 = nn.Linear(c, c, bias=False)\n        self.fc2 = nn.Linear(c, c, bias=False)",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class TransformerBlock(nn.Module):\n    # Vision Transformer https://arxiv.org/abs/2010.11929\n    def __init__(self, c1, c2, num_heads, num_layers):\n        super().__init__()\n        self.conv = None\n        if c1 != c2:\n            self.conv = Conv(c1, c2)\n        self.linear = nn.Linear(c2, c2)  # learnable position embedding\n        self.tr = nn.Sequential(*(TransformerLayer(c2, num_heads) for _ in range(num_layers)))\n        self.c2 = c2",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "Bottleneck",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class Bottleneck(nn.Module):\n    # Standard bottleneck\n    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_, c2, 3, 1, g=g)\n        self.add = shortcut and c1 == c2\n    def forward(self, x):\n        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "BottleneckCSP",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class BottleneckCSP(nn.Module):\n    # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)\n        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)\n        self.cv4 = Conv(2 * c_, c2, 1, 1)\n        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "CrossConv",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class CrossConv(nn.Module):\n    # Cross Convolution Downsample\n    def __init__(self, c1, c2, k=3, s=1, g=1, e=1.0, shortcut=False):\n        # ch_in, ch_out, kernel, stride, groups, expansion, shortcut\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, (1, k), (1, s))\n        self.cv2 = Conv(c_, c2, (k, 1), (s, 1), g=g)\n        self.add = shortcut and c1 == c2\n    def forward(self, x):",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "C3",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class C3(nn.Module):\n    # CSP Bottleneck with 3 convolutions\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c1, c_, 1, 1)\n        self.cv3 = Conv(2 * c_, c2, 1)  # optional act=FReLU(c2)\n        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))\n    def forward(self, x):",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "C3x",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class C3x(C3):\n    # C3 module with cross-convolutions\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n        super().__init__(c1, c2, n, shortcut, g, e)\n        c_ = int(c2 * e)\n        self.m = nn.Sequential(*(CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)))\nclass C3TR(C3):\n    # C3 module with TransformerBlock()\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n        super().__init__(c1, c2, n, shortcut, g, e)",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "C3TR",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class C3TR(C3):\n    # C3 module with TransformerBlock()\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n        super().__init__(c1, c2, n, shortcut, g, e)\n        c_ = int(c2 * e)\n        self.m = TransformerBlock(c_, c_, 4, n)\nclass C3SPP(C3):\n    # C3 module with SPP()\n    def __init__(self, c1, c2, k=(5, 9, 13), n=1, shortcut=True, g=1, e=0.5):\n        super().__init__(c1, c2, n, shortcut, g, e)",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "C3SPP",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class C3SPP(C3):\n    # C3 module with SPP()\n    def __init__(self, c1, c2, k=(5, 9, 13), n=1, shortcut=True, g=1, e=0.5):\n        super().__init__(c1, c2, n, shortcut, g, e)\n        c_ = int(c2 * e)\n        self.m = SPP(c_, c_, k)\nclass C3Ghost(C3):\n    # C3 module with GhostBottleneck()\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n        super().__init__(c1, c2, n, shortcut, g, e)",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "C3Ghost",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class C3Ghost(C3):\n    # C3 module with GhostBottleneck()\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n        super().__init__(c1, c2, n, shortcut, g, e)\n        c_ = int(c2 * e)  # hidden channels\n        self.m = nn.Sequential(*(GhostBottleneck(c_, c_) for _ in range(n)))\nclass SPP(nn.Module):\n    # Spatial Pyramid Pooling (SPP) layer https://arxiv.org/abs/1406.4729\n    def __init__(self, c1, c2, k=(5, 9, 13)):\n        super().__init__()",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "SPP",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class SPP(nn.Module):\n    # Spatial Pyramid Pooling (SPP) layer https://arxiv.org/abs/1406.4729\n    def __init__(self, c1, c2, k=(5, 9, 13)):\n        super().__init__()\n        c_ = c1 // 2  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)\n        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])\n    def forward(self, x):\n        x = self.cv1(x)",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "SPPF",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class SPPF(nn.Module):\n    # Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher\n    def __init__(self, c1, c2, k=5):  # equivalent to SPP(k=(5, 9, 13))\n        super().__init__()\n        c_ = c1 // 2  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_ * 4, c2, 1, 1)\n        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n    def forward(self, x):\n        x = self.cv1(x)",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "Focus",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class Focus(nn.Module):\n    # Focus wh information into c-space\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n        super().__init__()\n        self.conv = Conv(c1 * 4, c2, k, s, p, g, act=act)\n        # self.contract = Contract(gain=2)\n    def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2)\n        return self.conv(torch.cat((x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]), 1))\n        # return self.conv(self.contract(x))\nclass GhostConv(nn.Module):",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "GhostConv",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class GhostConv(nn.Module):\n    # Ghost Convolution https://github.com/huawei-noah/ghostnet\n    def __init__(self, c1, c2, k=1, s=1, g=1, act=True):  # ch_in, ch_out, kernel, stride, groups\n        super().__init__()\n        c_ = c2 // 2  # hidden channels\n        self.cv1 = Conv(c1, c_, k, s, None, g, act=act)\n        self.cv2 = Conv(c_, c_, 5, 1, None, c_, act=act)\n    def forward(self, x):\n        y = self.cv1(x)\n        return torch.cat((y, self.cv2(y)), 1)",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "GhostBottleneck",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class GhostBottleneck(nn.Module):\n    # Ghost Bottleneck https://github.com/huawei-noah/ghostnet\n    def __init__(self, c1, c2, k=3, s=1):  # ch_in, ch_out, kernel, stride\n        super().__init__()\n        c_ = c2 // 2\n        self.conv = nn.Sequential(\n            GhostConv(c1, c_, 1, 1),  # pw\n            DWConv(c_, c_, k, s, act=False) if s == 2 else nn.Identity(),  # dw\n            GhostConv(c_, c2, 1, 1, act=False))  # pw-linear\n        self.shortcut = nn.Sequential(DWConv(c1, c1, k, s, act=False), Conv(c1, c2, 1, 1,",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "Contract",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class Contract(nn.Module):\n    # Contract width-height into channels, i.e. x(1,64,80,80) to x(1,256,40,40)\n    def __init__(self, gain=2):\n        super().__init__()\n        self.gain = gain\n    def forward(self, x):\n        b, c, h, w = x.size()  # assert (h / s == 0) and (W / s == 0), 'Indivisible gain'\n        s = self.gain\n        x = x.view(b, c, h // s, s, w // s, s)  # x(1,64,40,2,40,2)\n        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # x(1,2,2,64,40,40)",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "Expand",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class Expand(nn.Module):\n    # Expand channels into width-height, i.e. x(1,64,80,80) to x(1,16,160,160)\n    def __init__(self, gain=2):\n        super().__init__()\n        self.gain = gain\n    def forward(self, x):\n        b, c, h, w = x.size()  # assert C / s ** 2 == 0, 'Indivisible gain'\n        s = self.gain\n        x = x.view(b, s, s, c // s ** 2, h, w)  # x(1,2,2,16,80,80)\n        x = x.permute(0, 3, 4, 1, 5, 2).contiguous()  # x(1,16,80,2,80,2)",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "Concat",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class Concat(nn.Module):\n    # Concatenate a list of tensors along dimension\n    def __init__(self, dimension=1):\n        super().__init__()\n        self.d = dimension\n    def forward(self, x):\n        return torch.cat(x, self.d)\nclass DetectMultiBackend(nn.Module):\n    # YOLOv5 MultiBackend class for python inference on various backends\n    def __init__(self, weights='yolov5s.pt', device=torch.device('cpu'), dnn=False, data=None, fp16=False, fuse=True):",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "DetectMultiBackend",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class DetectMultiBackend(nn.Module):\n    # YOLOv5 MultiBackend class for python inference on various backends\n    def __init__(self, weights='yolov5s.pt', device=torch.device('cpu'), dnn=False, data=None, fp16=False, fuse=True):\n        # Usage:\n        #   PyTorch:              weights = *.pt\n        #   TorchScript:                    *.torchscript\n        #   ONNX Runtime:                   *.onnx\n        #   ONNX OpenCV DNN:                *.onnx --dnn\n        #   OpenVINO:                       *_openvino_model\n        #   CoreML:                         *.mlmodel",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "AutoShape",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class AutoShape(nn.Module):\n    # YOLOv5 input-robust model wrapper for passing cv2/np/PIL/torch inputs. Includes preprocessing, inference and NMS\n    conf = 0.25  # NMS confidence threshold\n    iou = 0.45  # NMS IoU threshold\n    agnostic = False  # NMS class-agnostic\n    multi_label = False  # NMS multiple labels per box\n    classes = None  # (optional list) filter by class, i.e. = [0, 15, 16] for COCO persons, cats and dogs\n    max_det = 1000  # maximum number of detections per image\n    amp = False  # Automatic Mixed Precision (AMP) inference\n    def __init__(self, model, verbose=True):",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "Detections",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class Detections:\n    # YOLOv5 detections class for inference results\n    def __init__(self, ims, pred, files, times=(0, 0, 0), names=None, shape=None):\n        super().__init__()\n        d = pred[0].device  # device\n        gn = [torch.tensor([*(im.shape[i] for i in [1, 0, 1, 0]), 1, 1], device=d) for im in ims]  # normalizations\n        self.ims = ims  # list of images as numpy arrays\n        self.pred = pred  # list of tensors pred[0] = (xyxy, conf, cls)\n        self.names = names  # class names\n        self.files = files  # image filenames",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "Proto",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class Proto(nn.Module):\n    # YOLOv5 mask Proto module for segmentation models\n    def __init__(self, c1, c_=256, c2=32):  # ch_in, number of protos, number of masks\n        super().__init__()\n        self.cv1 = Conv(c1, c_, k=3)\n        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n        self.cv2 = Conv(c_, c_, k=3)\n        self.cv3 = Conv(c_, c2)\n    def forward(self, x):\n        return self.cv3(self.cv2(self.upsample(self.cv1(x))))",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "Classify",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "class Classify(nn.Module):\n    # YOLOv5 classification head, i.e. x(b,c1,20,20) to x(b,c2)\n    def __init__(self,\n                 c1,\n                 c2,\n                 k=1,\n                 s=1,\n                 p=None,\n                 g=1,\n                 dropout_p=0.0):  # ch_in, ch_out, kernel, stride, padding, groups, dropout probability",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "autopad",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.models.common",
        "description": "recommendation_assets.yolov5.models.common",
        "peekOfCode": "def autopad(k, p=None, d=1):  # kernel, padding, dilation\n    # Pad to 'same' shape outputs\n    if d > 1:\n        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size\n    if p is None:\n        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n    return p\nclass Conv(nn.Module):\n    # Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)\n    default_act = nn.SiLU()  # default activation",
        "detail": "recommendation_assets.yolov5.models.common",
        "documentation": {}
    },
    {
        "label": "Sum",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.experimental",
        "description": "recommendation_assets.yolov5.models.experimental",
        "peekOfCode": "class Sum(nn.Module):\n    # Weighted sum of 2 or more layers https://arxiv.org/abs/1911.09070\n    def __init__(self, n, weight=False):  # n: number of inputs\n        super().__init__()\n        self.weight = weight  # apply weights boolean\n        self.iter = range(n - 1)  # iter object\n        if weight:\n            self.w = nn.Parameter(-torch.arange(1.0, n) / 2, requires_grad=True)  # layer weights\n    def forward(self, x):\n        y = x[0]  # no weight",
        "detail": "recommendation_assets.yolov5.models.experimental",
        "documentation": {}
    },
    {
        "label": "MixConv2d",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.experimental",
        "description": "recommendation_assets.yolov5.models.experimental",
        "peekOfCode": "class MixConv2d(nn.Module):\n    # Mixed Depth-wise Conv https://arxiv.org/abs/1907.09595\n    def __init__(self, c1, c2, k=(1, 3), s=1, equal_ch=True):  # ch_in, ch_out, kernel, stride, ch_strategy\n        super().__init__()\n        n = len(k)  # number of convolutions\n        if equal_ch:  # equal c_ per group\n            i = torch.linspace(0, n - 1E-6, c2).floor()  # c2 indices\n            c_ = [(i == g).sum() for g in range(n)]  # intermediate channels\n        else:  # equal weight.numel() per group\n            b = [c2] + [0] * n",
        "detail": "recommendation_assets.yolov5.models.experimental",
        "documentation": {}
    },
    {
        "label": "Ensemble",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.experimental",
        "description": "recommendation_assets.yolov5.models.experimental",
        "peekOfCode": "class Ensemble(nn.ModuleList):\n    # Ensemble of models\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, augment=False, profile=False, visualize=False):\n        y = [module(x, augment, profile, visualize)[0] for module in self]\n        # y = torch.stack(y).max(0)[0]  # max ensemble\n        # y = torch.stack(y).mean(0)  # mean ensemble\n        y = torch.cat(y, 1)  # nms ensemble\n        return y, None  # inference, train output",
        "detail": "recommendation_assets.yolov5.models.experimental",
        "documentation": {}
    },
    {
        "label": "attempt_load",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.models.experimental",
        "description": "recommendation_assets.yolov5.models.experimental",
        "peekOfCode": "def attempt_load(weights, device=None, inplace=True, fuse=True):\n    # Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a\n    from models.yolo import Detect, Model\n    model = Ensemble()\n    for w in weights if isinstance(weights, list) else [weights]:\n        ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n        ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model\n        # Model compatibility updates\n        if not hasattr(ckpt, 'stride'):\n            ckpt.stride = torch.tensor([32.])",
        "detail": "recommendation_assets.yolov5.models.experimental",
        "documentation": {}
    },
    {
        "label": "TFBN",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFBN(keras.layers.Layer):\n    # TensorFlow BatchNormalization wrapper\n    def __init__(self, w=None):\n        super().__init__()\n        self.bn = keras.layers.BatchNormalization(\n            beta_initializer=keras.initializers.Constant(w.bias.numpy()),\n            gamma_initializer=keras.initializers.Constant(w.weight.numpy()),\n            moving_mean_initializer=keras.initializers.Constant(w.running_mean.numpy()),\n            moving_variance_initializer=keras.initializers.Constant(w.running_var.numpy()),\n            epsilon=w.eps)",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFPad",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFPad(keras.layers.Layer):\n    # Pad inputs in spatial dimensions 1 and 2\n    def __init__(self, pad):\n        super().__init__()\n        if isinstance(pad, int):\n            self.pad = tf.constant([[0, 0], [pad, pad], [pad, pad], [0, 0]])\n        else:  # tuple/list\n            self.pad = tf.constant([[0, 0], [pad[0], pad[0]], [pad[1], pad[1]], [0, 0]])\n    def call(self, inputs):\n        return tf.pad(inputs, self.pad, mode='constant', constant_values=0)",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFConv",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFConv(keras.layers.Layer):\n    # Standard convolution\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True, w=None):\n        # ch_in, ch_out, weights, kernel, stride, padding, groups\n        super().__init__()\n        assert g == 1, \"TF v2.2 Conv2D does not support 'groups' argument\"\n        # TensorFlow convolution padding is inconsistent with PyTorch (e.g. k=3 s=2 'SAME' padding)\n        # see https://stackoverflow.com/questions/52975843/comparing-conv2d-with-padding-between-tensorflow-and-pytorch\n        conv = keras.layers.Conv2D(\n            filters=c2,",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFDWConv",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFDWConv(keras.layers.Layer):\n    # Depthwise convolution\n    def __init__(self, c1, c2, k=1, s=1, p=None, act=True, w=None):\n        # ch_in, ch_out, weights, kernel, stride, padding, groups\n        super().__init__()\n        assert c2 % c1 == 0, f'TFDWConv() output={c2} must be a multiple of input={c1} channels'\n        conv = keras.layers.DepthwiseConv2D(\n            kernel_size=k,\n            depth_multiplier=c2 // c1,\n            strides=s,",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFDWConvTranspose2d",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFDWConvTranspose2d(keras.layers.Layer):\n    # Depthwise ConvTranspose2d\n    def __init__(self, c1, c2, k=1, s=1, p1=0, p2=0, w=None):\n        # ch_in, ch_out, weights, kernel, stride, padding, groups\n        super().__init__()\n        assert c1 == c2, f'TFDWConv() output={c2} must be equal to input={c1} channels'\n        assert k == 4 and p1 == 1, 'TFDWConv() only valid for k=4 and p1=1'\n        weight, bias = w.weight.permute(2, 3, 1, 0).numpy(), w.bias.numpy()\n        self.c1 = c1\n        self.conv = [",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFFocus",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFFocus(keras.layers.Layer):\n    # Focus wh information into c-space\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True, w=None):\n        # ch_in, ch_out, kernel, stride, padding, groups\n        super().__init__()\n        self.conv = TFConv(c1 * 4, c2, k, s, p, g, act, w.conv)\n    def call(self, inputs):  # x(b,w,h,c) -> y(b,w/2,h/2,4c)\n        # inputs = inputs / 255  # normalize 0-255 to 0-1\n        inputs = [inputs[:, ::2, ::2, :], inputs[:, 1::2, ::2, :], inputs[:, ::2, 1::2, :], inputs[:, 1::2, 1::2, :]]\n        return self.conv(tf.concat(inputs, 3))",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFBottleneck",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFBottleneck(keras.layers.Layer):\n    # Standard bottleneck\n    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5, w=None):  # ch_in, ch_out, shortcut, groups, expansion\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = TFConv(c1, c_, 1, 1, w=w.cv1)\n        self.cv2 = TFConv(c_, c2, 3, 1, g=g, w=w.cv2)\n        self.add = shortcut and c1 == c2\n    def call(self, inputs):\n        return inputs + self.cv2(self.cv1(inputs)) if self.add else self.cv2(self.cv1(inputs))",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFCrossConv",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFCrossConv(keras.layers.Layer):\n    # Cross Convolution\n    def __init__(self, c1, c2, k=3, s=1, g=1, e=1.0, shortcut=False, w=None):\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = TFConv(c1, c_, (1, k), (1, s), w=w.cv1)\n        self.cv2 = TFConv(c_, c2, (k, 1), (s, 1), g=g, w=w.cv2)\n        self.add = shortcut and c1 == c2\n    def call(self, inputs):\n        return inputs + self.cv2(self.cv1(inputs)) if self.add else self.cv2(self.cv1(inputs))",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFConv2d",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFConv2d(keras.layers.Layer):\n    # Substitution for PyTorch nn.Conv2D\n    def __init__(self, c1, c2, k, s=1, g=1, bias=True, w=None):\n        super().__init__()\n        assert g == 1, \"TF v2.2 Conv2D does not support 'groups' argument\"\n        self.conv = keras.layers.Conv2D(filters=c2,\n                                        kernel_size=k,\n                                        strides=s,\n                                        padding='VALID',\n                                        use_bias=bias,",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFBottleneckCSP",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFBottleneckCSP(keras.layers.Layer):\n    # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5, w=None):\n        # ch_in, ch_out, number, shortcut, groups, expansion\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = TFConv(c1, c_, 1, 1, w=w.cv1)\n        self.cv2 = TFConv2d(c1, c_, 1, 1, bias=False, w=w.cv2)\n        self.cv3 = TFConv2d(c_, c_, 1, 1, bias=False, w=w.cv3)\n        self.cv4 = TFConv(2 * c_, c2, 1, 1, w=w.cv4)",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFC3",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFC3(keras.layers.Layer):\n    # CSP Bottleneck with 3 convolutions\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5, w=None):\n        # ch_in, ch_out, number, shortcut, groups, expansion\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = TFConv(c1, c_, 1, 1, w=w.cv1)\n        self.cv2 = TFConv(c1, c_, 1, 1, w=w.cv2)\n        self.cv3 = TFConv(2 * c_, c2, 1, 1, w=w.cv3)\n        self.m = keras.Sequential([TFBottleneck(c_, c_, shortcut, g, e=1.0, w=w.m[j]) for j in range(n)])",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFC3x",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFC3x(keras.layers.Layer):\n    # 3 module with cross-convolutions\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5, w=None):\n        # ch_in, ch_out, number, shortcut, groups, expansion\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = TFConv(c1, c_, 1, 1, w=w.cv1)\n        self.cv2 = TFConv(c1, c_, 1, 1, w=w.cv2)\n        self.cv3 = TFConv(2 * c_, c2, 1, 1, w=w.cv3)\n        self.m = keras.Sequential([",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFSPP",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFSPP(keras.layers.Layer):\n    # Spatial pyramid pooling layer used in YOLOv3-SPP\n    def __init__(self, c1, c2, k=(5, 9, 13), w=None):\n        super().__init__()\n        c_ = c1 // 2  # hidden channels\n        self.cv1 = TFConv(c1, c_, 1, 1, w=w.cv1)\n        self.cv2 = TFConv(c_ * (len(k) + 1), c2, 1, 1, w=w.cv2)\n        self.m = [keras.layers.MaxPool2D(pool_size=x, strides=1, padding='SAME') for x in k]\n    def call(self, inputs):\n        x = self.cv1(inputs)",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFSPPF",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFSPPF(keras.layers.Layer):\n    # Spatial pyramid pooling-Fast layer\n    def __init__(self, c1, c2, k=5, w=None):\n        super().__init__()\n        c_ = c1 // 2  # hidden channels\n        self.cv1 = TFConv(c1, c_, 1, 1, w=w.cv1)\n        self.cv2 = TFConv(c_ * 4, c2, 1, 1, w=w.cv2)\n        self.m = keras.layers.MaxPool2D(pool_size=k, strides=1, padding='SAME')\n    def call(self, inputs):\n        x = self.cv1(inputs)",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFDetect",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFDetect(keras.layers.Layer):\n    # TF YOLOv5 Detect layer\n    def __init__(self, nc=80, anchors=(), ch=(), imgsz=(640, 640), w=None):  # detection layer\n        super().__init__()\n        self.stride = tf.convert_to_tensor(w.stride.numpy(), dtype=tf.float32)\n        self.nc = nc  # number of classes\n        self.no = nc + 5  # number of outputs per anchor\n        self.nl = len(anchors)  # number of detection layers\n        self.na = len(anchors[0]) // 2  # number of anchors\n        self.grid = [tf.zeros(1)] * self.nl  # init grid",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFSegment",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFSegment(TFDetect):\n    # YOLOv5 Segment head for segmentation models\n    def __init__(self, nc=80, anchors=(), nm=32, npr=256, ch=(), imgsz=(640, 640), w=None):\n        super().__init__(nc, anchors, ch, imgsz, w)\n        self.nm = nm  # number of masks\n        self.npr = npr  # number of protos\n        self.no = 5 + nc + self.nm  # number of outputs per anchor\n        self.m = [TFConv2d(x, self.no * self.na, 1, w=w.m[i]) for i, x in enumerate(ch)]  # output conv\n        self.proto = TFProto(ch[0], self.npr, self.nm, w=w.proto)  # protos\n        self.detect = TFDetect.call",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFProto",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFProto(keras.layers.Layer):\n    def __init__(self, c1, c_=256, c2=32, w=None):\n        super().__init__()\n        self.cv1 = TFConv(c1, c_, k=3, w=w.cv1)\n        self.upsample = TFUpsample(None, scale_factor=2, mode='nearest')\n        self.cv2 = TFConv(c_, c_, k=3, w=w.cv2)\n        self.cv3 = TFConv(c_, c2, w=w.cv3)\n    def call(self, inputs):\n        return self.cv3(self.cv2(self.upsample(self.cv1(inputs))))\nclass TFUpsample(keras.layers.Layer):",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFUpsample",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFUpsample(keras.layers.Layer):\n    # TF version of torch.nn.Upsample()\n    def __init__(self, size, scale_factor, mode, w=None):  # warning: all arguments needed including 'w'\n        super().__init__()\n        assert scale_factor % 2 == 0, 'scale_factor must be multiple of 2'\n        self.upsample = lambda x: tf.image.resize(x, (x.shape[1] * scale_factor, x.shape[2] * scale_factor), mode)\n        # self.upsample = keras.layers.UpSampling2D(size=scale_factor, interpolation=mode)\n        # with default arguments: align_corners=False, half_pixel_centers=False\n        # self.upsample = lambda x: tf.raw_ops.ResizeNearestNeighbor(images=x,\n        #                                                            size=(x.shape[1] * 2, x.shape[2] * 2))",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFConcat",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFConcat(keras.layers.Layer):\n    # TF version of torch.concat()\n    def __init__(self, dimension=1, w=None):\n        super().__init__()\n        assert dimension == 1, 'convert only NCHW to NHWC concat'\n        self.d = 3\n    def call(self, inputs):\n        return tf.concat(inputs, self.d)\ndef parse_model(d, ch, model, imgsz):  # model_dict, input_channels(3)\n    LOGGER.info(f\"\\n{'':>3}{'from':>18}{'n':>3}{'params':>10}  {'module':<40}{'arguments':<30}\")",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "TFModel",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class TFModel:\n    # TF YOLOv5 model\n    def __init__(self, cfg='yolov5s.yaml', ch=3, nc=None, model=None, imgsz=(640, 640)):  # model, channels, classes\n        super().__init__()\n        if isinstance(cfg, dict):\n            self.yaml = cfg  # model dict\n        else:  # is *.yaml\n            import yaml  # for torch hub\n            self.yaml_file = Path(cfg).name\n            with open(cfg) as f:",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "AgnosticNMS",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "class AgnosticNMS(keras.layers.Layer):\n    # TF Agnostic NMS\n    def call(self, input, topk_all, iou_thres, conf_thres):\n        # wrap map_fn to avoid TypeSpec related error https://stackoverflow.com/a/65809989/3036450\n        return tf.map_fn(lambda x: self._nms(x, topk_all, iou_thres, conf_thres),\n                         input,\n                         fn_output_signature=(tf.float32, tf.float32, tf.float32, tf.int32),\n                         name='agnostic_nms')\n    @staticmethod\n    def _nms(x, topk_all=100, iou_thres=0.45, conf_thres=0.25):  # agnostic NMS",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "parse_model",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "def parse_model(d, ch, model, imgsz):  # model_dict, input_channels(3)\n    LOGGER.info(f\"\\n{'':>3}{'from':>18}{'n':>3}{'params':>10}  {'module':<40}{'arguments':<30}\")\n    anchors, nc, gd, gw = d['anchors'], d['nc'], d['depth_multiple'], d['width_multiple']\n    na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors\n    no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)\n    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out\n    for i, (f, n, m, args) in enumerate(d['backbone'] + d['head']):  # from, number, module, args\n        m_str = m\n        m = eval(m) if isinstance(m, str) else m  # eval strings\n        for j, a in enumerate(args):",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "activations",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "def activations(act=nn.SiLU):\n    # Returns TF activation from input PyTorch activation\n    if isinstance(act, nn.LeakyReLU):\n        return lambda x: keras.activations.relu(x, alpha=0.1)\n    elif isinstance(act, nn.Hardswish):\n        return lambda x: x * tf.nn.relu6(x + 3) * 0.166666667\n    elif isinstance(act, (nn.SiLU, SiLU)):\n        return lambda x: keras.activations.swish(x)\n    else:\n        raise Exception(f'no matching TensorFlow activation found for PyTorch activation {act}')",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "representative_dataset_gen",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "def representative_dataset_gen(dataset, ncalib=100):\n    # Representative dataset generator for use with converter.representative_dataset, returns a generator of np arrays\n    for n, (path, img, im0s, vid_cap, string) in enumerate(dataset):\n        im = np.transpose(img, [1, 2, 0])\n        im = np.expand_dims(im, axis=0).astype(np.float32)\n        im /= 255\n        yield [im]\n        if n >= ncalib:\n            break\ndef run(",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "def run(\n        weights=ROOT / 'yolov5s.pt',  # weights path\n        imgsz=(640, 640),  # inference size h,w\n        batch_size=1,  # batch size\n        dynamic=False,  # dynamic batch size\n):\n    # PyTorch model\n    im = torch.zeros((batch_size, 3, *imgsz))  # BCHW image\n    model = attempt_load(weights, device=torch.device('cpu'), inplace=True, fuse=False)\n    _ = model(im)  # inference",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "parse_opt",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "def parse_opt():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--weights', type=str, default=ROOT / 'yolov5s.pt', help='weights path')\n    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')\n    parser.add_argument('--batch-size', type=int, default=1, help='batch size')\n    parser.add_argument('--dynamic', action='store_true', help='dynamic batch size')\n    opt = parser.parse_args()\n    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand\n    print_args(vars(opt))\n    return opt",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "def main(opt):\n    run(**vars(opt))\nif __name__ == '__main__':\n    opt = parse_opt()\n    main(opt)",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\n# ROOT = ROOT.relative_to(Path.cwd())  # relative\nimport numpy as np\nimport tensorflow as tf\nimport torch\nimport torch.nn as nn\nfrom tensorflow import keras",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.models.tf",
        "description": "recommendation_assets.yolov5.models.tf",
        "peekOfCode": "ROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\n# ROOT = ROOT.relative_to(Path.cwd())  # relative\nimport numpy as np\nimport tensorflow as tf\nimport torch\nimport torch.nn as nn\nfrom tensorflow import keras\nfrom models.common import (C3, SPP, SPPF, Bottleneck, BottleneckCSP, C3x, Concat, Conv, CrossConv, DWConv,",
        "detail": "recommendation_assets.yolov5.models.tf",
        "documentation": {}
    },
    {
        "label": "Detect",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.yolo",
        "description": "recommendation_assets.yolov5.models.yolo",
        "peekOfCode": "class Detect(nn.Module):\n    # YOLOv5 Detect head for detection models\n    stride = None  # strides computed during build\n    dynamic = False  # force grid reconstruction\n    export = False  # export mode\n    def __init__(self, nc=80, anchors=(), ch=(), inplace=True):  # detection layer\n        super().__init__()\n        self.nc = nc  # number of classes\n        self.no = nc + 5  # number of outputs per anchor\n        self.nl = len(anchors)  # number of detection layers",
        "detail": "recommendation_assets.yolov5.models.yolo",
        "documentation": {}
    },
    {
        "label": "Segment",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.yolo",
        "description": "recommendation_assets.yolov5.models.yolo",
        "peekOfCode": "class Segment(Detect):\n    # YOLOv5 Segment head for segmentation models\n    def __init__(self, nc=80, anchors=(), nm=32, npr=256, ch=(), inplace=True):\n        super().__init__(nc, anchors, ch, inplace)\n        self.nm = nm  # number of masks\n        self.npr = npr  # number of protos\n        self.no = 5 + nc + self.nm  # number of outputs per anchor\n        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv\n        self.proto = Proto(ch[0], self.npr, self.nm)  # protos\n        self.detect = Detect.forward",
        "detail": "recommendation_assets.yolov5.models.yolo",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.yolo",
        "description": "recommendation_assets.yolov5.models.yolo",
        "peekOfCode": "class BaseModel(nn.Module):\n    # YOLOv5 base model\n    def forward(self, x, profile=False, visualize=False):\n        return self._forward_once(x, profile, visualize)  # single-scale inference, train\n    def _forward_once(self, x, profile=False, visualize=False):\n        y, dt = [], []  # outputs\n        for m in self.model:\n            if m.f != -1:  # if not from previous layer\n                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers\n            if profile:",
        "detail": "recommendation_assets.yolov5.models.yolo",
        "documentation": {}
    },
    {
        "label": "DetectionModel",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.yolo",
        "description": "recommendation_assets.yolov5.models.yolo",
        "peekOfCode": "class DetectionModel(BaseModel):\n    # YOLOv5 detection model\n    def __init__(self, cfg='yolov5s.yaml', ch=3, nc=None, anchors=None):  # model, input channels, number of classes\n        super().__init__()\n        if isinstance(cfg, dict):\n            self.yaml = cfg  # model dict\n        else:  # is *.yaml\n            import yaml  # for torch hub\n            self.yaml_file = Path(cfg).name\n            with open(cfg, encoding='ascii', errors='ignore') as f:",
        "detail": "recommendation_assets.yolov5.models.yolo",
        "documentation": {}
    },
    {
        "label": "SegmentationModel",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.yolo",
        "description": "recommendation_assets.yolov5.models.yolo",
        "peekOfCode": "class SegmentationModel(DetectionModel):\n    # YOLOv5 segmentation model\n    def __init__(self, cfg='yolov5s-seg.yaml', ch=3, nc=None, anchors=None):\n        super().__init__(cfg, ch, nc, anchors)\nclass ClassificationModel(BaseModel):\n    # YOLOv5 classification model\n    def __init__(self, cfg=None, model=None, nc=1000, cutoff=10):  # yaml, model, number of classes, cutoff index\n        super().__init__()\n        self._from_detection_model(model, nc, cutoff) if model is not None else self._from_yaml(cfg)\n    def _from_detection_model(self, model, nc=1000, cutoff=10):",
        "detail": "recommendation_assets.yolov5.models.yolo",
        "documentation": {}
    },
    {
        "label": "ClassificationModel",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.models.yolo",
        "description": "recommendation_assets.yolov5.models.yolo",
        "peekOfCode": "class ClassificationModel(BaseModel):\n    # YOLOv5 classification model\n    def __init__(self, cfg=None, model=None, nc=1000, cutoff=10):  # yaml, model, number of classes, cutoff index\n        super().__init__()\n        self._from_detection_model(model, nc, cutoff) if model is not None else self._from_yaml(cfg)\n    def _from_detection_model(self, model, nc=1000, cutoff=10):\n        # Create a YOLOv5 classification model from a YOLOv5 detection model\n        if isinstance(model, DetectMultiBackend):\n            model = model.model  # unwrap DetectMultiBackend\n        model.model = model.model[:cutoff]  # backbone",
        "detail": "recommendation_assets.yolov5.models.yolo",
        "documentation": {}
    },
    {
        "label": "parse_model",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.models.yolo",
        "description": "recommendation_assets.yolov5.models.yolo",
        "peekOfCode": "def parse_model(d, ch):  # model_dict, input_channels(3)\n    # Parse a YOLOv5 model.yaml dictionary\n    LOGGER.info(f\"\\n{'':>3}{'from':>18}{'n':>3}{'params':>10}  {'module':<40}{'arguments':<30}\")\n    anchors, nc, gd, gw, act = d['anchors'], d['nc'], d['depth_multiple'], d['width_multiple'], d.get('activation')\n    if act:\n        Conv.default_act = eval(act)  # redefine default activation, i.e. Conv.default_act = nn.SiLU()\n        LOGGER.info(f\"{colorstr('activation:')} {act}\")  # print\n    na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors\n    no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)\n    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out",
        "detail": "recommendation_assets.yolov5.models.yolo",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.models.yolo",
        "description": "recommendation_assets.yolov5.models.yolo",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif platform.system() != 'Windows':\n    ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom models.common import *  # noqa\nfrom models.experimental import *  # noqa\nfrom utils.autoanchor import check_anchor_order\nfrom utils.general import LOGGER, check_version, check_yaml, make_divisible, print_args",
        "detail": "recommendation_assets.yolov5.models.yolo",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.models.yolo",
        "description": "recommendation_assets.yolov5.models.yolo",
        "peekOfCode": "ROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif platform.system() != 'Windows':\n    ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom models.common import *  # noqa\nfrom models.experimental import *  # noqa\nfrom utils.autoanchor import check_anchor_order\nfrom utils.general import LOGGER, check_version, check_yaml, make_divisible, print_args\nfrom utils.plots import feature_visualization",
        "detail": "recommendation_assets.yolov5.models.yolo",
        "documentation": {}
    },
    {
        "label": "Model",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.models.yolo",
        "description": "recommendation_assets.yolov5.models.yolo",
        "peekOfCode": "Model = DetectionModel  # retain YOLOv5 'Model' class for backwards compatibility\nclass SegmentationModel(DetectionModel):\n    # YOLOv5 segmentation model\n    def __init__(self, cfg='yolov5s-seg.yaml', ch=3, nc=None, anchors=None):\n        super().__init__(cfg, ch, nc, anchors)\nclass ClassificationModel(BaseModel):\n    # YOLOv5 classification model\n    def __init__(self, cfg=None, model=None, nc=1000, cutoff=10):  # yaml, model, number of classes, cutoff index\n        super().__init__()\n        self._from_detection_model(model, nc, cutoff) if model is not None else self._from_yaml(cfg)",
        "detail": "recommendation_assets.yolov5.models.yolo",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.segment.predict",
        "description": "recommendation_assets.yolov5.segment.predict",
        "peekOfCode": "def run(\n    weights=ROOT / 'yolov5s-seg.pt',  # model.pt path(s)\n    source=ROOT / 'data/images',  # file/dir/URL/glob/screen/0(webcam)\n    data=ROOT / 'data/coco128.yaml',  # dataset.yaml path\n    imgsz=(640, 640),  # inference size (height, width)\n    conf_thres=0.25,  # confidence threshold\n    iou_thres=0.45,  # NMS IOU threshold\n    max_det=1000,  # maximum detections per image\n    device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n    view_img=False,  # show results",
        "detail": "recommendation_assets.yolov5.segment.predict",
        "documentation": {}
    },
    {
        "label": "parse_opt",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.segment.predict",
        "description": "recommendation_assets.yolov5.segment.predict",
        "peekOfCode": "def parse_opt():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s-seg.pt', help='model path(s)')\n    parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob/screen/0(webcam)')\n    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) dataset.yaml path')\n    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')\n    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')\n    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')\n    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')\n    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')",
        "detail": "recommendation_assets.yolov5.segment.predict",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.segment.predict",
        "description": "recommendation_assets.yolov5.segment.predict",
        "peekOfCode": "def main(opt):\n    check_requirements(ROOT / 'requirements.txt', exclude=('tensorboard', 'thop'))\n    run(**vars(opt))\nif __name__ == '__main__':\n    opt = parse_opt()\n    main(opt)",
        "detail": "recommendation_assets.yolov5.segment.predict",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.segment.predict",
        "description": "recommendation_assets.yolov5.segment.predict",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom ultralytics.utils.plotting import Annotator, colors, save_one_box\nfrom models.common import DetectMultiBackend\nfrom utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\nfrom utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n                           increment_path, non_max_suppression, print_args, scale_boxes, scale_segments,",
        "detail": "recommendation_assets.yolov5.segment.predict",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.segment.predict",
        "description": "recommendation_assets.yolov5.segment.predict",
        "peekOfCode": "ROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom ultralytics.utils.plotting import Annotator, colors, save_one_box\nfrom models.common import DetectMultiBackend\nfrom utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\nfrom utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n                           increment_path, non_max_suppression, print_args, scale_boxes, scale_segments,\n                           strip_optimizer)",
        "detail": "recommendation_assets.yolov5.segment.predict",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.segment.predict",
        "description": "recommendation_assets.yolov5.segment.predict",
        "peekOfCode": "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom ultralytics.utils.plotting import Annotator, colors, save_one_box\nfrom models.common import DetectMultiBackend\nfrom utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\nfrom utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n                           increment_path, non_max_suppression, print_args, scale_boxes, scale_segments,\n                           strip_optimizer)\nfrom utils.segment.general import masks2segments, process_mask, process_mask_native\nfrom utils.torch_utils import select_device, smart_inference_mode\n@smart_inference_mode()",
        "detail": "recommendation_assets.yolov5.segment.predict",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.segment.train",
        "description": "recommendation_assets.yolov5.segment.train",
        "peekOfCode": "def train(hyp, opt, device, callbacks):  # hyp is path/to/hyp.yaml or hyp dictionary\n    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze, mask_ratio = \\\n        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \\\n        opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze, opt.mask_ratio\n    # callbacks.run('on_pretrain_routine_start')\n    # Directories\n    w = save_dir / 'weights'  # weights dir\n    (w.parent if evolve else w).mkdir(parents=True, exist_ok=True)  # make dir\n    last, best = w / 'last.pt', w / 'best.pt'\n    # Hyperparameters",
        "detail": "recommendation_assets.yolov5.segment.train",
        "documentation": {}
    },
    {
        "label": "parse_opt",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.segment.train",
        "description": "recommendation_assets.yolov5.segment.train",
        "peekOfCode": "def parse_opt(known=False):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--weights', type=str, default=ROOT / 'yolov5s-seg.pt', help='initial weights path')\n    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')\n    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128-seg.yaml', help='dataset.yaml path')\n    parser.add_argument('--hyp', type=str, default=ROOT / 'data/hyps/hyp.scratch-low.yaml', help='hyperparameters path')\n    parser.add_argument('--epochs', type=int, default=100, help='total training epochs')\n    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs, -1 for autobatch')\n    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')\n    parser.add_argument('--rect', action='store_true', help='rectangular training')",
        "detail": "recommendation_assets.yolov5.segment.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.segment.train",
        "description": "recommendation_assets.yolov5.segment.train",
        "peekOfCode": "def main(opt, callbacks=Callbacks()):\n    # Checks\n    if RANK in {-1, 0}:\n        print_args(vars(opt))\n        check_git_status()\n        check_requirements(ROOT / 'requirements.txt')\n    # Resume\n    if opt.resume and not opt.evolve:  # resume from specified or most recent last.pt\n        last = Path(check_file(opt.resume) if isinstance(opt.resume, str) else get_latest_run())\n        opt_yaml = last.parent.parent / 'opt.yaml'  # train options yaml",
        "detail": "recommendation_assets.yolov5.segment.train",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.segment.train",
        "description": "recommendation_assets.yolov5.segment.train",
        "peekOfCode": "def run(**kwargs):\n    # Usage: import train; train.run(data='coco128.yaml', imgsz=320, weights='yolov5m.pt')\n    opt = parse_opt(True)\n    for k, v in kwargs.items():\n        setattr(opt, k, v)\n    main(opt)\n    return opt\nif __name__ == '__main__':\n    opt = parse_opt()\n    main(opt)",
        "detail": "recommendation_assets.yolov5.segment.train",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.segment.train",
        "description": "recommendation_assets.yolov5.segment.train",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nimport segment.val as validate  # for end-of-epoch mAP\nfrom models.experimental import attempt_load\nfrom models.yolo import SegmentationModel\nfrom utils.autoanchor import check_anchors\nfrom utils.autobatch import check_train_batch_size",
        "detail": "recommendation_assets.yolov5.segment.train",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.segment.train",
        "description": "recommendation_assets.yolov5.segment.train",
        "peekOfCode": "ROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nimport segment.val as validate  # for end-of-epoch mAP\nfrom models.experimental import attempt_load\nfrom models.yolo import SegmentationModel\nfrom utils.autoanchor import check_anchors\nfrom utils.autobatch import check_train_batch_size\nfrom utils.callbacks import Callbacks",
        "detail": "recommendation_assets.yolov5.segment.train",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.segment.train",
        "description": "recommendation_assets.yolov5.segment.train",
        "peekOfCode": "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nimport segment.val as validate  # for end-of-epoch mAP\nfrom models.experimental import attempt_load\nfrom models.yolo import SegmentationModel\nfrom utils.autoanchor import check_anchors\nfrom utils.autobatch import check_train_batch_size\nfrom utils.callbacks import Callbacks\nfrom utils.downloads import attempt_download, is_url\nfrom utils.general import (LOGGER, TQDM_BAR_FORMAT, check_amp, check_dataset, check_file, check_git_info,\n                           check_git_status, check_img_size, check_requirements, check_suffix, check_yaml, colorstr,",
        "detail": "recommendation_assets.yolov5.segment.train",
        "documentation": {}
    },
    {
        "label": "LOCAL_RANK",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.segment.train",
        "description": "recommendation_assets.yolov5.segment.train",
        "peekOfCode": "LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html\nRANK = int(os.getenv('RANK', -1))\nWORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))\nGIT_INFO = check_git_info()\ndef train(hyp, opt, device, callbacks):  # hyp is path/to/hyp.yaml or hyp dictionary\n    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze, mask_ratio = \\\n        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \\\n        opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze, opt.mask_ratio\n    # callbacks.run('on_pretrain_routine_start')\n    # Directories",
        "detail": "recommendation_assets.yolov5.segment.train",
        "documentation": {}
    },
    {
        "label": "RANK",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.segment.train",
        "description": "recommendation_assets.yolov5.segment.train",
        "peekOfCode": "RANK = int(os.getenv('RANK', -1))\nWORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))\nGIT_INFO = check_git_info()\ndef train(hyp, opt, device, callbacks):  # hyp is path/to/hyp.yaml or hyp dictionary\n    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze, mask_ratio = \\\n        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \\\n        opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze, opt.mask_ratio\n    # callbacks.run('on_pretrain_routine_start')\n    # Directories\n    w = save_dir / 'weights'  # weights dir",
        "detail": "recommendation_assets.yolov5.segment.train",
        "documentation": {}
    },
    {
        "label": "WORLD_SIZE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.segment.train",
        "description": "recommendation_assets.yolov5.segment.train",
        "peekOfCode": "WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))\nGIT_INFO = check_git_info()\ndef train(hyp, opt, device, callbacks):  # hyp is path/to/hyp.yaml or hyp dictionary\n    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze, mask_ratio = \\\n        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \\\n        opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze, opt.mask_ratio\n    # callbacks.run('on_pretrain_routine_start')\n    # Directories\n    w = save_dir / 'weights'  # weights dir\n    (w.parent if evolve else w).mkdir(parents=True, exist_ok=True)  # make dir",
        "detail": "recommendation_assets.yolov5.segment.train",
        "documentation": {}
    },
    {
        "label": "GIT_INFO",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.segment.train",
        "description": "recommendation_assets.yolov5.segment.train",
        "peekOfCode": "GIT_INFO = check_git_info()\ndef train(hyp, opt, device, callbacks):  # hyp is path/to/hyp.yaml or hyp dictionary\n    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze, mask_ratio = \\\n        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \\\n        opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze, opt.mask_ratio\n    # callbacks.run('on_pretrain_routine_start')\n    # Directories\n    w = save_dir / 'weights'  # weights dir\n    (w.parent if evolve else w).mkdir(parents=True, exist_ok=True)  # make dir\n    last, best = w / 'last.pt', w / 'best.pt'",
        "detail": "recommendation_assets.yolov5.segment.train",
        "documentation": {}
    },
    {
        "label": "save_one_txt",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.segment.val",
        "description": "recommendation_assets.yolov5.segment.val",
        "peekOfCode": "def save_one_txt(predn, save_conf, shape, file):\n    # Save one txt result\n    gn = torch.tensor(shape)[[1, 0, 1, 0]]  # normalization gain whwh\n    for *xyxy, conf, cls in predn.tolist():\n        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n        with open(file, 'a') as f:\n            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\ndef save_one_json(predn, jdict, path, class_map, pred_masks):\n    # Save one JSON result {\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}",
        "detail": "recommendation_assets.yolov5.segment.val",
        "documentation": {}
    },
    {
        "label": "save_one_json",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.segment.val",
        "description": "recommendation_assets.yolov5.segment.val",
        "peekOfCode": "def save_one_json(predn, jdict, path, class_map, pred_masks):\n    # Save one JSON result {\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}\n    from pycocotools.mask import encode\n    def single_encode(x):\n        rle = encode(np.asarray(x[:, :, None], order='F', dtype='uint8'))[0]\n        rle['counts'] = rle['counts'].decode('utf-8')\n        return rle\n    image_id = int(path.stem) if path.stem.isnumeric() else path.stem\n    box = xyxy2xywh(predn[:, :4])  # xywh\n    box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner",
        "detail": "recommendation_assets.yolov5.segment.val",
        "documentation": {}
    },
    {
        "label": "process_batch",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.segment.val",
        "description": "recommendation_assets.yolov5.segment.val",
        "peekOfCode": "def process_batch(detections, labels, iouv, pred_masks=None, gt_masks=None, overlap=False, masks=False):\n    \"\"\"\n    Return correct prediction matrix\n    Arguments:\n        detections (array[N, 6]), x1, y1, x2, y2, conf, class\n        labels (array[M, 5]), class, x1, y1, x2, y2\n    Returns:\n        correct (array[N, 10]), for 10 IoU levels\n    \"\"\"\n    if masks:",
        "detail": "recommendation_assets.yolov5.segment.val",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.segment.val",
        "description": "recommendation_assets.yolov5.segment.val",
        "peekOfCode": "def run(\n        data,\n        weights=None,  # model.pt path(s)\n        batch_size=32,  # batch size\n        imgsz=640,  # inference size (pixels)\n        conf_thres=0.001,  # confidence threshold\n        iou_thres=0.6,  # NMS IoU threshold\n        max_det=300,  # maximum detections per image\n        task='val',  # train, val, test, speed or study\n        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu",
        "detail": "recommendation_assets.yolov5.segment.val",
        "documentation": {}
    },
    {
        "label": "parse_opt",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.segment.val",
        "description": "recommendation_assets.yolov5.segment.val",
        "peekOfCode": "def parse_opt():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128-seg.yaml', help='dataset.yaml path')\n    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s-seg.pt', help='model path(s)')\n    parser.add_argument('--batch-size', type=int, default=32, help='batch size')\n    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='inference size (pixels)')\n    parser.add_argument('--conf-thres', type=float, default=0.001, help='confidence threshold')\n    parser.add_argument('--iou-thres', type=float, default=0.6, help='NMS IoU threshold')\n    parser.add_argument('--max-det', type=int, default=300, help='maximum detections per image')\n    parser.add_argument('--task', default='val', help='train, val, test, speed or study')",
        "detail": "recommendation_assets.yolov5.segment.val",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.segment.val",
        "description": "recommendation_assets.yolov5.segment.val",
        "peekOfCode": "def main(opt):\n    check_requirements(ROOT / 'requirements.txt', exclude=('tensorboard', 'thop'))\n    if opt.task in ('train', 'val', 'test'):  # run normally\n        if opt.conf_thres > 0.001:  # https://github.com/ultralytics/yolov5/issues/1466\n            LOGGER.warning(f'WARNING  confidence threshold {opt.conf_thres} > 0.001 produces invalid results')\n        if opt.save_hybrid:\n            LOGGER.warning('WARNING  --save-hybrid returns high mAP from hybrid labels, not from predictions alone')\n        run(**vars(opt))\n    else:\n        weights = opt.weights if isinstance(opt.weights, list) else [opt.weights]",
        "detail": "recommendation_assets.yolov5.segment.val",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.segment.val",
        "description": "recommendation_assets.yolov5.segment.val",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nimport torch.nn.functional as F\nfrom models.common import DetectMultiBackend\nfrom models.yolo import SegmentationModel\nfrom utils.callbacks import Callbacks\nfrom utils.general import (LOGGER, NUM_THREADS, TQDM_BAR_FORMAT, Profile, check_dataset, check_img_size,",
        "detail": "recommendation_assets.yolov5.segment.val",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.segment.val",
        "description": "recommendation_assets.yolov5.segment.val",
        "peekOfCode": "ROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nimport torch.nn.functional as F\nfrom models.common import DetectMultiBackend\nfrom models.yolo import SegmentationModel\nfrom utils.callbacks import Callbacks\nfrom utils.general import (LOGGER, NUM_THREADS, TQDM_BAR_FORMAT, Profile, check_dataset, check_img_size,\n                           check_requirements, check_yaml, coco80_to_coco91_class, colorstr, increment_path,",
        "detail": "recommendation_assets.yolov5.segment.val",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.segment.val",
        "description": "recommendation_assets.yolov5.segment.val",
        "peekOfCode": "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nimport torch.nn.functional as F\nfrom models.common import DetectMultiBackend\nfrom models.yolo import SegmentationModel\nfrom utils.callbacks import Callbacks\nfrom utils.general import (LOGGER, NUM_THREADS, TQDM_BAR_FORMAT, Profile, check_dataset, check_img_size,\n                           check_requirements, check_yaml, coco80_to_coco91_class, colorstr, increment_path,\n                           non_max_suppression, print_args, scale_boxes, xywh2xyxy, xyxy2xywh)\nfrom utils.metrics import ConfusionMatrix, box_iou\nfrom utils.plots import output_to_target, plot_val_study",
        "detail": "recommendation_assets.yolov5.segment.val",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.aws.resume",
        "description": "recommendation_assets.yolov5.utils.aws.resume",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[2]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nport = 0  # --master_port\npath = Path('').resolve()\nfor last in path.rglob('*/**/last.pt'):\n    ckpt = torch.load(last)\n    if ckpt['optimizer'] is None:\n        continue",
        "detail": "recommendation_assets.yolov5.utils.aws.resume",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.aws.resume",
        "description": "recommendation_assets.yolov5.utils.aws.resume",
        "peekOfCode": "ROOT = FILE.parents[2]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nport = 0  # --master_port\npath = Path('').resolve()\nfor last in path.rglob('*/**/last.pt'):\n    ckpt = torch.load(last)\n    if ckpt['optimizer'] is None:\n        continue\n    # Load opt.yaml",
        "detail": "recommendation_assets.yolov5.utils.aws.resume",
        "documentation": {}
    },
    {
        "label": "port",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.aws.resume",
        "description": "recommendation_assets.yolov5.utils.aws.resume",
        "peekOfCode": "port = 0  # --master_port\npath = Path('').resolve()\nfor last in path.rglob('*/**/last.pt'):\n    ckpt = torch.load(last)\n    if ckpt['optimizer'] is None:\n        continue\n    # Load opt.yaml\n    with open(last.parent.parent / 'opt.yaml', errors='ignore') as f:\n        opt = yaml.safe_load(f)\n    # Get device count",
        "detail": "recommendation_assets.yolov5.utils.aws.resume",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.aws.resume",
        "description": "recommendation_assets.yolov5.utils.aws.resume",
        "peekOfCode": "path = Path('').resolve()\nfor last in path.rglob('*/**/last.pt'):\n    ckpt = torch.load(last)\n    if ckpt['optimizer'] is None:\n        continue\n    # Load opt.yaml\n    with open(last.parent.parent / 'opt.yaml', errors='ignore') as f:\n        opt = yaml.safe_load(f)\n    # Get device count\n    d = opt['device'].split(',')  # devices",
        "detail": "recommendation_assets.yolov5.utils.aws.resume",
        "documentation": {}
    },
    {
        "label": "DETECTION_URL",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.flask_rest_api.example_request",
        "description": "recommendation_assets.yolov5.utils.flask_rest_api.example_request",
        "peekOfCode": "DETECTION_URL = 'http://localhost:5000/v1/object-detection/yolov5s'\nIMAGE = 'zidane.jpg'\n# Read image\nwith open(IMAGE, 'rb') as f:\n    image_data = f.read()\nresponse = requests.post(DETECTION_URL, files={'image': image_data}).json()\npprint.pprint(response)",
        "detail": "recommendation_assets.yolov5.utils.flask_rest_api.example_request",
        "documentation": {}
    },
    {
        "label": "IMAGE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.flask_rest_api.example_request",
        "description": "recommendation_assets.yolov5.utils.flask_rest_api.example_request",
        "peekOfCode": "IMAGE = 'zidane.jpg'\n# Read image\nwith open(IMAGE, 'rb') as f:\n    image_data = f.read()\nresponse = requests.post(DETECTION_URL, files={'image': image_data}).json()\npprint.pprint(response)",
        "detail": "recommendation_assets.yolov5.utils.flask_rest_api.example_request",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.flask_rest_api.example_request",
        "description": "recommendation_assets.yolov5.utils.flask_rest_api.example_request",
        "peekOfCode": "response = requests.post(DETECTION_URL, files={'image': image_data}).json()\npprint.pprint(response)",
        "detail": "recommendation_assets.yolov5.utils.flask_rest_api.example_request",
        "documentation": {}
    },
    {
        "label": "predict",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.flask_rest_api.restapi",
        "description": "recommendation_assets.yolov5.utils.flask_rest_api.restapi",
        "peekOfCode": "def predict(model):\n    if request.method != 'POST':\n        return\n    if request.files.get('image'):\n        # Method 1\n        # with request.files[\"image\"] as f:\n        #     im = Image.open(io.BytesIO(f.read()))\n        # Method 2\n        im_file = request.files['image']\n        im_bytes = im_file.read()",
        "detail": "recommendation_assets.yolov5.utils.flask_rest_api.restapi",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.flask_rest_api.restapi",
        "description": "recommendation_assets.yolov5.utils.flask_rest_api.restapi",
        "peekOfCode": "app = Flask(__name__)\nmodels = {}\nDETECTION_URL = '/v1/object-detection/<model>'\n@app.route(DETECTION_URL, methods=['POST'])\ndef predict(model):\n    if request.method != 'POST':\n        return\n    if request.files.get('image'):\n        # Method 1\n        # with request.files[\"image\"] as f:",
        "detail": "recommendation_assets.yolov5.utils.flask_rest_api.restapi",
        "documentation": {}
    },
    {
        "label": "models",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.flask_rest_api.restapi",
        "description": "recommendation_assets.yolov5.utils.flask_rest_api.restapi",
        "peekOfCode": "models = {}\nDETECTION_URL = '/v1/object-detection/<model>'\n@app.route(DETECTION_URL, methods=['POST'])\ndef predict(model):\n    if request.method != 'POST':\n        return\n    if request.files.get('image'):\n        # Method 1\n        # with request.files[\"image\"] as f:\n        #     im = Image.open(io.BytesIO(f.read()))",
        "detail": "recommendation_assets.yolov5.utils.flask_rest_api.restapi",
        "documentation": {}
    },
    {
        "label": "DETECTION_URL",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.flask_rest_api.restapi",
        "description": "recommendation_assets.yolov5.utils.flask_rest_api.restapi",
        "peekOfCode": "DETECTION_URL = '/v1/object-detection/<model>'\n@app.route(DETECTION_URL, methods=['POST'])\ndef predict(model):\n    if request.method != 'POST':\n        return\n    if request.files.get('image'):\n        # Method 1\n        # with request.files[\"image\"] as f:\n        #     im = Image.open(io.BytesIO(f.read()))\n        # Method 2",
        "detail": "recommendation_assets.yolov5.utils.flask_rest_api.restapi",
        "documentation": {}
    },
    {
        "label": "ClearmlLogger",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.loggers.clearml.clearml_utils",
        "description": "recommendation_assets.yolov5.utils.loggers.clearml.clearml_utils",
        "peekOfCode": "class ClearmlLogger:\n    \"\"\"Log training runs, datasets, models, and predictions to ClearML.\n    This logger sends information to ClearML at app.clear.ml or to your own hosted server. By default,\n    this information includes hyperparameters, system configuration and metrics, model metrics, code information and\n    basic data metrics and analyses.\n    By providing additional command line arguments to train.py, datasets,\n    models and predictions can also be logged.\n    \"\"\"\n    def __init__(self, opt, hyp):\n        \"\"\"",
        "detail": "recommendation_assets.yolov5.utils.loggers.clearml.clearml_utils",
        "documentation": {}
    },
    {
        "label": "construct_dataset",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.loggers.clearml.clearml_utils",
        "description": "recommendation_assets.yolov5.utils.loggers.clearml.clearml_utils",
        "peekOfCode": "def construct_dataset(clearml_info_string):\n    \"\"\"Load in a clearml dataset and fill the internal data_dict with its contents.\n    \"\"\"\n    dataset_id = clearml_info_string.replace('clearml://', '')\n    dataset = Dataset.get(dataset_id=dataset_id)\n    dataset_root_path = Path(dataset.get_local_copy())\n    # We'll search for the yaml file definition in the dataset\n    yaml_filenames = list(glob.glob(str(dataset_root_path / '*.yaml')) + glob.glob(str(dataset_root_path / '*.yml')))\n    if len(yaml_filenames) > 1:\n        raise ValueError('More than one yaml file was found in the dataset root, cannot determine which one contains '",
        "detail": "recommendation_assets.yolov5.utils.loggers.clearml.clearml_utils",
        "documentation": {}
    },
    {
        "label": "task",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.loggers.clearml.hpo",
        "description": "recommendation_assets.yolov5.utils.loggers.clearml.hpo",
        "peekOfCode": "task = Task.init(project_name='Hyper-Parameter Optimization',\n                 task_name='YOLOv5',\n                 task_type=Task.TaskTypes.optimizer,\n                 reuse_last_task_id=False)\n# Example use case:\noptimizer = HyperParameterOptimizer(\n    # This is the experiment we want to optimize\n    base_task_id='<your_template_task_id>',\n    # here we define the hyper-parameters to optimize\n    # Notice: The parameter name should exactly match what you see in the UI: <section_name>/<parameter>",
        "detail": "recommendation_assets.yolov5.utils.loggers.clearml.hpo",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.loggers.clearml.hpo",
        "description": "recommendation_assets.yolov5.utils.loggers.clearml.hpo",
        "peekOfCode": "optimizer = HyperParameterOptimizer(\n    # This is the experiment we want to optimize\n    base_task_id='<your_template_task_id>',\n    # here we define the hyper-parameters to optimize\n    # Notice: The parameter name should exactly match what you see in the UI: <section_name>/<parameter>\n    # For Example, here we see in the base experiment a section Named: \"General\"\n    # under it a parameter named \"batch_size\", this becomes \"General/batch_size\"\n    # If you have `argparse` for example, then arguments will appear under the \"Args\" section,\n    # and you should instead pass \"Args/batch_size\"\n    hyper_parameters=[",
        "detail": "recommendation_assets.yolov5.utils.loggers.clearml.hpo",
        "documentation": {}
    },
    {
        "label": "download_model_checkpoint",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "description": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "peekOfCode": "def download_model_checkpoint(opt, experiment):\n    model_dir = f'{opt.project}/{experiment.name}'\n    os.makedirs(model_dir, exist_ok=True)\n    model_name = COMET_MODEL_NAME\n    model_asset_list = experiment.get_model_asset_list(model_name)\n    if len(model_asset_list) == 0:\n        logger.error(f'COMET ERROR: No checkpoints found for model name : {model_name}')\n        return\n    model_asset_list = sorted(\n        model_asset_list,",
        "detail": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "documentation": {}
    },
    {
        "label": "set_opt_parameters",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "description": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "peekOfCode": "def set_opt_parameters(opt, experiment):\n    \"\"\"Update the opts Namespace with parameters\n    from Comet's ExistingExperiment when resuming a run\n    Args:\n        opt (argparse.Namespace): Namespace of command line options\n        experiment (comet_ml.APIExperiment): Comet API Experiment object\n    \"\"\"\n    asset_list = experiment.get_asset_list()\n    resume_string = opt.resume\n    for asset in asset_list:",
        "detail": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "documentation": {}
    },
    {
        "label": "check_comet_weights",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "description": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "peekOfCode": "def check_comet_weights(opt):\n    \"\"\"Downloads model weights from Comet and updates the\n    weights path to point to saved weights location\n    Args:\n        opt (argparse.Namespace): Command Line arguments passed\n            to YOLOv5 training script\n    Returns:\n        None/bool: Return True if weights are successfully downloaded\n            else return None\n    \"\"\"",
        "detail": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "documentation": {}
    },
    {
        "label": "check_comet_resume",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "description": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "peekOfCode": "def check_comet_resume(opt):\n    \"\"\"Restores run parameters to its original state based on the model checkpoint\n    and logged Experiment parameters.\n    Args:\n        opt (argparse.Namespace): Command Line arguments passed\n            to YOLOv5 training script\n    Returns:\n        None/bool: Return True if the run is restored successfully\n            else return None\n    \"\"\"",
        "detail": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "description": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "peekOfCode": "logger = logging.getLogger(__name__)\nCOMET_PREFIX = 'comet://'\nCOMET_MODEL_NAME = os.getenv('COMET_MODEL_NAME', 'yolov5')\nCOMET_DEFAULT_CHECKPOINT_FILENAME = os.getenv('COMET_DEFAULT_CHECKPOINT_FILENAME', 'last.pt')\ndef download_model_checkpoint(opt, experiment):\n    model_dir = f'{opt.project}/{experiment.name}'\n    os.makedirs(model_dir, exist_ok=True)\n    model_name = COMET_MODEL_NAME\n    model_asset_list = experiment.get_model_asset_list(model_name)\n    if len(model_asset_list) == 0:",
        "detail": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "documentation": {}
    },
    {
        "label": "COMET_PREFIX",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "description": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "peekOfCode": "COMET_PREFIX = 'comet://'\nCOMET_MODEL_NAME = os.getenv('COMET_MODEL_NAME', 'yolov5')\nCOMET_DEFAULT_CHECKPOINT_FILENAME = os.getenv('COMET_DEFAULT_CHECKPOINT_FILENAME', 'last.pt')\ndef download_model_checkpoint(opt, experiment):\n    model_dir = f'{opt.project}/{experiment.name}'\n    os.makedirs(model_dir, exist_ok=True)\n    model_name = COMET_MODEL_NAME\n    model_asset_list = experiment.get_model_asset_list(model_name)\n    if len(model_asset_list) == 0:\n        logger.error(f'COMET ERROR: No checkpoints found for model name : {model_name}')",
        "detail": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "documentation": {}
    },
    {
        "label": "COMET_MODEL_NAME",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "description": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "peekOfCode": "COMET_MODEL_NAME = os.getenv('COMET_MODEL_NAME', 'yolov5')\nCOMET_DEFAULT_CHECKPOINT_FILENAME = os.getenv('COMET_DEFAULT_CHECKPOINT_FILENAME', 'last.pt')\ndef download_model_checkpoint(opt, experiment):\n    model_dir = f'{opt.project}/{experiment.name}'\n    os.makedirs(model_dir, exist_ok=True)\n    model_name = COMET_MODEL_NAME\n    model_asset_list = experiment.get_model_asset_list(model_name)\n    if len(model_asset_list) == 0:\n        logger.error(f'COMET ERROR: No checkpoints found for model name : {model_name}')\n        return",
        "detail": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "documentation": {}
    },
    {
        "label": "COMET_DEFAULT_CHECKPOINT_FILENAME",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "description": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "peekOfCode": "COMET_DEFAULT_CHECKPOINT_FILENAME = os.getenv('COMET_DEFAULT_CHECKPOINT_FILENAME', 'last.pt')\ndef download_model_checkpoint(opt, experiment):\n    model_dir = f'{opt.project}/{experiment.name}'\n    os.makedirs(model_dir, exist_ok=True)\n    model_name = COMET_MODEL_NAME\n    model_asset_list = experiment.get_model_asset_list(model_name)\n    if len(model_asset_list) == 0:\n        logger.error(f'COMET ERROR: No checkpoints found for model name : {model_name}')\n        return\n    model_asset_list = sorted(",
        "detail": "recommendation_assets.yolov5.utils.loggers.comet.comet_utils",
        "documentation": {}
    },
    {
        "label": "get_args",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "description": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "peekOfCode": "def get_args(known=False):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--weights', type=str, default=ROOT / 'yolov5s.pt', help='initial weights path')\n    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')\n    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')\n    parser.add_argument('--hyp', type=str, default=ROOT / 'data/hyps/hyp.scratch-low.yaml', help='hyperparameters path')\n    parser.add_argument('--epochs', type=int, default=300, help='total training epochs')\n    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs, -1 for autobatch')\n    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')\n    parser.add_argument('--rect', action='store_true', help='rectangular training')",
        "detail": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "description": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "peekOfCode": "def run(parameters, opt):\n    hyp_dict = {k: v for k, v in parameters.items() if k not in ['epochs', 'batch_size']}\n    opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok or opt.evolve))\n    opt.batch_size = parameters.get('batch_size')\n    opt.epochs = parameters.get('epochs')\n    device = select_device(opt.device, batch_size=opt.batch_size)\n    train(hyp_dict, opt, device, callbacks=Callbacks())\nif __name__ == '__main__':\n    opt = get_args(known=True)\n    opt.weights = str(opt.weights)",
        "detail": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "description": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "peekOfCode": "logger = logging.getLogger(__name__)\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[3]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nfrom train import train\nfrom utils.callbacks import Callbacks\nfrom utils.general import increment_path\nfrom utils.torch_utils import select_device\n# Project Configuration",
        "detail": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "description": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[3]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nfrom train import train\nfrom utils.callbacks import Callbacks\nfrom utils.general import increment_path\nfrom utils.torch_utils import select_device\n# Project Configuration\nconfig = comet_ml.config.get_config()",
        "detail": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "description": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "peekOfCode": "ROOT = FILE.parents[3]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nfrom train import train\nfrom utils.callbacks import Callbacks\nfrom utils.general import increment_path\nfrom utils.torch_utils import select_device\n# Project Configuration\nconfig = comet_ml.config.get_config()\nCOMET_PROJECT_NAME = config.get_string(os.getenv('COMET_PROJECT_NAME'), 'comet.project_name', default='yolov5')",
        "detail": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "description": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "peekOfCode": "config = comet_ml.config.get_config()\nCOMET_PROJECT_NAME = config.get_string(os.getenv('COMET_PROJECT_NAME'), 'comet.project_name', default='yolov5')\ndef get_args(known=False):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--weights', type=str, default=ROOT / 'yolov5s.pt', help='initial weights path')\n    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')\n    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')\n    parser.add_argument('--hyp', type=str, default=ROOT / 'data/hyps/hyp.scratch-low.yaml', help='hyperparameters path')\n    parser.add_argument('--epochs', type=int, default=300, help='total training epochs')\n    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs, -1 for autobatch')",
        "detail": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "documentation": {}
    },
    {
        "label": "COMET_PROJECT_NAME",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "description": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "peekOfCode": "COMET_PROJECT_NAME = config.get_string(os.getenv('COMET_PROJECT_NAME'), 'comet.project_name', default='yolov5')\ndef get_args(known=False):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--weights', type=str, default=ROOT / 'yolov5s.pt', help='initial weights path')\n    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')\n    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')\n    parser.add_argument('--hyp', type=str, default=ROOT / 'data/hyps/hyp.scratch-low.yaml', help='hyperparameters path')\n    parser.add_argument('--epochs', type=int, default=300, help='total training epochs')\n    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs, -1 for autobatch')\n    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')",
        "detail": "recommendation_assets.yolov5.utils.loggers.comet.hpo",
        "documentation": {}
    },
    {
        "label": "WandbLogger",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.loggers.wandb.wandb_utils",
        "description": "recommendation_assets.yolov5.utils.loggers.wandb.wandb_utils",
        "peekOfCode": "class WandbLogger():\n    \"\"\"Log training runs, datasets, models, and predictions to Weights & Biases.\n    This logger sends information to W&B at wandb.ai. By default, this information\n    includes hyperparameters, system configuration and metrics, model metrics,\n    and basic data metrics and analyses.\n    By providing additional command line arguments to train.py, datasets,\n    models and predictions can also be logged.\n    For more on how this logger is used, see the Weights & Biases documentation:\n    https://docs.wandb.com/guides/integrations/yolov5\n    \"\"\"",
        "detail": "recommendation_assets.yolov5.utils.loggers.wandb.wandb_utils",
        "documentation": {}
    },
    {
        "label": "all_logging_disabled",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.loggers.wandb.wandb_utils",
        "description": "recommendation_assets.yolov5.utils.loggers.wandb.wandb_utils",
        "peekOfCode": "def all_logging_disabled(highest_level=logging.CRITICAL):\n    \"\"\" source - https://gist.github.com/simon-weber/7853144\n    A context manager that will prevent any logging messages triggered during the body from being processed.\n    :param highest_level: the maximum logging level in use.\n      This would only need to be changed if a custom level greater than CRITICAL is defined.\n    \"\"\"\n    previous_level = logging.root.manager.disable\n    logging.disable(highest_level)\n    try:\n        yield",
        "detail": "recommendation_assets.yolov5.utils.loggers.wandb.wandb_utils",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.loggers.wandb.wandb_utils",
        "description": "recommendation_assets.yolov5.utils.loggers.wandb.wandb_utils",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[3]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nRANK = int(os.getenv('RANK', -1))\nDEPRECATION_WARNING = f\"{colorstr('wandb')}: WARNING  wandb is deprecated and will be removed in a future release. \" \\\n                      f'See supported integrations at https://github.com/ultralytics/yolov5#integrations.'\ntry:\n    import wandb\n    assert hasattr(wandb, '__version__')  # verify package import not local dir",
        "detail": "recommendation_assets.yolov5.utils.loggers.wandb.wandb_utils",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.loggers.wandb.wandb_utils",
        "description": "recommendation_assets.yolov5.utils.loggers.wandb.wandb_utils",
        "peekOfCode": "ROOT = FILE.parents[3]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nRANK = int(os.getenv('RANK', -1))\nDEPRECATION_WARNING = f\"{colorstr('wandb')}: WARNING  wandb is deprecated and will be removed in a future release. \" \\\n                      f'See supported integrations at https://github.com/ultralytics/yolov5#integrations.'\ntry:\n    import wandb\n    assert hasattr(wandb, '__version__')  # verify package import not local dir\n    LOGGER.warning(DEPRECATION_WARNING)",
        "detail": "recommendation_assets.yolov5.utils.loggers.wandb.wandb_utils",
        "documentation": {}
    },
    {
        "label": "RANK",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.loggers.wandb.wandb_utils",
        "description": "recommendation_assets.yolov5.utils.loggers.wandb.wandb_utils",
        "peekOfCode": "RANK = int(os.getenv('RANK', -1))\nDEPRECATION_WARNING = f\"{colorstr('wandb')}: WARNING  wandb is deprecated and will be removed in a future release. \" \\\n                      f'See supported integrations at https://github.com/ultralytics/yolov5#integrations.'\ntry:\n    import wandb\n    assert hasattr(wandb, '__version__')  # verify package import not local dir\n    LOGGER.warning(DEPRECATION_WARNING)\nexcept (ImportError, AssertionError):\n    wandb = None\nclass WandbLogger():",
        "detail": "recommendation_assets.yolov5.utils.loggers.wandb.wandb_utils",
        "documentation": {}
    },
    {
        "label": "DEPRECATION_WARNING",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.loggers.wandb.wandb_utils",
        "description": "recommendation_assets.yolov5.utils.loggers.wandb.wandb_utils",
        "peekOfCode": "DEPRECATION_WARNING = f\"{colorstr('wandb')}: WARNING  wandb is deprecated and will be removed in a future release. \" \\\n                      f'See supported integrations at https://github.com/ultralytics/yolov5#integrations.'\ntry:\n    import wandb\n    assert hasattr(wandb, '__version__')  # verify package import not local dir\n    LOGGER.warning(DEPRECATION_WARNING)\nexcept (ImportError, AssertionError):\n    wandb = None\nclass WandbLogger():\n    \"\"\"Log training runs, datasets, models, and predictions to Weights & Biases.",
        "detail": "recommendation_assets.yolov5.utils.loggers.wandb.wandb_utils",
        "documentation": {}
    },
    {
        "label": "mixup",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.segment.augmentations",
        "description": "recommendation_assets.yolov5.utils.segment.augmentations",
        "peekOfCode": "def mixup(im, labels, segments, im2, labels2, segments2):\n    # Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf\n    r = np.random.beta(32.0, 32.0)  # mixup ratio, alpha=beta=32.0\n    im = (im * r + im2 * (1 - r)).astype(np.uint8)\n    labels = np.concatenate((labels, labels2), 0)\n    segments = np.concatenate((segments, segments2), 0)\n    return im, labels, segments\ndef random_perspective(im,\n                       targets=(),\n                       segments=(),",
        "detail": "recommendation_assets.yolov5.utils.segment.augmentations",
        "documentation": {}
    },
    {
        "label": "random_perspective",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.segment.augmentations",
        "description": "recommendation_assets.yolov5.utils.segment.augmentations",
        "peekOfCode": "def random_perspective(im,\n                       targets=(),\n                       segments=(),\n                       degrees=10,\n                       translate=.1,\n                       scale=.1,\n                       shear=10,\n                       perspective=0.0,\n                       border=(0, 0)):\n    # torchvision.transforms.RandomAffine(degrees=(-10, 10), translate=(.1, .1), scale=(.9, 1.1), shear=(-10, 10))",
        "detail": "recommendation_assets.yolov5.utils.segment.augmentations",
        "documentation": {}
    },
    {
        "label": "LoadImagesAndLabelsAndMasks",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.segment.dataloaders",
        "description": "recommendation_assets.yolov5.utils.segment.dataloaders",
        "peekOfCode": "class LoadImagesAndLabelsAndMasks(LoadImagesAndLabels):  # for training/testing\n    def __init__(\n        self,\n        path,\n        img_size=640,\n        batch_size=16,\n        augment=False,\n        hyp=None,\n        rect=False,\n        image_weights=False,",
        "detail": "recommendation_assets.yolov5.utils.segment.dataloaders",
        "documentation": {}
    },
    {
        "label": "create_dataloader",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.segment.dataloaders",
        "description": "recommendation_assets.yolov5.utils.segment.dataloaders",
        "peekOfCode": "def create_dataloader(path,\n                      imgsz,\n                      batch_size,\n                      stride,\n                      single_cls=False,\n                      hyp=None,\n                      augment=False,\n                      cache=False,\n                      pad=0.0,\n                      rect=False,",
        "detail": "recommendation_assets.yolov5.utils.segment.dataloaders",
        "documentation": {}
    },
    {
        "label": "polygon2mask",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.segment.dataloaders",
        "description": "recommendation_assets.yolov5.utils.segment.dataloaders",
        "peekOfCode": "def polygon2mask(img_size, polygons, color=1, downsample_ratio=1):\n    \"\"\"\n    Args:\n        img_size (tuple): The image size.\n        polygons (np.ndarray): [N, M], N is the number of polygons,\n            M is the number of points(Be divided by 2).\n    \"\"\"\n    mask = np.zeros(img_size, dtype=np.uint8)\n    polygons = np.asarray(polygons)\n    polygons = polygons.astype(np.int32)",
        "detail": "recommendation_assets.yolov5.utils.segment.dataloaders",
        "documentation": {}
    },
    {
        "label": "polygons2masks",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.segment.dataloaders",
        "description": "recommendation_assets.yolov5.utils.segment.dataloaders",
        "peekOfCode": "def polygons2masks(img_size, polygons, color, downsample_ratio=1):\n    \"\"\"\n    Args:\n        img_size (tuple): The image size.\n        polygons (list[np.ndarray]): each polygon is [N, M],\n            N is the number of polygons,\n            M is the number of points(Be divided by 2).\n    \"\"\"\n    masks = []\n    for si in range(len(polygons)):",
        "detail": "recommendation_assets.yolov5.utils.segment.dataloaders",
        "documentation": {}
    },
    {
        "label": "polygons2masks_overlap",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.segment.dataloaders",
        "description": "recommendation_assets.yolov5.utils.segment.dataloaders",
        "peekOfCode": "def polygons2masks_overlap(img_size, segments, downsample_ratio=1):\n    \"\"\"Return a (640, 640) overlap mask.\"\"\"\n    masks = np.zeros((img_size[0] // downsample_ratio, img_size[1] // downsample_ratio),\n                     dtype=np.int32 if len(segments) > 255 else np.uint8)\n    areas = []\n    ms = []\n    for si in range(len(segments)):\n        mask = polygon2mask(\n            img_size,\n            [segments[si].reshape(-1)],",
        "detail": "recommendation_assets.yolov5.utils.segment.dataloaders",
        "documentation": {}
    },
    {
        "label": "RANK",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.segment.dataloaders",
        "description": "recommendation_assets.yolov5.utils.segment.dataloaders",
        "peekOfCode": "RANK = int(os.getenv('RANK', -1))\ndef create_dataloader(path,\n                      imgsz,\n                      batch_size,\n                      stride,\n                      single_cls=False,\n                      hyp=None,\n                      augment=False,\n                      cache=False,\n                      pad=0.0,",
        "detail": "recommendation_assets.yolov5.utils.segment.dataloaders",
        "documentation": {}
    },
    {
        "label": "crop_mask",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.segment.general",
        "description": "recommendation_assets.yolov5.utils.segment.general",
        "peekOfCode": "def crop_mask(masks, boxes):\n    \"\"\"\n    \"Crop\" predicted masks by zeroing out everything not in the predicted bbox.\n    Vectorized by Chong (thanks Chong).\n    Args:\n        - masks should be a size [n, h, w] tensor of masks\n        - boxes should be a size [n, 4] tensor of bbox coords in relative point form\n    \"\"\"\n    n, h, w = masks.shape\n    x1, y1, x2, y2 = torch.chunk(boxes[:, :, None], 4, 1)  # x1 shape(1,1,n)",
        "detail": "recommendation_assets.yolov5.utils.segment.general",
        "documentation": {}
    },
    {
        "label": "process_mask_upsample",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.segment.general",
        "description": "recommendation_assets.yolov5.utils.segment.general",
        "peekOfCode": "def process_mask_upsample(protos, masks_in, bboxes, shape):\n    \"\"\"\n    Crop after upsample.\n    protos: [mask_dim, mask_h, mask_w]\n    masks_in: [n, mask_dim], n is number of masks after nms\n    bboxes: [n, 4], n is number of masks after nms\n    shape: input_image_size, (h, w)\n    return: h, w, n\n    \"\"\"\n    c, mh, mw = protos.shape  # CHW",
        "detail": "recommendation_assets.yolov5.utils.segment.general",
        "documentation": {}
    },
    {
        "label": "process_mask",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.segment.general",
        "description": "recommendation_assets.yolov5.utils.segment.general",
        "peekOfCode": "def process_mask(protos, masks_in, bboxes, shape, upsample=False):\n    \"\"\"\n    Crop before upsample.\n    proto_out: [mask_dim, mask_h, mask_w]\n    out_masks: [n, mask_dim], n is number of masks after nms\n    bboxes: [n, 4], n is number of masks after nms\n    shape:input_image_size, (h, w)\n    return: h, w, n\n    \"\"\"\n    c, mh, mw = protos.shape  # CHW",
        "detail": "recommendation_assets.yolov5.utils.segment.general",
        "documentation": {}
    },
    {
        "label": "process_mask_native",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.segment.general",
        "description": "recommendation_assets.yolov5.utils.segment.general",
        "peekOfCode": "def process_mask_native(protos, masks_in, bboxes, shape):\n    \"\"\"\n    Crop after upsample.\n    protos: [mask_dim, mask_h, mask_w]\n    masks_in: [n, mask_dim], n is number of masks after nms\n    bboxes: [n, 4], n is number of masks after nms\n    shape: input_image_size, (h, w)\n    return: h, w, n\n    \"\"\"\n    c, mh, mw = protos.shape  # CHW",
        "detail": "recommendation_assets.yolov5.utils.segment.general",
        "documentation": {}
    },
    {
        "label": "scale_image",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.segment.general",
        "description": "recommendation_assets.yolov5.utils.segment.general",
        "peekOfCode": "def scale_image(im1_shape, masks, im0_shape, ratio_pad=None):\n    \"\"\"\n    img1_shape: model input shape, [h, w]\n    img0_shape: origin pic shape, [h, w, 3]\n    masks: [h, w, num]\n    \"\"\"\n    # Rescale coordinates (xyxy) from im1_shape to im0_shape\n    if ratio_pad is None:  # calculate from im0_shape\n        gain = min(im1_shape[0] / im0_shape[0], im1_shape[1] / im0_shape[1])  # gain  = old / new\n        pad = (im1_shape[1] - im0_shape[1] * gain) / 2, (im1_shape[0] - im0_shape[0] * gain) / 2  # wh padding",
        "detail": "recommendation_assets.yolov5.utils.segment.general",
        "documentation": {}
    },
    {
        "label": "mask_iou",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.segment.general",
        "description": "recommendation_assets.yolov5.utils.segment.general",
        "peekOfCode": "def mask_iou(mask1, mask2, eps=1e-7):\n    \"\"\"\n    mask1: [N, n] m1 means number of predicted objects\n    mask2: [M, n] m2 means number of gt objects\n    Note: n means image_w x image_h\n    return: masks iou, [N, M]\n    \"\"\"\n    intersection = torch.matmul(mask1, mask2.t()).clamp(0)\n    union = (mask1.sum(1)[:, None] + mask2.sum(1)[None]) - intersection  # (area1 + area2) - intersection\n    return intersection / (union + eps)",
        "detail": "recommendation_assets.yolov5.utils.segment.general",
        "documentation": {}
    },
    {
        "label": "masks_iou",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.segment.general",
        "description": "recommendation_assets.yolov5.utils.segment.general",
        "peekOfCode": "def masks_iou(mask1, mask2, eps=1e-7):\n    \"\"\"\n    mask1: [N, n] m1 means number of predicted objects\n    mask2: [N, n] m2 means number of gt objects\n    Note: n means image_w x image_h\n    return: masks iou, (N, )\n    \"\"\"\n    intersection = (mask1 * mask2).sum(1).clamp(0)  # (N, )\n    union = (mask1.sum(1) + mask2.sum(1))[None] - intersection  # (area1 + area2) - intersection\n    return intersection / (union + eps)",
        "detail": "recommendation_assets.yolov5.utils.segment.general",
        "documentation": {}
    },
    {
        "label": "masks2segments",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.segment.general",
        "description": "recommendation_assets.yolov5.utils.segment.general",
        "peekOfCode": "def masks2segments(masks, strategy='largest'):\n    # Convert masks(n,160,160) into segments(n,xy)\n    segments = []\n    for x in masks.int().cpu().numpy().astype('uint8'):\n        c = cv2.findContours(x, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n        if c:\n            if strategy == 'concat':  # concatenate all segments\n                c = np.concatenate([x.reshape(-1, 2) for x in c])\n            elif strategy == 'largest':  # select largest segment\n                c = np.array(c[np.array([len(x) for x in c]).argmax()]).reshape(-1, 2)",
        "detail": "recommendation_assets.yolov5.utils.segment.general",
        "documentation": {}
    },
    {
        "label": "ComputeLoss",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.segment.loss",
        "description": "recommendation_assets.yolov5.utils.segment.loss",
        "peekOfCode": "class ComputeLoss:\n    # Compute losses\n    def __init__(self, model, autobalance=False, overlap=False):\n        self.sort_obj_iou = False\n        self.overlap = overlap\n        device = next(model.parameters()).device  # get model device\n        h = model.hyp  # hyperparameters\n        # Define criteria\n        BCEcls = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h['cls_pw']], device=device))\n        BCEobj = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h['obj_pw']], device=device))",
        "detail": "recommendation_assets.yolov5.utils.segment.loss",
        "documentation": {}
    },
    {
        "label": "Metric",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.segment.metrics",
        "description": "recommendation_assets.yolov5.utils.segment.metrics",
        "peekOfCode": "class Metric:\n    def __init__(self) -> None:\n        self.p = []  # (nc, )\n        self.r = []  # (nc, )\n        self.f1 = []  # (nc, )\n        self.all_ap = []  # (nc, 10)\n        self.ap_class_index = []  # (nc, )\n    @property\n    def ap50(self):\n        \"\"\"AP@0.5 of all classes.",
        "detail": "recommendation_assets.yolov5.utils.segment.metrics",
        "documentation": {}
    },
    {
        "label": "Metrics",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.segment.metrics",
        "description": "recommendation_assets.yolov5.utils.segment.metrics",
        "peekOfCode": "class Metrics:\n    \"\"\"Metric for boxes and masks.\"\"\"\n    def __init__(self) -> None:\n        self.metric_box = Metric()\n        self.metric_mask = Metric()\n    def update(self, results):\n        \"\"\"\n        Args:\n            results: Dict{'boxes': Dict{}, 'masks': Dict{}}\n        \"\"\"",
        "detail": "recommendation_assets.yolov5.utils.segment.metrics",
        "documentation": {}
    },
    {
        "label": "fitness",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.segment.metrics",
        "description": "recommendation_assets.yolov5.utils.segment.metrics",
        "peekOfCode": "def fitness(x):\n    # Model fitness as a weighted combination of metrics\n    w = [0.0, 0.0, 0.1, 0.9, 0.0, 0.0, 0.1, 0.9]\n    return (x[:, :8] * w).sum(1)\ndef ap_per_class_box_and_mask(\n        tp_m,\n        tp_b,\n        conf,\n        pred_cls,\n        target_cls,",
        "detail": "recommendation_assets.yolov5.utils.segment.metrics",
        "documentation": {}
    },
    {
        "label": "ap_per_class_box_and_mask",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.segment.metrics",
        "description": "recommendation_assets.yolov5.utils.segment.metrics",
        "peekOfCode": "def ap_per_class_box_and_mask(\n        tp_m,\n        tp_b,\n        conf,\n        pred_cls,\n        target_cls,\n        plot=False,\n        save_dir='.',\n        names=(),\n):",
        "detail": "recommendation_assets.yolov5.utils.segment.metrics",
        "documentation": {}
    },
    {
        "label": "KEYS",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.segment.metrics",
        "description": "recommendation_assets.yolov5.utils.segment.metrics",
        "peekOfCode": "KEYS = [\n    'train/box_loss',\n    'train/seg_loss',  # train loss\n    'train/obj_loss',\n    'train/cls_loss',\n    'metrics/precision(B)',\n    'metrics/recall(B)',\n    'metrics/mAP_0.5(B)',\n    'metrics/mAP_0.5:0.95(B)',  # metrics\n    'metrics/precision(M)',",
        "detail": "recommendation_assets.yolov5.utils.segment.metrics",
        "documentation": {}
    },
    {
        "label": "BEST_KEYS",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.segment.metrics",
        "description": "recommendation_assets.yolov5.utils.segment.metrics",
        "peekOfCode": "BEST_KEYS = [\n    'best/epoch',\n    'best/precision(B)',\n    'best/recall(B)',\n    'best/mAP_0.5(B)',\n    'best/mAP_0.5:0.95(B)',\n    'best/precision(M)',\n    'best/recall(M)',\n    'best/mAP_0.5(M)',\n    'best/mAP_0.5:0.95(M)', ]",
        "detail": "recommendation_assets.yolov5.utils.segment.metrics",
        "documentation": {}
    },
    {
        "label": "plot_images_and_masks",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.segment.plots",
        "description": "recommendation_assets.yolov5.utils.segment.plots",
        "peekOfCode": "def plot_images_and_masks(images, targets, masks, paths=None, fname='images.jpg', names=None):\n    # Plot image grid with labels\n    if isinstance(images, torch.Tensor):\n        images = images.cpu().float().numpy()\n    if isinstance(targets, torch.Tensor):\n        targets = targets.cpu().numpy()\n    if isinstance(masks, torch.Tensor):\n        masks = masks.cpu().numpy().astype(int)\n    max_size = 1920  # max image size\n    max_subplots = 16  # max image subplots, i.e. 4x4",
        "detail": "recommendation_assets.yolov5.utils.segment.plots",
        "documentation": {}
    },
    {
        "label": "plot_results_with_masks",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.segment.plots",
        "description": "recommendation_assets.yolov5.utils.segment.plots",
        "peekOfCode": "def plot_results_with_masks(file='path/to/results.csv', dir='', best=True):\n    # Plot training results.csv. Usage: from utils.plots import *; plot_results('path/to/results.csv')\n    save_dir = Path(file).parent if file else Path(dir)\n    fig, ax = plt.subplots(2, 8, figsize=(18, 6), tight_layout=True)\n    ax = ax.ravel()\n    files = list(save_dir.glob('results*.csv'))\n    assert len(files), f'No results.csv files found in {save_dir.resolve()}, nothing to plot.'\n    for f in files:\n        try:\n            data = pd.read_csv(f)",
        "detail": "recommendation_assets.yolov5.utils.segment.plots",
        "documentation": {}
    },
    {
        "label": "SiLU",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.activations",
        "description": "recommendation_assets.yolov5.utils.activations",
        "peekOfCode": "class SiLU(nn.Module):\n    # SiLU activation https://arxiv.org/pdf/1606.08415.pdf\n    @staticmethod\n    def forward(x):\n        return x * torch.sigmoid(x)\nclass Hardswish(nn.Module):\n    # Hard-SiLU activation\n    @staticmethod\n    def forward(x):\n        # return x * F.hardsigmoid(x)  # for TorchScript and CoreML",
        "detail": "recommendation_assets.yolov5.utils.activations",
        "documentation": {}
    },
    {
        "label": "Hardswish",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.activations",
        "description": "recommendation_assets.yolov5.utils.activations",
        "peekOfCode": "class Hardswish(nn.Module):\n    # Hard-SiLU activation\n    @staticmethod\n    def forward(x):\n        # return x * F.hardsigmoid(x)  # for TorchScript and CoreML\n        return x * F.hardtanh(x + 3, 0.0, 6.0) / 6.0  # for TorchScript, CoreML and ONNX\nclass Mish(nn.Module):\n    # Mish activation https://github.com/digantamisra98/Mish\n    @staticmethod\n    def forward(x):",
        "detail": "recommendation_assets.yolov5.utils.activations",
        "documentation": {}
    },
    {
        "label": "Mish",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.activations",
        "description": "recommendation_assets.yolov5.utils.activations",
        "peekOfCode": "class Mish(nn.Module):\n    # Mish activation https://github.com/digantamisra98/Mish\n    @staticmethod\n    def forward(x):\n        return x * F.softplus(x).tanh()\nclass MemoryEfficientMish(nn.Module):\n    # Mish activation memory-efficient\n    class F(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, x):",
        "detail": "recommendation_assets.yolov5.utils.activations",
        "documentation": {}
    },
    {
        "label": "MemoryEfficientMish",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.activations",
        "description": "recommendation_assets.yolov5.utils.activations",
        "peekOfCode": "class MemoryEfficientMish(nn.Module):\n    # Mish activation memory-efficient\n    class F(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, x):\n            ctx.save_for_backward(x)\n            return x.mul(torch.tanh(F.softplus(x)))  # x * tanh(ln(1 + exp(x)))\n        @staticmethod\n        def backward(ctx, grad_output):\n            x = ctx.saved_tensors[0]",
        "detail": "recommendation_assets.yolov5.utils.activations",
        "documentation": {}
    },
    {
        "label": "FReLU",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.activations",
        "description": "recommendation_assets.yolov5.utils.activations",
        "peekOfCode": "class FReLU(nn.Module):\n    # FReLU activation https://arxiv.org/abs/2007.11824\n    def __init__(self, c1, k=3):  # ch_in, kernel\n        super().__init__()\n        self.conv = nn.Conv2d(c1, c1, k, 1, 1, groups=c1, bias=False)\n        self.bn = nn.BatchNorm2d(c1)\n    def forward(self, x):\n        return torch.max(x, self.bn(self.conv(x)))\nclass AconC(nn.Module):\n    r\"\"\" ACON activation (activate or not)",
        "detail": "recommendation_assets.yolov5.utils.activations",
        "documentation": {}
    },
    {
        "label": "AconC",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.activations",
        "description": "recommendation_assets.yolov5.utils.activations",
        "peekOfCode": "class AconC(nn.Module):\n    r\"\"\" ACON activation (activate or not)\n    AconC: (p1*x-p2*x) * sigmoid(beta*(p1*x-p2*x)) + p2*x, beta is a learnable parameter\n    according to \"Activate or Not: Learning Customized Activation\" <https://arxiv.org/pdf/2009.04759.pdf>.\n    \"\"\"\n    def __init__(self, c1):\n        super().__init__()\n        self.p1 = nn.Parameter(torch.randn(1, c1, 1, 1))\n        self.p2 = nn.Parameter(torch.randn(1, c1, 1, 1))\n        self.beta = nn.Parameter(torch.ones(1, c1, 1, 1))",
        "detail": "recommendation_assets.yolov5.utils.activations",
        "documentation": {}
    },
    {
        "label": "MetaAconC",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.activations",
        "description": "recommendation_assets.yolov5.utils.activations",
        "peekOfCode": "class MetaAconC(nn.Module):\n    r\"\"\" ACON activation (activate or not)\n    MetaAconC: (p1*x-p2*x) * sigmoid(beta*(p1*x-p2*x)) + p2*x, beta is generated by a small network\n    according to \"Activate or Not: Learning Customized Activation\" <https://arxiv.org/pdf/2009.04759.pdf>.\n    \"\"\"\n    def __init__(self, c1, k=1, s=1, r=16):  # ch_in, kernel, stride, r\n        super().__init__()\n        c2 = max(r, c1 // r)\n        self.p1 = nn.Parameter(torch.randn(1, c1, 1, 1))\n        self.p2 = nn.Parameter(torch.randn(1, c1, 1, 1))",
        "detail": "recommendation_assets.yolov5.utils.activations",
        "documentation": {}
    },
    {
        "label": "Albumentations",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "class Albumentations:\n    # YOLOv5 Albumentations class (optional, only used if package is installed)\n    def __init__(self, size=640):\n        self.transform = None\n        prefix = colorstr('albumentations: ')\n        try:\n            import albumentations as A\n            check_version(A.__version__, '1.0.3', hard=True)  # version requirement\n            T = [\n                A.RandomResizedCrop(height=size, width=size, scale=(0.8, 1.0), ratio=(0.9, 1.11), p=0.0),",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "LetterBox",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "class LetterBox:\n    # YOLOv5 LetterBox class for image preprocessing, i.e. T.Compose([LetterBox(size), ToTensor()])\n    def __init__(self, size=(640, 640), auto=False, stride=32):\n        super().__init__()\n        self.h, self.w = (size, size) if isinstance(size, int) else size\n        self.auto = auto  # pass max size integer, automatically solve for short side using stride\n        self.stride = stride  # used with auto\n    def __call__(self, im):  # im = np.array HWC\n        imh, imw = im.shape[:2]\n        r = min(self.h / imh, self.w / imw)  # ratio of new/old",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "CenterCrop",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "class CenterCrop:\n    # YOLOv5 CenterCrop class for image preprocessing, i.e. T.Compose([CenterCrop(size), ToTensor()])\n    def __init__(self, size=640):\n        super().__init__()\n        self.h, self.w = (size, size) if isinstance(size, int) else size\n    def __call__(self, im):  # im = np.array HWC\n        imh, imw = im.shape[:2]\n        m = min(imh, imw)  # min dimension\n        top, left = (imh - m) // 2, (imw - m) // 2\n        return cv2.resize(im[top:top + m, left:left + m], (self.w, self.h), interpolation=cv2.INTER_LINEAR)",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "ToTensor",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "class ToTensor:\n    # YOLOv5 ToTensor class for image preprocessing, i.e. T.Compose([LetterBox(size), ToTensor()])\n    def __init__(self, half=False):\n        super().__init__()\n        self.half = half\n    def __call__(self, im):  # im = np.array HWC in BGR order\n        im = np.ascontiguousarray(im.transpose((2, 0, 1))[::-1])  # HWC to CHW -> BGR to RGB -> contiguous\n        im = torch.from_numpy(im)  # to torch\n        im = im.half() if self.half else im.float()  # uint8 to fp16/32\n        im /= 255.0  # 0-255 to 0.0-1.0",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "normalize",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "def normalize(x, mean=IMAGENET_MEAN, std=IMAGENET_STD, inplace=False):\n    # Denormalize RGB images x per ImageNet stats in BCHW format, i.e. = (x - mean) / std\n    return TF.normalize(x, mean, std, inplace=inplace)\ndef denormalize(x, mean=IMAGENET_MEAN, std=IMAGENET_STD):\n    # Denormalize RGB images x per ImageNet stats in BCHW format, i.e. = x * std + mean\n    for i in range(3):\n        x[:, i] = x[:, i] * std[i] + mean[i]\n    return x\ndef augment_hsv(im, hgain=0.5, sgain=0.5, vgain=0.5):\n    # HSV color-space augmentation",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "denormalize",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "def denormalize(x, mean=IMAGENET_MEAN, std=IMAGENET_STD):\n    # Denormalize RGB images x per ImageNet stats in BCHW format, i.e. = x * std + mean\n    for i in range(3):\n        x[:, i] = x[:, i] * std[i] + mean[i]\n    return x\ndef augment_hsv(im, hgain=0.5, sgain=0.5, vgain=0.5):\n    # HSV color-space augmentation\n    if hgain or sgain or vgain:\n        r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains\n        hue, sat, val = cv2.split(cv2.cvtColor(im, cv2.COLOR_BGR2HSV))",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "augment_hsv",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "def augment_hsv(im, hgain=0.5, sgain=0.5, vgain=0.5):\n    # HSV color-space augmentation\n    if hgain or sgain or vgain:\n        r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains\n        hue, sat, val = cv2.split(cv2.cvtColor(im, cv2.COLOR_BGR2HSV))\n        dtype = im.dtype  # uint8\n        x = np.arange(0, 256, dtype=r.dtype)\n        lut_hue = ((x * r[0]) % 180).astype(dtype)\n        lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n        lut_val = np.clip(x * r[2], 0, 255).astype(dtype)",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "hist_equalize",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "def hist_equalize(im, clahe=True, bgr=False):\n    # Equalize histogram on BGR image 'im' with im.shape(n,m,3) and range 0-255\n    yuv = cv2.cvtColor(im, cv2.COLOR_BGR2YUV if bgr else cv2.COLOR_RGB2YUV)\n    if clahe:\n        c = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        yuv[:, :, 0] = c.apply(yuv[:, :, 0])\n    else:\n        yuv[:, :, 0] = cv2.equalizeHist(yuv[:, :, 0])  # equalize Y channel histogram\n    return cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR if bgr else cv2.COLOR_YUV2RGB)  # convert YUV image to RGB\ndef replicate(im, labels):",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "replicate",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "def replicate(im, labels):\n    # Replicate labels\n    h, w = im.shape[:2]\n    boxes = labels[:, 1:].astype(int)\n    x1, y1, x2, y2 = boxes.T\n    s = ((x2 - x1) + (y2 - y1)) / 2  # side length (pixels)\n    for i in s.argsort()[:round(s.size * 0.5)]:  # smallest indices\n        x1b, y1b, x2b, y2b = boxes[i]\n        bh, bw = y2b - y1b, x2b - x1b\n        yc, xc = int(random.uniform(0, h - bh)), int(random.uniform(0, w - bw))  # offset x, y",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "letterbox",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n    # Resize and pad image while meeting stride-multiple constraints\n    shape = im.shape[:2]  # current shape [height, width]\n    if isinstance(new_shape, int):\n        new_shape = (new_shape, new_shape)\n    # Scale ratio (new / old)\n    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n        r = min(r, 1.0)\n    # Compute padding",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "random_perspective",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "def random_perspective(im,\n                       targets=(),\n                       segments=(),\n                       degrees=10,\n                       translate=.1,\n                       scale=.1,\n                       shear=10,\n                       perspective=0.0,\n                       border=(0, 0)):\n    # torchvision.transforms.RandomAffine(degrees=(-10, 10), translate=(0.1, 0.1), scale=(0.9, 1.1), shear=(-10, 10))",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "copy_paste",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "def copy_paste(im, labels, segments, p=0.5):\n    # Implement Copy-Paste augmentation https://arxiv.org/abs/2012.07177, labels as nx5 np.array(cls, xyxy)\n    n = len(segments)\n    if p and n:\n        h, w, c = im.shape  # height, width, channels\n        im_new = np.zeros(im.shape, np.uint8)\n        for j in random.sample(range(n), k=round(p * n)):\n            l, s = labels[j], segments[j]\n            box = w - l[3], l[2], w - l[1], l[4]\n            ioa = bbox_ioa(box, labels[:, 1:5])  # intersection over area",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "cutout",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "def cutout(im, labels, p=0.5):\n    # Applies image cutout augmentation https://arxiv.org/abs/1708.04552\n    if random.random() < p:\n        h, w = im.shape[:2]\n        scales = [0.5] * 1 + [0.25] * 2 + [0.125] * 4 + [0.0625] * 8 + [0.03125] * 16  # image size fraction\n        for s in scales:\n            mask_h = random.randint(1, int(h * s))  # create random masks\n            mask_w = random.randint(1, int(w * s))\n            # box\n            xmin = max(0, random.randint(0, w) - mask_w // 2)",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "mixup",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "def mixup(im, labels, im2, labels2):\n    # Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf\n    r = np.random.beta(32.0, 32.0)  # mixup ratio, alpha=beta=32.0\n    im = (im * r + im2 * (1 - r)).astype(np.uint8)\n    labels = np.concatenate((labels, labels2), 0)\n    return im, labels\ndef box_candidates(box1, box2, wh_thr=2, ar_thr=100, area_thr=0.1, eps=1e-16):  # box1(4,n), box2(4,n)\n    # Compute candidate boxes: box1 before augment, box2 after augment, wh_thr (pixels), aspect_ratio_thr, area_ratio\n    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "box_candidates",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "def box_candidates(box1, box2, wh_thr=2, ar_thr=100, area_thr=0.1, eps=1e-16):  # box1(4,n), box2(4,n)\n    # Compute candidate boxes: box1 before augment, box2 after augment, wh_thr (pixels), aspect_ratio_thr, area_ratio\n    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n    ar = np.maximum(w2 / (h2 + eps), h2 / (w2 + eps))  # aspect ratio\n    return (w2 > wh_thr) & (h2 > wh_thr) & (w2 * h2 / (w1 * h1 + eps) > area_thr) & (ar < ar_thr)  # candidates\ndef classify_albumentations(\n        augment=True,\n        size=224,\n        scale=(0.08, 1.0),",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "classify_albumentations",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "def classify_albumentations(\n        augment=True,\n        size=224,\n        scale=(0.08, 1.0),\n        ratio=(0.75, 1.0 / 0.75),  # 0.75, 1.33\n        hflip=0.5,\n        vflip=0.0,\n        jitter=0.4,\n        mean=IMAGENET_MEAN,\n        std=IMAGENET_STD,",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "classify_transforms",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "def classify_transforms(size=224):\n    # Transforms to apply if albumentations not installed\n    assert isinstance(size, int), f'ERROR: classify_transforms size {size} must be integer, not (list, tuple)'\n    # T.Compose([T.ToTensor(), T.Resize(size), T.CenterCrop(size), T.Normalize(IMAGENET_MEAN, IMAGENET_STD)])\n    return T.Compose([CenterCrop(size), ToTensor(), T.Normalize(IMAGENET_MEAN, IMAGENET_STD)])\nclass LetterBox:\n    # YOLOv5 LetterBox class for image preprocessing, i.e. T.Compose([LetterBox(size), ToTensor()])\n    def __init__(self, size=(640, 640), auto=False, stride=32):\n        super().__init__()\n        self.h, self.w = (size, size) if isinstance(size, int) else size",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "IMAGENET_MEAN",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "IMAGENET_MEAN = 0.485, 0.456, 0.406  # RGB mean\nIMAGENET_STD = 0.229, 0.224, 0.225  # RGB standard deviation\nclass Albumentations:\n    # YOLOv5 Albumentations class (optional, only used if package is installed)\n    def __init__(self, size=640):\n        self.transform = None\n        prefix = colorstr('albumentations: ')\n        try:\n            import albumentations as A\n            check_version(A.__version__, '1.0.3', hard=True)  # version requirement",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "IMAGENET_STD",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.augmentations",
        "description": "recommendation_assets.yolov5.utils.augmentations",
        "peekOfCode": "IMAGENET_STD = 0.229, 0.224, 0.225  # RGB standard deviation\nclass Albumentations:\n    # YOLOv5 Albumentations class (optional, only used if package is installed)\n    def __init__(self, size=640):\n        self.transform = None\n        prefix = colorstr('albumentations: ')\n        try:\n            import albumentations as A\n            check_version(A.__version__, '1.0.3', hard=True)  # version requirement\n            T = [",
        "detail": "recommendation_assets.yolov5.utils.augmentations",
        "documentation": {}
    },
    {
        "label": "check_anchor_order",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.autoanchor",
        "description": "recommendation_assets.yolov5.utils.autoanchor",
        "peekOfCode": "def check_anchor_order(m):\n    # Check anchor order against stride order for YOLOv5 Detect() module m, and correct if necessary\n    a = m.anchors.prod(-1).mean(-1).view(-1)  # mean anchor area per output layer\n    da = a[-1] - a[0]  # delta a\n    ds = m.stride[-1] - m.stride[0]  # delta s\n    if da and (da.sign() != ds.sign()):  # same order\n        LOGGER.info(f'{PREFIX}Reversing anchor order')\n        m.anchors[:] = m.anchors.flip(0)\n@TryExcept(f'{PREFIX}ERROR')\ndef check_anchors(dataset, model, thr=4.0, imgsz=640):",
        "detail": "recommendation_assets.yolov5.utils.autoanchor",
        "documentation": {}
    },
    {
        "label": "check_anchors",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.autoanchor",
        "description": "recommendation_assets.yolov5.utils.autoanchor",
        "peekOfCode": "def check_anchors(dataset, model, thr=4.0, imgsz=640):\n    # Check anchor fit to data, recompute if necessary\n    m = model.module.model[-1] if hasattr(model, 'module') else model.model[-1]  # Detect()\n    shapes = imgsz * dataset.shapes / dataset.shapes.max(1, keepdims=True)\n    scale = np.random.uniform(0.9, 1.1, size=(shapes.shape[0], 1))  # augment scale\n    wh = torch.tensor(np.concatenate([l[:, 3:5] * s for s, l in zip(shapes * scale, dataset.labels)])).float()  # wh\n    def metric(k):  # compute metric\n        r = wh[:, None] / k[None]\n        x = torch.min(r, 1 / r).min(2)[0]  # ratio metric\n        best = x.max(1)[0]  # best_x",
        "detail": "recommendation_assets.yolov5.utils.autoanchor",
        "documentation": {}
    },
    {
        "label": "kmean_anchors",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.autoanchor",
        "description": "recommendation_assets.yolov5.utils.autoanchor",
        "peekOfCode": "def kmean_anchors(dataset='./data/coco128.yaml', n=9, img_size=640, thr=4.0, gen=1000, verbose=True):\n    \"\"\" Creates kmeans-evolved anchors from training dataset\n        Arguments:\n            dataset: path to data.yaml, or a loaded dataset\n            n: number of anchors\n            img_size: image size used for training\n            thr: anchor-label wh ratio threshold hyperparameter hyp['anchor_t'] used for training, default=4.0\n            gen: generations to evolve anchors using genetic algorithm\n            verbose: print all results\n        Return:",
        "detail": "recommendation_assets.yolov5.utils.autoanchor",
        "documentation": {}
    },
    {
        "label": "PREFIX",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.autoanchor",
        "description": "recommendation_assets.yolov5.utils.autoanchor",
        "peekOfCode": "PREFIX = colorstr('AutoAnchor: ')\ndef check_anchor_order(m):\n    # Check anchor order against stride order for YOLOv5 Detect() module m, and correct if necessary\n    a = m.anchors.prod(-1).mean(-1).view(-1)  # mean anchor area per output layer\n    da = a[-1] - a[0]  # delta a\n    ds = m.stride[-1] - m.stride[0]  # delta s\n    if da and (da.sign() != ds.sign()):  # same order\n        LOGGER.info(f'{PREFIX}Reversing anchor order')\n        m.anchors[:] = m.anchors.flip(0)\n@TryExcept(f'{PREFIX}ERROR')",
        "detail": "recommendation_assets.yolov5.utils.autoanchor",
        "documentation": {}
    },
    {
        "label": "check_train_batch_size",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.autobatch",
        "description": "recommendation_assets.yolov5.utils.autobatch",
        "peekOfCode": "def check_train_batch_size(model, imgsz=640, amp=True):\n    # Check YOLOv5 training batch size\n    with torch.cuda.amp.autocast(amp):\n        return autobatch(deepcopy(model).train(), imgsz)  # compute optimal batch size\ndef autobatch(model, imgsz=640, fraction=0.8, batch_size=16):\n    # Automatically estimate best YOLOv5 batch size to use `fraction` of available CUDA memory\n    # Usage:\n    #     import torch\n    #     from utils.autobatch import autobatch\n    #     model = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False)",
        "detail": "recommendation_assets.yolov5.utils.autobatch",
        "documentation": {}
    },
    {
        "label": "autobatch",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.autobatch",
        "description": "recommendation_assets.yolov5.utils.autobatch",
        "peekOfCode": "def autobatch(model, imgsz=640, fraction=0.8, batch_size=16):\n    # Automatically estimate best YOLOv5 batch size to use `fraction` of available CUDA memory\n    # Usage:\n    #     import torch\n    #     from utils.autobatch import autobatch\n    #     model = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False)\n    #     print(autobatch(model))\n    # Check device\n    prefix = colorstr('AutoBatch: ')\n    LOGGER.info(f'{prefix}Computing optimal batch size for --imgsz {imgsz}')",
        "detail": "recommendation_assets.yolov5.utils.autobatch",
        "documentation": {}
    },
    {
        "label": "Callbacks",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.callbacks",
        "description": "recommendation_assets.yolov5.utils.callbacks",
        "peekOfCode": "class Callbacks:\n    \"\"\"\"\n    Handles all registered callbacks for YOLOv5 Hooks\n    \"\"\"\n    def __init__(self):\n        # Define the available callbacks\n        self._callbacks = {\n            'on_pretrain_routine_start': [],\n            'on_pretrain_routine_end': [],\n            'on_train_start': [],",
        "detail": "recommendation_assets.yolov5.utils.callbacks",
        "documentation": {}
    },
    {
        "label": "InfiniteDataLoader",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "class InfiniteDataLoader(dataloader.DataLoader):\n    \"\"\" Dataloader that reuses workers\n    Uses same syntax as vanilla DataLoader\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        object.__setattr__(self, 'batch_sampler', _RepeatSampler(self.batch_sampler))\n        self.iterator = super().__iter__()\n    def __len__(self):\n        return len(self.batch_sampler.sampler)",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "_RepeatSampler",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "class _RepeatSampler:\n    \"\"\" Sampler that repeats forever\n    Args:\n        sampler (Sampler)\n    \"\"\"\n    def __init__(self, sampler):\n        self.sampler = sampler\n    def __iter__(self):\n        while True:\n            yield from iter(self.sampler)",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "LoadScreenshots",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "class LoadScreenshots:\n    # YOLOv5 screenshot dataloader, i.e. `python detect.py --source \"screen 0 100 100 512 256\"`\n    def __init__(self, source, img_size=640, stride=32, auto=True, transforms=None):\n        # source = [screen_number left top width height] (pixels)\n        check_requirements('mss')\n        import mss\n        source, *params = source.split()\n        self.screen, left, top, width, height = 0, None, None, None, None  # default to full screen 0\n        if len(params) == 1:\n            self.screen = int(params[0])",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "LoadImages",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "class LoadImages:\n    # YOLOv5 image/video dataloader, i.e. `python detect.py --source image.jpg/vid.mp4`\n    def __init__(self, path, img_size=640, stride=32, auto=True, transforms=None, vid_stride=1):\n        if isinstance(path, str) and Path(path).suffix == '.txt':  # *.txt file with img/vid/dir on each line\n            path = Path(path).read_text().rsplit()\n        files = []\n        for p in sorted(path) if isinstance(path, (list, tuple)) else [path]:\n            p = str(Path(p).resolve())\n            if '*' in p:\n                files.extend(sorted(glob.glob(p, recursive=True)))  # glob",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "LoadStreams",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "class LoadStreams:\n    # YOLOv5 streamloader, i.e. `python detect.py --source 'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP streams`\n    def __init__(self, sources='file.streams', img_size=640, stride=32, auto=True, transforms=None, vid_stride=1):\n        torch.backends.cudnn.benchmark = True  # faster for fixed-size inference\n        self.mode = 'stream'\n        self.img_size = img_size\n        self.stride = stride\n        self.vid_stride = vid_stride  # video frame-rate stride\n        sources = Path(sources).read_text().rsplit() if os.path.isfile(sources) else [sources]\n        n = len(sources)",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "LoadImagesAndLabels",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "class LoadImagesAndLabels(Dataset):\n    # YOLOv5 train_loader/val_loader, loads images and labels for training and validation\n    cache_version = 0.6  # dataset labels *.cache version\n    rand_interp_methods = [cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4]\n    def __init__(self,\n                 path,\n                 img_size=640,\n                 batch_size=16,\n                 augment=False,\n                 hyp=None,",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "HUBDatasetStats",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "class HUBDatasetStats():\n    \"\"\" Class for generating HUB dataset JSON and `-hub` dataset directory\n    Arguments\n        path:           Path to data.yaml or data.zip (with data.yaml inside data.zip)\n        autodownload:   Attempt to download dataset if not found locally\n    Usage\n        from utils.dataloaders import HUBDatasetStats\n        stats = HUBDatasetStats('coco128.yaml', autodownload=True)  # usage 1\n        stats = HUBDatasetStats('path/to/coco128.zip')  # usage 2\n        stats.get_json(save=False)",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "ClassificationDataset",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "class ClassificationDataset(torchvision.datasets.ImageFolder):\n    \"\"\"\n    YOLOv5 Classification Dataset.\n    Arguments\n        root:  Dataset path\n        transform:  torchvision transforms, used by default\n        album_transform: Albumentations transforms, used if installed\n    \"\"\"\n    def __init__(self, root, augment, imgsz, cache=False):\n        super().__init__(root=root)",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "get_hash",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "def get_hash(paths):\n    # Returns a single hash value of a list of paths (files or dirs)\n    size = sum(os.path.getsize(p) for p in paths if os.path.exists(p))  # sizes\n    h = hashlib.sha256(str(size).encode())  # hash sizes\n    h.update(''.join(paths).encode())  # hash paths\n    return h.hexdigest()  # return hash\ndef exif_size(img):\n    # Returns exif-corrected PIL size\n    s = img.size  # (width, height)\n    with contextlib.suppress(Exception):",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "exif_size",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "def exif_size(img):\n    # Returns exif-corrected PIL size\n    s = img.size  # (width, height)\n    with contextlib.suppress(Exception):\n        rotation = dict(img._getexif().items())[orientation]\n        if rotation in [6, 8]:  # rotation 270 or 90\n            s = (s[1], s[0])\n    return s\ndef exif_transpose(image):\n    \"\"\"",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "exif_transpose",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "def exif_transpose(image):\n    \"\"\"\n    Transpose a PIL image accordingly if it has an EXIF Orientation tag.\n    Inplace version of https://github.com/python-pillow/Pillow/blob/master/src/PIL/ImageOps.py exif_transpose()\n    :param image: The image to transpose.\n    :return: An image.\n    \"\"\"\n    exif = image.getexif()\n    orientation = exif.get(0x0112, 1)  # default 1\n    if orientation > 1:",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "seed_worker",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "def seed_worker(worker_id):\n    # Set dataloader worker seed https://pytorch.org/docs/stable/notes/randomness.html#dataloader\n    worker_seed = torch.initial_seed() % 2 ** 32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\ndef create_dataloader(path,\n                      imgsz,\n                      batch_size,\n                      stride,\n                      single_cls=False,",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "create_dataloader",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "def create_dataloader(path,\n                      imgsz,\n                      batch_size,\n                      stride,\n                      single_cls=False,\n                      hyp=None,\n                      augment=False,\n                      cache=False,\n                      pad=0.0,\n                      rect=False,",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "img2label_paths",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "def img2label_paths(img_paths):\n    # Define label paths as a function of image paths\n    sa, sb = f'{os.sep}images{os.sep}', f'{os.sep}labels{os.sep}'  # /images/, /labels/ substrings\n    return [sb.join(x.rsplit(sa, 1)).rsplit('.', 1)[0] + '.txt' for x in img_paths]\nclass LoadImagesAndLabels(Dataset):\n    # YOLOv5 train_loader/val_loader, loads images and labels for training and validation\n    cache_version = 0.6  # dataset labels *.cache version\n    rand_interp_methods = [cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4]\n    def __init__(self,\n                 path,",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "flatten_recursive",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "def flatten_recursive(path=DATASETS_DIR / 'coco128'):\n    # Flatten a recursive directory by bringing all files to top level\n    new_path = Path(f'{str(path)}_flat')\n    if os.path.exists(new_path):\n        shutil.rmtree(new_path)  # delete output folder\n    os.makedirs(new_path)  # make new output folder\n    for file in tqdm(glob.glob(f'{str(Path(path))}/**/*.*', recursive=True)):\n        shutil.copyfile(file, new_path / Path(file).name)\ndef extract_boxes(path=DATASETS_DIR / 'coco128'):  # from utils.dataloaders import *; extract_boxes()\n    # Convert detection dataset into classification dataset, with one directory per class",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "extract_boxes",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "def extract_boxes(path=DATASETS_DIR / 'coco128'):  # from utils.dataloaders import *; extract_boxes()\n    # Convert detection dataset into classification dataset, with one directory per class\n    path = Path(path)  # images dir\n    shutil.rmtree(path / 'classification') if (path / 'classification').is_dir() else None  # remove existing\n    files = list(path.rglob('*.*'))\n    n = len(files)  # number of files\n    for im_file in tqdm(files, total=n):\n        if im_file.suffix[1:] in IMG_FORMATS:\n            # image\n            im = cv2.imread(str(im_file))[..., ::-1]  # BGR to RGB",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "autosplit",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "def autosplit(path=DATASETS_DIR / 'coco128/images', weights=(0.9, 0.1, 0.0), annotated_only=False):\n    \"\"\" Autosplit a dataset into train/val/test splits and save path/autosplit_*.txt files\n    Usage: from utils.dataloaders import *; autosplit()\n    Arguments\n        path:            Path to images directory\n        weights:         Train, val, test weights (list, tuple)\n        annotated_only:  Only use images with an annotated txt file\n    \"\"\"\n    path = Path(path)  # images dir\n    files = sorted(x for x in path.rglob('*.*') if x.suffix[1:].lower() in IMG_FORMATS)  # image files only",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "verify_image_label",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "def verify_image_label(args):\n    # Verify one image-label pair\n    im_file, lb_file, prefix = args\n    nm, nf, ne, nc, msg, segments = 0, 0, 0, 0, '', []  # number (missing, found, empty, corrupt), message, segments\n    try:\n        # verify images\n        im = Image.open(im_file)\n        im.verify()  # PIL verify\n        shape = exif_size(im)  # image size\n        assert (shape[0] > 9) & (shape[1] > 9), f'image size {shape} <10 pixels'",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "create_classification_dataloader",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "def create_classification_dataloader(path,\n                                     imgsz=224,\n                                     batch_size=16,\n                                     augment=True,\n                                     cache=False,\n                                     rank=-1,\n                                     workers=8,\n                                     shuffle=True):\n    # Returns Dataloader object to be used with YOLOv5 Classifier\n    with torch_distributed_zero_first(rank):  # init dataset *.cache only once if DDP",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "HELP_URL",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "HELP_URL = 'See https://docs.ultralytics.com/yolov5/tutorials/train_custom_data'\nIMG_FORMATS = 'bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp', 'pfm'  # include image suffixes\nVID_FORMATS = 'asf', 'avi', 'gif', 'm4v', 'mkv', 'mov', 'mp4', 'mpeg', 'mpg', 'ts', 'wmv'  # include video suffixes\nLOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html\nRANK = int(os.getenv('RANK', -1))\nPIN_MEMORY = str(os.getenv('PIN_MEMORY', True)).lower() == 'true'  # global pin_memory for dataloaders\n# Get orientation exif tag\nfor orientation in ExifTags.TAGS.keys():\n    if ExifTags.TAGS[orientation] == 'Orientation':\n        break",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "IMG_FORMATS",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "IMG_FORMATS = 'bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp', 'pfm'  # include image suffixes\nVID_FORMATS = 'asf', 'avi', 'gif', 'm4v', 'mkv', 'mov', 'mp4', 'mpeg', 'mpg', 'ts', 'wmv'  # include video suffixes\nLOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html\nRANK = int(os.getenv('RANK', -1))\nPIN_MEMORY = str(os.getenv('PIN_MEMORY', True)).lower() == 'true'  # global pin_memory for dataloaders\n# Get orientation exif tag\nfor orientation in ExifTags.TAGS.keys():\n    if ExifTags.TAGS[orientation] == 'Orientation':\n        break\ndef get_hash(paths):",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "VID_FORMATS",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "VID_FORMATS = 'asf', 'avi', 'gif', 'm4v', 'mkv', 'mov', 'mp4', 'mpeg', 'mpg', 'ts', 'wmv'  # include video suffixes\nLOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html\nRANK = int(os.getenv('RANK', -1))\nPIN_MEMORY = str(os.getenv('PIN_MEMORY', True)).lower() == 'true'  # global pin_memory for dataloaders\n# Get orientation exif tag\nfor orientation in ExifTags.TAGS.keys():\n    if ExifTags.TAGS[orientation] == 'Orientation':\n        break\ndef get_hash(paths):\n    # Returns a single hash value of a list of paths (files or dirs)",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "LOCAL_RANK",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html\nRANK = int(os.getenv('RANK', -1))\nPIN_MEMORY = str(os.getenv('PIN_MEMORY', True)).lower() == 'true'  # global pin_memory for dataloaders\n# Get orientation exif tag\nfor orientation in ExifTags.TAGS.keys():\n    if ExifTags.TAGS[orientation] == 'Orientation':\n        break\ndef get_hash(paths):\n    # Returns a single hash value of a list of paths (files or dirs)\n    size = sum(os.path.getsize(p) for p in paths if os.path.exists(p))  # sizes",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "RANK",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "RANK = int(os.getenv('RANK', -1))\nPIN_MEMORY = str(os.getenv('PIN_MEMORY', True)).lower() == 'true'  # global pin_memory for dataloaders\n# Get orientation exif tag\nfor orientation in ExifTags.TAGS.keys():\n    if ExifTags.TAGS[orientation] == 'Orientation':\n        break\ndef get_hash(paths):\n    # Returns a single hash value of a list of paths (files or dirs)\n    size = sum(os.path.getsize(p) for p in paths if os.path.exists(p))  # sizes\n    h = hashlib.sha256(str(size).encode())  # hash sizes",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "PIN_MEMORY",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.dataloaders",
        "description": "recommendation_assets.yolov5.utils.dataloaders",
        "peekOfCode": "PIN_MEMORY = str(os.getenv('PIN_MEMORY', True)).lower() == 'true'  # global pin_memory for dataloaders\n# Get orientation exif tag\nfor orientation in ExifTags.TAGS.keys():\n    if ExifTags.TAGS[orientation] == 'Orientation':\n        break\ndef get_hash(paths):\n    # Returns a single hash value of a list of paths (files or dirs)\n    size = sum(os.path.getsize(p) for p in paths if os.path.exists(p))  # sizes\n    h = hashlib.sha256(str(size).encode())  # hash sizes\n    h.update(''.join(paths).encode())  # hash paths",
        "detail": "recommendation_assets.yolov5.utils.dataloaders",
        "documentation": {}
    },
    {
        "label": "is_url",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.downloads",
        "description": "recommendation_assets.yolov5.utils.downloads",
        "peekOfCode": "def is_url(url, check=True):\n    # Check if string is URL and check if URL exists\n    try:\n        url = str(url)\n        result = urllib.parse.urlparse(url)\n        assert all([result.scheme, result.netloc])  # check if is url\n        return (urllib.request.urlopen(url).getcode() == 200) if check else True  # check if exists online\n    except (AssertionError, urllib.request.HTTPError):\n        return False\ndef gsutil_getsize(url=''):",
        "detail": "recommendation_assets.yolov5.utils.downloads",
        "documentation": {}
    },
    {
        "label": "gsutil_getsize",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.downloads",
        "description": "recommendation_assets.yolov5.utils.downloads",
        "peekOfCode": "def gsutil_getsize(url=''):\n    # gs://bucket/file size https://cloud.google.com/storage/docs/gsutil/commands/du\n    output = subprocess.check_output(['gsutil', 'du', url], shell=True, encoding='utf-8')\n    if output:\n        return int(output.split()[0])\n    return 0\ndef url_getsize(url='https://ultralytics.com/images/bus.jpg'):\n    # Return downloadable file size in bytes\n    response = requests.head(url, allow_redirects=True)\n    return int(response.headers.get('content-length', -1))",
        "detail": "recommendation_assets.yolov5.utils.downloads",
        "documentation": {}
    },
    {
        "label": "url_getsize",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.downloads",
        "description": "recommendation_assets.yolov5.utils.downloads",
        "peekOfCode": "def url_getsize(url='https://ultralytics.com/images/bus.jpg'):\n    # Return downloadable file size in bytes\n    response = requests.head(url, allow_redirects=True)\n    return int(response.headers.get('content-length', -1))\ndef curl_download(url, filename, *, silent: bool = False) -> bool:\n    \"\"\"\n    Download a file from a url to a filename using curl.\n    \"\"\"\n    silent_option = 'sS' if silent else ''  # silent\n    proc = subprocess.run([",
        "detail": "recommendation_assets.yolov5.utils.downloads",
        "documentation": {}
    },
    {
        "label": "curl_download",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.downloads",
        "description": "recommendation_assets.yolov5.utils.downloads",
        "peekOfCode": "def curl_download(url, filename, *, silent: bool = False) -> bool:\n    \"\"\"\n    Download a file from a url to a filename using curl.\n    \"\"\"\n    silent_option = 'sS' if silent else ''  # silent\n    proc = subprocess.run([\n        'curl',\n        '-#',\n        f'-{silent_option}L',\n        url,",
        "detail": "recommendation_assets.yolov5.utils.downloads",
        "documentation": {}
    },
    {
        "label": "safe_download",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.downloads",
        "description": "recommendation_assets.yolov5.utils.downloads",
        "peekOfCode": "def safe_download(file, url, url2=None, min_bytes=1E0, error_msg=''):\n    # Attempts to download file from url or url2, checks and removes incomplete downloads < min_bytes\n    from utils.general import LOGGER\n    file = Path(file)\n    assert_msg = f\"Downloaded file '{file}' does not exist or size is < min_bytes={min_bytes}\"\n    try:  # url1\n        LOGGER.info(f'Downloading {url} to {file}...')\n        torch.hub.download_url_to_file(url, str(file), progress=LOGGER.level <= logging.INFO)\n        assert file.exists() and file.stat().st_size > min_bytes, assert_msg  # check\n    except Exception as e:  # url2",
        "detail": "recommendation_assets.yolov5.utils.downloads",
        "documentation": {}
    },
    {
        "label": "attempt_download",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.downloads",
        "description": "recommendation_assets.yolov5.utils.downloads",
        "peekOfCode": "def attempt_download(file, repo='ultralytics/yolov5', release='v7.0'):\n    # Attempt file download from GitHub release assets if not found locally. release = 'latest', 'v7.0', etc.\n    from utils.general import LOGGER\n    def github_assets(repository, version='latest'):\n        # Return GitHub repo tag (i.e. 'v7.0') and assets (i.e. ['yolov5s.pt', 'yolov5m.pt', ...])\n        if version != 'latest':\n            version = f'tags/{version}'  # i.e. tags/v7.0\n        response = requests.get(f'https://api.github.com/repos/{repository}/releases/{version}').json()  # github api\n        return response['tag_name'], [x['name'] for x in response['assets']]  # tag, assets\n    file = Path(str(file).strip().replace(\"'\", ''))",
        "detail": "recommendation_assets.yolov5.utils.downloads",
        "documentation": {}
    },
    {
        "label": "Profile",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "class Profile(contextlib.ContextDecorator):\n    # YOLOv5 Profile class. Usage: @Profile() decorator or 'with Profile():' context manager\n    def __init__(self, t=0.0):\n        self.t = t\n        self.cuda = torch.cuda.is_available()\n    def __enter__(self):\n        self.start = self.time()\n        return self\n    def __exit__(self, type, value, traceback):\n        self.dt = self.time() - self.start  # delta-time",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "Timeout",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "class Timeout(contextlib.ContextDecorator):\n    # YOLOv5 Timeout class. Usage: @Timeout(seconds) decorator or 'with Timeout(seconds):' context manager\n    def __init__(self, seconds, *, timeout_msg='', suppress_timeout_errors=True):\n        self.seconds = int(seconds)\n        self.timeout_message = timeout_msg\n        self.suppress = bool(suppress_timeout_errors)\n    def _timeout_handler(self, signum, frame):\n        raise TimeoutError(self.timeout_message)\n    def __enter__(self):\n        if platform.system() != 'Windows':  # not supported on Windows",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "WorkingDirectory",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "class WorkingDirectory(contextlib.ContextDecorator):\n    # Usage: @WorkingDirectory(dir) decorator or 'with WorkingDirectory(dir):' context manager\n    def __init__(self, new_dir):\n        self.dir = new_dir  # new dir\n        self.cwd = Path.cwd().resolve()  # current dir\n    def __enter__(self):\n        os.chdir(self.dir)\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        os.chdir(self.cwd)\ndef methods(instance):",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "is_ascii",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def is_ascii(s=''):\n    # Is string composed of all ASCII (no UTF) characters? (note str().isascii() introduced in python 3.7)\n    s = str(s)  # convert list, tuple, None, etc. to str\n    return len(s.encode().decode('ascii', 'ignore')) == len(s)\ndef is_chinese(s=''):\n    # Is string composed of any Chinese characters?\n    return bool(re.search('[\\u4e00-\\u9fff]', str(s)))\ndef is_colab():\n    # Is environment a Google Colab instance?\n    return 'google.colab' in sys.modules",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "is_chinese",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def is_chinese(s=''):\n    # Is string composed of any Chinese characters?\n    return bool(re.search('[\\u4e00-\\u9fff]', str(s)))\ndef is_colab():\n    # Is environment a Google Colab instance?\n    return 'google.colab' in sys.modules\ndef is_jupyter():\n    \"\"\"\n    Check if the current script is running inside a Jupyter Notebook.\n    Verified on Colab, Jupyterlab, Kaggle, Paperspace.",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "is_colab",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def is_colab():\n    # Is environment a Google Colab instance?\n    return 'google.colab' in sys.modules\ndef is_jupyter():\n    \"\"\"\n    Check if the current script is running inside a Jupyter Notebook.\n    Verified on Colab, Jupyterlab, Kaggle, Paperspace.\n    Returns:\n        bool: True if running inside a Jupyter Notebook, False otherwise.\n    \"\"\"",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "is_jupyter",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def is_jupyter():\n    \"\"\"\n    Check if the current script is running inside a Jupyter Notebook.\n    Verified on Colab, Jupyterlab, Kaggle, Paperspace.\n    Returns:\n        bool: True if running inside a Jupyter Notebook, False otherwise.\n    \"\"\"\n    with contextlib.suppress(Exception):\n        from IPython import get_ipython\n        return get_ipython() is not None",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "is_kaggle",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def is_kaggle():\n    # Is environment a Kaggle Notebook?\n    return os.environ.get('PWD') == '/kaggle/working' and os.environ.get('KAGGLE_URL_BASE') == 'https://www.kaggle.com'\ndef is_docker() -> bool:\n    \"\"\"Check if the process runs inside a docker container.\"\"\"\n    if Path('/.dockerenv').exists():\n        return True\n    try:  # check if docker is in control groups\n        with open('/proc/self/cgroup') as file:\n            return any('docker' in line for line in file)",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "is_docker",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def is_docker() -> bool:\n    \"\"\"Check if the process runs inside a docker container.\"\"\"\n    if Path('/.dockerenv').exists():\n        return True\n    try:  # check if docker is in control groups\n        with open('/proc/self/cgroup') as file:\n            return any('docker' in line for line in file)\n    except OSError:\n        return False\ndef is_writeable(dir, test=False):",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "is_writeable",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def is_writeable(dir, test=False):\n    # Return True if directory has write permissions, test opening a file with write permissions if test=True\n    if not test:\n        return os.access(dir, os.W_OK)  # possible issues on Windows\n    file = Path(dir) / 'tmp.txt'\n    try:\n        with open(file, 'w'):  # open file with write permissions\n            pass\n        file.unlink()  # remove file\n        return True",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "set_logging",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def set_logging(name=LOGGING_NAME, verbose=True):\n    # sets up logging for the given name\n    rank = int(os.getenv('RANK', -1))  # rank in world for Multi-GPU trainings\n    level = logging.INFO if verbose and rank in {-1, 0} else logging.ERROR\n    logging.config.dictConfig({\n        'version': 1,\n        'disable_existing_loggers': False,\n        'formatters': {\n            name: {\n                'format': '%(message)s'}},",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "user_config_dir",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def user_config_dir(dir='Ultralytics', env_var='YOLOV5_CONFIG_DIR'):\n    # Return path of user configuration directory. Prefer environment variable if exists. Make dir if required.\n    env = os.getenv(env_var)\n    if env:\n        path = Path(env)  # use environment variable\n    else:\n        cfg = {'Windows': 'AppData/Roaming', 'Linux': '.config', 'Darwin': 'Library/Application Support'}  # 3 OS dirs\n        path = Path.home() / cfg.get(platform.system(), '')  # OS-specific config dir\n        path = (path if is_writeable(path) else Path('/tmp')) / dir  # GCP and AWS lambda fix, only /tmp is writeable\n    path.mkdir(exist_ok=True)  # make if required",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "methods",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def methods(instance):\n    # Get class/instance methods\n    return [f for f in dir(instance) if callable(getattr(instance, f)) and not f.startswith('__')]\ndef print_args(args: Optional[dict] = None, show_file=True, show_func=False):\n    # Print function arguments (optional args dict)\n    x = inspect.currentframe().f_back  # previous frame\n    file, _, func, _, _ = inspect.getframeinfo(x)\n    if args is None:  # get args automatically\n        args, _, _, frm = inspect.getargvalues(x)\n        args = {k: v for k, v in frm.items() if k in args}",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "print_args",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def print_args(args: Optional[dict] = None, show_file=True, show_func=False):\n    # Print function arguments (optional args dict)\n    x = inspect.currentframe().f_back  # previous frame\n    file, _, func, _, _ = inspect.getframeinfo(x)\n    if args is None:  # get args automatically\n        args, _, _, frm = inspect.getargvalues(x)\n        args = {k: v for k, v in frm.items() if k in args}\n    try:\n        file = Path(file).resolve().relative_to(ROOT).with_suffix('')\n    except ValueError:",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "init_seeds",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def init_seeds(seed=0, deterministic=False):\n    # Initialize random number generator (RNG) seeds https://pytorch.org/docs/stable/notes/randomness.html\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # for Multi-GPU, exception safe\n    # torch.backends.cudnn.benchmark = True  # AutoBatch problem https://github.com/ultralytics/yolov5/issues/9287\n    if deterministic and check_version(torch.__version__, '1.12.0'):  # https://github.com/ultralytics/yolov5/pull/8213\n        torch.use_deterministic_algorithms(True)",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "intersect_dicts",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def intersect_dicts(da, db, exclude=()):\n    # Dictionary intersection of matching keys and shapes, omitting 'exclude' keys, using da values\n    return {k: v for k, v in da.items() if k in db and all(x not in k for x in exclude) and v.shape == db[k].shape}\ndef get_default_args(func):\n    # Get func() default arguments\n    signature = inspect.signature(func)\n    return {k: v.default for k, v in signature.parameters.items() if v.default is not inspect.Parameter.empty}\ndef get_latest_run(search_dir='.'):\n    # Return path to most recent 'last.pt' in /runs (i.e. to --resume from)\n    last_list = glob.glob(f'{search_dir}/**/last*.pt', recursive=True)",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "get_default_args",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def get_default_args(func):\n    # Get func() default arguments\n    signature = inspect.signature(func)\n    return {k: v.default for k, v in signature.parameters.items() if v.default is not inspect.Parameter.empty}\ndef get_latest_run(search_dir='.'):\n    # Return path to most recent 'last.pt' in /runs (i.e. to --resume from)\n    last_list = glob.glob(f'{search_dir}/**/last*.pt', recursive=True)\n    return max(last_list, key=os.path.getctime) if last_list else ''\ndef file_age(path=__file__):\n    # Return days since last file update",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "get_latest_run",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def get_latest_run(search_dir='.'):\n    # Return path to most recent 'last.pt' in /runs (i.e. to --resume from)\n    last_list = glob.glob(f'{search_dir}/**/last*.pt', recursive=True)\n    return max(last_list, key=os.path.getctime) if last_list else ''\ndef file_age(path=__file__):\n    # Return days since last file update\n    dt = (datetime.now() - datetime.fromtimestamp(Path(path).stat().st_mtime))  # delta\n    return dt.days  # + dt.seconds / 86400  # fractional days\ndef file_date(path=__file__):\n    # Return human-readable file modification date, i.e. '2021-3-26'",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "file_age",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def file_age(path=__file__):\n    # Return days since last file update\n    dt = (datetime.now() - datetime.fromtimestamp(Path(path).stat().st_mtime))  # delta\n    return dt.days  # + dt.seconds / 86400  # fractional days\ndef file_date(path=__file__):\n    # Return human-readable file modification date, i.e. '2021-3-26'\n    t = datetime.fromtimestamp(Path(path).stat().st_mtime)\n    return f'{t.year}-{t.month}-{t.day}'\ndef file_size(path):\n    # Return file/dir size (MB)",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "file_date",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def file_date(path=__file__):\n    # Return human-readable file modification date, i.e. '2021-3-26'\n    t = datetime.fromtimestamp(Path(path).stat().st_mtime)\n    return f'{t.year}-{t.month}-{t.day}'\ndef file_size(path):\n    # Return file/dir size (MB)\n    mb = 1 << 20  # bytes to MiB (1024 ** 2)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / mb",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "file_size",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def file_size(path):\n    # Return file/dir size (MB)\n    mb = 1 << 20  # bytes to MiB (1024 ** 2)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / mb\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob('**/*') if f.is_file()) / mb\n    else:\n        return 0.0",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "check_online",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def check_online():\n    # Check internet connectivity\n    import socket\n    def run_once():\n        # Check once\n        try:\n            socket.create_connection(('1.1.1.1', 443), 5)  # check host accessibility\n            return True\n        except OSError:\n            return False",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "git_describe",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def git_describe(path=ROOT):  # path must be a directory\n    # Return human-readable git description, i.e. v5.0-5-g3e25f1e https://git-scm.com/docs/git-describe\n    try:\n        assert (Path(path) / '.git').is_dir()\n        return check_output(f'git -C {path} describe --tags --long --always', shell=True).decode()[:-1]\n    except Exception:\n        return ''\n@TryExcept()\n@WorkingDirectory(ROOT)\ndef check_git_status(repo='ultralytics/yolov5', branch='master'):",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "check_git_status",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def check_git_status(repo='ultralytics/yolov5', branch='master'):\n    # YOLOv5 status check, recommend 'git pull' if code is out of date\n    url = f'https://github.com/{repo}'\n    msg = f', for updates see {url}'\n    s = colorstr('github: ')  # string\n    assert Path('.git').exists(), s + 'skipping check (not a git repository)' + msg\n    assert check_online(), s + 'skipping check (offline)' + msg\n    splits = re.split(pattern=r'\\s', string=check_output('git remote -v', shell=True).decode())\n    matches = [repo in s for s in splits]\n    if any(matches):",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "check_git_info",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def check_git_info(path='.'):\n    # YOLOv5 git info check, return {remote, branch, commit}\n    check_requirements('gitpython')\n    import git\n    try:\n        repo = git.Repo(path)\n        remote = repo.remotes.origin.url.replace('.git', '')  # i.e. 'https://github.com/ultralytics/yolov5'\n        commit = repo.head.commit.hexsha  # i.e. '3134699c73af83aac2a481435550b968d5792c0d'\n        try:\n            branch = repo.active_branch.name  # i.e. 'main'",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "check_python",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def check_python(minimum='3.8.0'):\n    # Check current python version vs. required python version\n    check_version(platform.python_version(), minimum, name='Python ', hard=True)\ndef check_version(current='0.0.0', minimum='0.0.0', name='version ', pinned=False, hard=False, verbose=False):\n    # Check version vs. required version\n    current, minimum = (pkg.parse_version(x) for x in (current, minimum))\n    result = (current == minimum) if pinned else (current >= minimum)  # bool\n    s = f'WARNING  {name}{minimum} is required by YOLOv5, but {name}{current} is currently installed'  # string\n    if hard:\n        assert result, emojis(s)  # assert min requirements met",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "check_version",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def check_version(current='0.0.0', minimum='0.0.0', name='version ', pinned=False, hard=False, verbose=False):\n    # Check version vs. required version\n    current, minimum = (pkg.parse_version(x) for x in (current, minimum))\n    result = (current == minimum) if pinned else (current >= minimum)  # bool\n    s = f'WARNING  {name}{minimum} is required by YOLOv5, but {name}{current} is currently installed'  # string\n    if hard:\n        assert result, emojis(s)  # assert min requirements met\n    if verbose and not result:\n        LOGGER.warning(s)\n    return result",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "check_img_size",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def check_img_size(imgsz, s=32, floor=0):\n    # Verify image size is a multiple of stride s in each dimension\n    if isinstance(imgsz, int):  # integer i.e. img_size=640\n        new_size = max(make_divisible(imgsz, int(s)), floor)\n    else:  # list i.e. img_size=[640, 480]\n        imgsz = list(imgsz)  # convert to list if tuple\n        new_size = [max(make_divisible(x, int(s)), floor) for x in imgsz]\n    if new_size != imgsz:\n        LOGGER.warning(f'WARNING  --img-size {imgsz} must be multiple of max stride {s}, updating to {new_size}')\n    return new_size",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "check_imshow",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def check_imshow(warn=False):\n    # Check if environment supports image displays\n    try:\n        assert not is_jupyter()\n        assert not is_docker()\n        cv2.imshow('test', np.zeros((1, 1, 3)))\n        cv2.waitKey(1)\n        cv2.destroyAllWindows()\n        cv2.waitKey(1)\n        return True",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "check_suffix",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def check_suffix(file='yolov5s.pt', suffix=('.pt', ), msg=''):\n    # Check file(s) for acceptable suffix\n    if file and suffix:\n        if isinstance(suffix, str):\n            suffix = [suffix]\n        for f in file if isinstance(file, (list, tuple)) else [file]:\n            s = Path(f).suffix.lower()  # file suffix\n            if len(s):\n                assert s in suffix, f'{msg}{f} acceptable suffix is {suffix}'\ndef check_yaml(file, suffix=('.yaml', '.yml')):",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "check_yaml",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def check_yaml(file, suffix=('.yaml', '.yml')):\n    # Search/download YAML file (if necessary) and return path, checking suffix\n    return check_file(file, suffix)\ndef check_file(file, suffix=''):\n    # Search/download file (if necessary) and return path\n    check_suffix(file, suffix)  # optional\n    file = str(file)  # convert to str()\n    if os.path.isfile(file) or not file:  # exists\n        return file\n    elif file.startswith(('http:/', 'https:/')):  # download",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "check_file",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def check_file(file, suffix=''):\n    # Search/download file (if necessary) and return path\n    check_suffix(file, suffix)  # optional\n    file = str(file)  # convert to str()\n    if os.path.isfile(file) or not file:  # exists\n        return file\n    elif file.startswith(('http:/', 'https:/')):  # download\n        url = file  # warning: Pathlib turns :// -> :/\n        file = Path(urllib.parse.unquote(file).split('?')[0]).name  # '%2F' to '/', split https://url.com/file.txt?auth\n        if os.path.isfile(file):",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "check_font",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def check_font(font=FONT, progress=False):\n    # Download font to CONFIG_DIR if necessary\n    font = Path(font)\n    file = CONFIG_DIR / font.name\n    if not font.exists() and not file.exists():\n        url = f'https://ultralytics.com/assets/{font.name}'\n        LOGGER.info(f'Downloading {url} to {file}...')\n        torch.hub.download_url_to_file(url, str(file), progress=progress)\ndef check_dataset(data, autodownload=True):\n    # Download, check and/or unzip dataset if not found locally",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "check_dataset",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def check_dataset(data, autodownload=True):\n    # Download, check and/or unzip dataset if not found locally\n    # Download (optional)\n    extract_dir = ''\n    if isinstance(data, (str, Path)) and (is_zipfile(data) or is_tarfile(data)):\n        download(data, dir=f'{DATASETS_DIR}/{Path(data).stem}', unzip=True, delete=False, curl=False, threads=1)\n        data = next((DATASETS_DIR / Path(data).stem).rglob('*.yaml'))\n        extract_dir, autodownload = data.parent, False\n    # Read yaml (optional)\n    if isinstance(data, (str, Path)):",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "check_amp",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def check_amp(model):\n    # Check PyTorch Automatic Mixed Precision (AMP) functionality. Return True on correct operation\n    from models.common import AutoShape, DetectMultiBackend\n    def amp_allclose(model, im):\n        # All close FP32 vs AMP results\n        m = AutoShape(model, verbose=False)  # model\n        a = m(im).xywhn[0]  # FP32 inference\n        m.amp = True\n        b = m(im).xywhn[0]  # AMP inference\n        return a.shape == b.shape and torch.allclose(a, b, atol=0.1)  # close to 10% absolute tolerance",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "yaml_load",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def yaml_load(file='data.yaml'):\n    # Single-line safe yaml loading\n    with open(file, errors='ignore') as f:\n        return yaml.safe_load(f)\ndef yaml_save(file='data.yaml', data={}):\n    # Single-line safe yaml saving\n    with open(file, 'w') as f:\n        yaml.safe_dump({k: str(v) if isinstance(v, Path) else v for k, v in data.items()}, f, sort_keys=False)\ndef unzip_file(file, path=None, exclude=('.DS_Store', '__MACOSX')):\n    # Unzip a *.zip file to path/, excluding files containing strings in exclude list",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "yaml_save",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def yaml_save(file='data.yaml', data={}):\n    # Single-line safe yaml saving\n    with open(file, 'w') as f:\n        yaml.safe_dump({k: str(v) if isinstance(v, Path) else v for k, v in data.items()}, f, sort_keys=False)\ndef unzip_file(file, path=None, exclude=('.DS_Store', '__MACOSX')):\n    # Unzip a *.zip file to path/, excluding files containing strings in exclude list\n    if path is None:\n        path = Path(file).parent  # default path\n    with ZipFile(file) as zipObj:\n        for f in zipObj.namelist():  # list all archived filenames in the zip",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "unzip_file",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def unzip_file(file, path=None, exclude=('.DS_Store', '__MACOSX')):\n    # Unzip a *.zip file to path/, excluding files containing strings in exclude list\n    if path is None:\n        path = Path(file).parent  # default path\n    with ZipFile(file) as zipObj:\n        for f in zipObj.namelist():  # list all archived filenames in the zip\n            if all(x not in f for x in exclude):\n                zipObj.extract(f, path=path)\ndef url2file(url):\n    # Convert URL to filename, i.e. https://url.com/file.txt?auth -> file.txt",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "url2file",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def url2file(url):\n    # Convert URL to filename, i.e. https://url.com/file.txt?auth -> file.txt\n    url = str(Path(url)).replace(':/', '://')  # Pathlib turns :// -> :/\n    return Path(urllib.parse.unquote(url)).name.split('?')[0]  # '%2F' to '/', split https://url.com/file.txt?auth\ndef download(url, dir='.', unzip=True, delete=True, curl=False, threads=1, retry=3):\n    # Multithreaded file download and unzip function, used in data.yaml for autodownload\n    def download_one(url, dir):\n        # Download 1 file\n        success = True\n        if os.path.isfile(url):",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "download",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def download(url, dir='.', unzip=True, delete=True, curl=False, threads=1, retry=3):\n    # Multithreaded file download and unzip function, used in data.yaml for autodownload\n    def download_one(url, dir):\n        # Download 1 file\n        success = True\n        if os.path.isfile(url):\n            f = Path(url)  # filename\n        else:  # does not exist\n            f = dir / Path(url).name\n            LOGGER.info(f'Downloading {url} to {f}...')",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "make_divisible",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def make_divisible(x, divisor):\n    # Returns nearest x divisible by divisor\n    if isinstance(divisor, torch.Tensor):\n        divisor = int(divisor.max())  # to int\n    return math.ceil(x / divisor) * divisor\ndef clean_str(s):\n    # Cleans a string by replacing special characters with underscore _\n    return re.sub(pattern='[|@#!$%&()=?^*;:,><+]', repl='_', string=s)\ndef one_cycle(y1=0.0, y2=1.0, steps=100):\n    # lambda function for sinusoidal ramp from y1 to y2 https://arxiv.org/pdf/1812.01187.pdf",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "clean_str",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def clean_str(s):\n    # Cleans a string by replacing special characters with underscore _\n    return re.sub(pattern='[|@#!$%&()=?^*;:,><+]', repl='_', string=s)\ndef one_cycle(y1=0.0, y2=1.0, steps=100):\n    # lambda function for sinusoidal ramp from y1 to y2 https://arxiv.org/pdf/1812.01187.pdf\n    return lambda x: ((1 - math.cos(x * math.pi / steps)) / 2) * (y2 - y1) + y1\ndef colorstr(*input):\n    # Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e.  colorstr('blue', 'hello world')\n    *args, string = input if len(input) > 1 else ('blue', 'bold', input[0])  # color arguments, string\n    colors = {",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "one_cycle",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def one_cycle(y1=0.0, y2=1.0, steps=100):\n    # lambda function for sinusoidal ramp from y1 to y2 https://arxiv.org/pdf/1812.01187.pdf\n    return lambda x: ((1 - math.cos(x * math.pi / steps)) / 2) * (y2 - y1) + y1\ndef colorstr(*input):\n    # Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e.  colorstr('blue', 'hello world')\n    *args, string = input if len(input) > 1 else ('blue', 'bold', input[0])  # color arguments, string\n    colors = {\n        'black': '\\033[30m',  # basic colors\n        'red': '\\033[31m',\n        'green': '\\033[32m',",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def colorstr(*input):\n    # Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e.  colorstr('blue', 'hello world')\n    *args, string = input if len(input) > 1 else ('blue', 'bold', input[0])  # color arguments, string\n    colors = {\n        'black': '\\033[30m',  # basic colors\n        'red': '\\033[31m',\n        'green': '\\033[32m',\n        'yellow': '\\033[33m',\n        'blue': '\\033[34m',\n        'magenta': '\\033[35m',",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "labels_to_class_weights",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def labels_to_class_weights(labels, nc=80):\n    # Get class weights (inverse frequency) from training labels\n    if labels[0] is None:  # no labels loaded\n        return torch.Tensor()\n    labels = np.concatenate(labels, 0)  # labels.shape = (866643, 5) for COCO\n    classes = labels[:, 0].astype(int)  # labels = [class xywh]\n    weights = np.bincount(classes, minlength=nc)  # occurrences per class\n    # Prepend gridpoint count (for uCE training)\n    # gpi = ((320 / 32 * np.array([1, 2, 4])) ** 2 * 3).sum()  # gridpoints per image\n    # weights = np.hstack([gpi * len(labels)  - weights.sum() * 9, weights * 9]) ** 0.5  # prepend gridpoints to start",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "labels_to_image_weights",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def labels_to_image_weights(labels, nc=80, class_weights=np.ones(80)):\n    # Produces image weights based on class_weights and image contents\n    # Usage: index = random.choices(range(n), weights=image_weights, k=1)  # weighted image sample\n    class_counts = np.array([np.bincount(x[:, 0].astype(int), minlength=nc) for x in labels])\n    return (class_weights.reshape(1, nc) * class_counts).sum(1)\ndef coco80_to_coco91_class():  # converts 80-index (val2014) to 91-index (paper)\n    # https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n    # a = np.loadtxt('data/coco.names', dtype='str', delimiter='\\n')\n    # b = np.loadtxt('data/coco_paper.names', dtype='str', delimiter='\\n')\n    # x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]  # darknet to coco",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "coco80_to_coco91_class",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def coco80_to_coco91_class():  # converts 80-index (val2014) to 91-index (paper)\n    # https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n    # a = np.loadtxt('data/coco.names', dtype='str', delimiter='\\n')\n    # b = np.loadtxt('data/coco_paper.names', dtype='str', delimiter='\\n')\n    # x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]  # darknet to coco\n    # x2 = [list(b[i] == a).index(True) if any(b[i] == a) else None for i in range(91)]  # coco to darknet\n    return [\n        1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34,\n        35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n        64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "xyxy2xywh",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def xyxy2xywh(x):\n    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n    y[..., 2] = x[..., 2] - x[..., 0]  # width\n    y[..., 3] = x[..., 3] - x[..., 1]  # height\n    return y\ndef xywh2xyxy(x):\n    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "xywh2xyxy",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def xywh2xyxy(x):\n    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n    return y\ndef xywhn2xyxy(x, w=640, h=640, padw=0, padh=0):\n    # Convert nx4 boxes from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "xywhn2xyxy",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def xywhn2xyxy(x, w=640, h=640, padw=0, padh=0):\n    # Convert nx4 boxes from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[..., 0] = w * (x[..., 0] - x[..., 2] / 2) + padw  # top left x\n    y[..., 1] = h * (x[..., 1] - x[..., 3] / 2) + padh  # top left y\n    y[..., 2] = w * (x[..., 0] + x[..., 2] / 2) + padw  # bottom right x\n    y[..., 3] = h * (x[..., 1] + x[..., 3] / 2) + padh  # bottom right y\n    return y\ndef xyxy2xywhn(x, w=640, h=640, clip=False, eps=0.0):\n    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "xyxy2xywhn",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def xyxy2xywhn(x, w=640, h=640, clip=False, eps=0.0):\n    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n    if clip:\n        clip_boxes(x, (h - eps, w - eps))  # warning: inplace clip\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[..., 0] = ((x[..., 0] + x[..., 2]) / 2) / w  # x center\n    y[..., 1] = ((x[..., 1] + x[..., 3]) / 2) / h  # y center\n    y[..., 2] = (x[..., 2] - x[..., 0]) / w  # width\n    y[..., 3] = (x[..., 3] - x[..., 1]) / h  # height\n    return y",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "xyn2xy",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def xyn2xy(x, w=640, h=640, padw=0, padh=0):\n    # Convert normalized segments into pixel segments, shape (n,2)\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[..., 0] = w * x[..., 0] + padw  # top left x\n    y[..., 1] = h * x[..., 1] + padh  # top left y\n    return y\ndef segment2box(segment, width=640, height=640):\n    # Convert 1 segment label to 1 box label, applying inside-image constraint, i.e. (xy1, xy2, ...) to (xyxy)\n    x, y = segment.T  # segment xy\n    inside = (x >= 0) & (y >= 0) & (x <= width) & (y <= height)",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "segment2box",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def segment2box(segment, width=640, height=640):\n    # Convert 1 segment label to 1 box label, applying inside-image constraint, i.e. (xy1, xy2, ...) to (xyxy)\n    x, y = segment.T  # segment xy\n    inside = (x >= 0) & (y >= 0) & (x <= width) & (y <= height)\n    x, y, = x[inside], y[inside]\n    return np.array([x.min(), y.min(), x.max(), y.max()]) if any(x) else np.zeros((1, 4))  # xyxy\ndef segments2boxes(segments):\n    # Convert segment labels to box labels, i.e. (cls, xy1, xy2, ...) to (cls, xywh)\n    boxes = []\n    for s in segments:",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "segments2boxes",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def segments2boxes(segments):\n    # Convert segment labels to box labels, i.e. (cls, xy1, xy2, ...) to (cls, xywh)\n    boxes = []\n    for s in segments:\n        x, y = s.T  # segment xy\n        boxes.append([x.min(), y.min(), x.max(), y.max()])  # cls, xyxy\n    return xyxy2xywh(np.array(boxes))  # cls, xywh\ndef resample_segments(segments, n=1000):\n    # Up-sample an (n,2) segment\n    for i, s in enumerate(segments):",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "resample_segments",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def resample_segments(segments, n=1000):\n    # Up-sample an (n,2) segment\n    for i, s in enumerate(segments):\n        s = np.concatenate((s, s[0:1, :]), axis=0)\n        x = np.linspace(0, len(s) - 1, n)\n        xp = np.arange(len(s))\n        segments[i] = np.concatenate([np.interp(x, xp, s[:, i]) for i in range(2)]).reshape(2, -1).T  # segment xy\n    return segments\ndef scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None):\n    # Rescale boxes (xyxy) from img1_shape to img0_shape",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "scale_boxes",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None):\n    # Rescale boxes (xyxy) from img1_shape to img0_shape\n    if ratio_pad is None:  # calculate from img0_shape\n        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n    else:\n        gain = ratio_pad[0][0]\n        pad = ratio_pad[1]\n    boxes[..., [0, 2]] -= pad[0]  # x padding\n    boxes[..., [1, 3]] -= pad[1]  # y padding",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "scale_segments",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def scale_segments(img1_shape, segments, img0_shape, ratio_pad=None, normalize=False):\n    # Rescale coords (xyxy) from img1_shape to img0_shape\n    if ratio_pad is None:  # calculate from img0_shape\n        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n    else:\n        gain = ratio_pad[0][0]\n        pad = ratio_pad[1]\n    segments[:, 0] -= pad[0]  # x padding\n    segments[:, 1] -= pad[1]  # y padding",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "clip_boxes",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def clip_boxes(boxes, shape):\n    # Clip boxes (xyxy) to image shape (height, width)\n    if isinstance(boxes, torch.Tensor):  # faster individually\n        boxes[..., 0].clamp_(0, shape[1])  # x1\n        boxes[..., 1].clamp_(0, shape[0])  # y1\n        boxes[..., 2].clamp_(0, shape[1])  # x2\n        boxes[..., 3].clamp_(0, shape[0])  # y2\n    else:  # np.array (faster grouped)\n        boxes[..., [0, 2]] = boxes[..., [0, 2]].clip(0, shape[1])  # x1, x2\n        boxes[..., [1, 3]] = boxes[..., [1, 3]].clip(0, shape[0])  # y1, y2",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "clip_segments",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def clip_segments(segments, shape):\n    # Clip segments (xy1,xy2,...) to image shape (height, width)\n    if isinstance(segments, torch.Tensor):  # faster individually\n        segments[:, 0].clamp_(0, shape[1])  # x\n        segments[:, 1].clamp_(0, shape[0])  # y\n    else:  # np.array (faster grouped)\n        segments[:, 0] = segments[:, 0].clip(0, shape[1])  # x\n        segments[:, 1] = segments[:, 1].clip(0, shape[0])  # y\ndef non_max_suppression(\n        prediction,",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "non_max_suppression",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def non_max_suppression(\n        prediction,\n        conf_thres=0.25,\n        iou_thres=0.45,\n        classes=None,\n        agnostic=False,\n        multi_label=False,\n        labels=(),\n        max_det=300,\n        nm=0,  # number of masks",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "strip_optimizer",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def strip_optimizer(f='best.pt', s=''):  # from utils.general import *; strip_optimizer()\n    # Strip optimizer from 'f' to finalize training, optionally save as 's'\n    x = torch.load(f, map_location=torch.device('cpu'))\n    if x.get('ema'):\n        x['model'] = x['ema']  # replace model with ema\n    for k in 'optimizer', 'best_fitness', 'ema', 'updates':  # keys\n        x[k] = None\n    x['epoch'] = -1\n    x['model'].half()  # to FP16\n    for p in x['model'].parameters():",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "print_mutation",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def print_mutation(keys, results, hyp, save_dir, bucket, prefix=colorstr('evolve: ')):\n    evolve_csv = save_dir / 'evolve.csv'\n    evolve_yaml = save_dir / 'hyp_evolve.yaml'\n    keys = tuple(keys) + tuple(hyp.keys())  # [results + hyps]\n    keys = tuple(x.strip() for x in keys)\n    vals = results + tuple(hyp.values())\n    n = len(keys)\n    # Download (optional)\n    if bucket:\n        url = f'gs://{bucket}/evolve.csv'",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "apply_classifier",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def apply_classifier(x, model, img, im0):\n    # Apply a second stage classifier to YOLO outputs\n    # Example model = torchvision.models.__dict__['efficientnet_b0'](pretrained=True).to(device).eval()\n    im0 = [im0] if isinstance(im0, np.ndarray) else im0\n    for i, d in enumerate(x):  # per image\n        if d is not None and len(d):\n            d = d.clone()\n            # Reshape and pad cutouts\n            b = xyxy2xywh(d[:, :4])  # boxes\n            b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # rectangle to square",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "increment_path",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def increment_path(path, exist_ok=False, sep='', mkdir=False):\n    # Increment file or directory path, i.e. runs/exp --> runs/exp{sep}2, runs/exp{sep}3, ... etc.\n    path = Path(path)  # os-agnostic\n    if path.exists() and not exist_ok:\n        path, suffix = (path.with_suffix(''), path.suffix) if path.is_file() else (path, '')\n        # Method 1\n        for n in range(2, 9999):\n            p = f'{path}{sep}{n}{suffix}'  # increment path\n            if not os.path.exists(p):  #\n                break",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "imread",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def imread(filename, flags=cv2.IMREAD_COLOR):\n    return cv2.imdecode(np.fromfile(filename, np.uint8), flags)\ndef imwrite(filename, img):\n    try:\n        cv2.imencode(Path(filename).suffix, img)[1].tofile(filename)\n        return True\n    except Exception:\n        return False\ndef imshow(path, im):\n    imshow_(path.encode('unicode_escape').decode(), im)",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "imwrite",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def imwrite(filename, img):\n    try:\n        cv2.imencode(Path(filename).suffix, img)[1].tofile(filename)\n        return True\n    except Exception:\n        return False\ndef imshow(path, im):\n    imshow_(path.encode('unicode_escape').decode(), im)\nif Path(inspect.stack()[0].filename).parent.parent.as_posix() in inspect.stack()[-1].filename:\n    cv2.imread, cv2.imwrite, cv2.imshow = imread, imwrite, imshow  # redefine",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "imshow",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "def imshow(path, im):\n    imshow_(path.encode('unicode_escape').decode(), im)\nif Path(inspect.stack()[0].filename).parent.parent.as_posix() in inspect.stack()[-1].filename:\n    cv2.imread, cv2.imwrite, cv2.imshow = imread, imwrite, imshow  # redefine\n# Variables ------------------------------------------------------------------------------------------------------------",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[1]  # YOLOv5 root directory\nRANK = int(os.getenv('RANK', -1))\n# Settings\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of YOLOv5 multiprocessing threads\nDATASETS_DIR = Path(os.getenv('YOLOv5_DATASETS_DIR', ROOT.parent / 'datasets'))  # global datasets directory\nAUTOINSTALL = str(os.getenv('YOLOv5_AUTOINSTALL', True)).lower() == 'true'  # global auto-install mode\nVERBOSE = str(os.getenv('YOLOv5_VERBOSE', True)).lower() == 'true'  # global verbose mode\nTQDM_BAR_FORMAT = '{l_bar}{bar:10}{r_bar}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "ROOT = FILE.parents[1]  # YOLOv5 root directory\nRANK = int(os.getenv('RANK', -1))\n# Settings\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of YOLOv5 multiprocessing threads\nDATASETS_DIR = Path(os.getenv('YOLOv5_DATASETS_DIR', ROOT.parent / 'datasets'))  # global datasets directory\nAUTOINSTALL = str(os.getenv('YOLOv5_AUTOINSTALL', True)).lower() == 'true'  # global auto-install mode\nVERBOSE = str(os.getenv('YOLOv5_VERBOSE', True)).lower() == 'true'  # global verbose mode\nTQDM_BAR_FORMAT = '{l_bar}{bar:10}{r_bar}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "RANK",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "RANK = int(os.getenv('RANK', -1))\n# Settings\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of YOLOv5 multiprocessing threads\nDATASETS_DIR = Path(os.getenv('YOLOv5_DATASETS_DIR', ROOT.parent / 'datasets'))  # global datasets directory\nAUTOINSTALL = str(os.getenv('YOLOv5_AUTOINSTALL', True)).lower() == 'true'  # global auto-install mode\nVERBOSE = str(os.getenv('YOLOv5_VERBOSE', True)).lower() == 'true'  # global verbose mode\nTQDM_BAR_FORMAT = '{l_bar}{bar:10}{r_bar}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "NUM_THREADS",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "NUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of YOLOv5 multiprocessing threads\nDATASETS_DIR = Path(os.getenv('YOLOv5_DATASETS_DIR', ROOT.parent / 'datasets'))  # global datasets directory\nAUTOINSTALL = str(os.getenv('YOLOv5_AUTOINSTALL', True)).lower() == 'true'  # global auto-install mode\nVERBOSE = str(os.getenv('YOLOv5_VERBOSE', True)).lower() == 'true'  # global verbose mode\nTQDM_BAR_FORMAT = '{l_bar}{bar:10}{r_bar}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "DATASETS_DIR",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "DATASETS_DIR = Path(os.getenv('YOLOv5_DATASETS_DIR', ROOT.parent / 'datasets'))  # global datasets directory\nAUTOINSTALL = str(os.getenv('YOLOv5_AUTOINSTALL', True)).lower() == 'true'  # global auto-install mode\nVERBOSE = str(os.getenv('YOLOv5_VERBOSE', True)).lower() == 'true'  # global verbose mode\nTQDM_BAR_FORMAT = '{l_bar}{bar:10}{r_bar}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "AUTOINSTALL",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "AUTOINSTALL = str(os.getenv('YOLOv5_AUTOINSTALL', True)).lower() == 'true'  # global auto-install mode\nVERBOSE = str(os.getenv('YOLOv5_VERBOSE', True)).lower() == 'true'  # global verbose mode\nTQDM_BAR_FORMAT = '{l_bar}{bar:10}{r_bar}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads\nos.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "VERBOSE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "VERBOSE = str(os.getenv('YOLOv5_VERBOSE', True)).lower() == 'true'  # global verbose mode\nTQDM_BAR_FORMAT = '{l_bar}{bar:10}{r_bar}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads\nos.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # suppress verbose TF compiler warnings in Colab",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "TQDM_BAR_FORMAT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "TQDM_BAR_FORMAT = '{l_bar}{bar:10}{r_bar}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads\nos.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # suppress verbose TF compiler warnings in Colab\ndef is_ascii(s=''):",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "FONT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "FONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads\nos.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # suppress verbose TF compiler warnings in Colab\ndef is_ascii(s=''):\n    # Is string composed of all ASCII (no UTF) characters? (note str().isascii() introduced in python 3.7)",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "pd.options.display.max_columns",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "pd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads\nos.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # suppress verbose TF compiler warnings in Colab\ndef is_ascii(s=''):\n    # Is string composed of all ASCII (no UTF) characters? (note str().isascii() introduced in python 3.7)\n    s = str(s)  # convert list, tuple, None, etc. to str\n    return len(s.encode().decode('ascii', 'ignore')) == len(s)\ndef is_chinese(s=''):",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "os.environ['NUMEXPR_MAX_THREADS']",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "os.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads\nos.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # suppress verbose TF compiler warnings in Colab\ndef is_ascii(s=''):\n    # Is string composed of all ASCII (no UTF) characters? (note str().isascii() introduced in python 3.7)\n    s = str(s)  # convert list, tuple, None, etc. to str\n    return len(s.encode().decode('ascii', 'ignore')) == len(s)\ndef is_chinese(s=''):\n    # Is string composed of any Chinese characters?\n    return bool(re.search('[\\u4e00-\\u9fff]', str(s)))",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "os.environ['OMP_NUM_THREADS']",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "os.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # suppress verbose TF compiler warnings in Colab\ndef is_ascii(s=''):\n    # Is string composed of all ASCII (no UTF) characters? (note str().isascii() introduced in python 3.7)\n    s = str(s)  # convert list, tuple, None, etc. to str\n    return len(s.encode().decode('ascii', 'ignore')) == len(s)\ndef is_chinese(s=''):\n    # Is string composed of any Chinese characters?\n    return bool(re.search('[\\u4e00-\\u9fff]', str(s)))\ndef is_colab():",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "os.environ['TF_CPP_MIN_LOG_LEVEL']",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # suppress verbose TF compiler warnings in Colab\ndef is_ascii(s=''):\n    # Is string composed of all ASCII (no UTF) characters? (note str().isascii() introduced in python 3.7)\n    s = str(s)  # convert list, tuple, None, etc. to str\n    return len(s.encode().decode('ascii', 'ignore')) == len(s)\ndef is_chinese(s=''):\n    # Is string composed of any Chinese characters?\n    return bool(re.search('[\\u4e00-\\u9fff]', str(s)))\ndef is_colab():\n    # Is environment a Google Colab instance?",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGING_NAME",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "LOGGING_NAME = 'yolov5'\ndef set_logging(name=LOGGING_NAME, verbose=True):\n    # sets up logging for the given name\n    rank = int(os.getenv('RANK', -1))  # rank in world for Multi-GPU trainings\n    level = logging.INFO if verbose and rank in {-1, 0} else logging.ERROR\n    logging.config.dictConfig({\n        'version': 1,\n        'disable_existing_loggers': False,\n        'formatters': {\n            name: {",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "LOGGER",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "LOGGER = logging.getLogger(LOGGING_NAME)  # define globally (used in train.py, val.py, detect.py, etc.)\nif platform.system() == 'Windows':\n    for fn in LOGGER.info, LOGGER.warning:\n        setattr(LOGGER, fn.__name__, lambda x: fn(emojis(x)))  # emoji safe logging\ndef user_config_dir(dir='Ultralytics', env_var='YOLOV5_CONFIG_DIR'):\n    # Return path of user configuration directory. Prefer environment variable if exists. Make dir if required.\n    env = os.getenv(env_var)\n    if env:\n        path = Path(env)  # use environment variable\n    else:",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "CONFIG_DIR",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "CONFIG_DIR = user_config_dir()  # Ultralytics settings dir\nclass Profile(contextlib.ContextDecorator):\n    # YOLOv5 Profile class. Usage: @Profile() decorator or 'with Profile():' context manager\n    def __init__(self, t=0.0):\n        self.t = t\n        self.cuda = torch.cuda.is_available()\n    def __enter__(self):\n        self.start = self.time()\n        return self\n    def __exit__(self, type, value, traceback):",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "imshow_",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.general",
        "description": "recommendation_assets.yolov5.utils.general",
        "peekOfCode": "imshow_ = cv2.imshow  # copy to avoid recursion errors\ndef imread(filename, flags=cv2.IMREAD_COLOR):\n    return cv2.imdecode(np.fromfile(filename, np.uint8), flags)\ndef imwrite(filename, img):\n    try:\n        cv2.imencode(Path(filename).suffix, img)[1].tofile(filename)\n        return True\n    except Exception:\n        return False\ndef imshow(path, im):",
        "detail": "recommendation_assets.yolov5.utils.general",
        "documentation": {}
    },
    {
        "label": "BCEBlurWithLogitsLoss",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.loss",
        "description": "recommendation_assets.yolov5.utils.loss",
        "peekOfCode": "class BCEBlurWithLogitsLoss(nn.Module):\n    # BCEwithLogitLoss() with reduced missing label effects.\n    def __init__(self, alpha=0.05):\n        super().__init__()\n        self.loss_fcn = nn.BCEWithLogitsLoss(reduction='none')  # must be nn.BCEWithLogitsLoss()\n        self.alpha = alpha\n    def forward(self, pred, true):\n        loss = self.loss_fcn(pred, true)\n        pred = torch.sigmoid(pred)  # prob from logits\n        dx = pred - true  # reduce only missing label effects",
        "detail": "recommendation_assets.yolov5.utils.loss",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.loss",
        "description": "recommendation_assets.yolov5.utils.loss",
        "peekOfCode": "class FocalLoss(nn.Module):\n    # Wraps focal loss around existing loss_fcn(), i.e. criteria = FocalLoss(nn.BCEWithLogitsLoss(), gamma=1.5)\n    def __init__(self, loss_fcn, gamma=1.5, alpha=0.25):\n        super().__init__()\n        self.loss_fcn = loss_fcn  # must be nn.BCEWithLogitsLoss()\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = loss_fcn.reduction\n        self.loss_fcn.reduction = 'none'  # required to apply FL to each element\n    def forward(self, pred, true):",
        "detail": "recommendation_assets.yolov5.utils.loss",
        "documentation": {}
    },
    {
        "label": "QFocalLoss",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.loss",
        "description": "recommendation_assets.yolov5.utils.loss",
        "peekOfCode": "class QFocalLoss(nn.Module):\n    # Wraps Quality focal loss around existing loss_fcn(), i.e. criteria = FocalLoss(nn.BCEWithLogitsLoss(), gamma=1.5)\n    def __init__(self, loss_fcn, gamma=1.5, alpha=0.25):\n        super().__init__()\n        self.loss_fcn = loss_fcn  # must be nn.BCEWithLogitsLoss()\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = loss_fcn.reduction\n        self.loss_fcn.reduction = 'none'  # required to apply FL to each element\n    def forward(self, pred, true):",
        "detail": "recommendation_assets.yolov5.utils.loss",
        "documentation": {}
    },
    {
        "label": "ComputeLoss",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.loss",
        "description": "recommendation_assets.yolov5.utils.loss",
        "peekOfCode": "class ComputeLoss:\n    sort_obj_iou = False\n    # Compute losses\n    def __init__(self, model, autobalance=False):\n        device = next(model.parameters()).device  # get model device\n        h = model.hyp  # hyperparameters\n        # Define criteria\n        BCEcls = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h['cls_pw']], device=device))\n        BCEobj = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h['obj_pw']], device=device))\n        # Class label smoothing https://arxiv.org/pdf/1902.04103.pdf eqn 3",
        "detail": "recommendation_assets.yolov5.utils.loss",
        "documentation": {}
    },
    {
        "label": "smooth_BCE",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.loss",
        "description": "recommendation_assets.yolov5.utils.loss",
        "peekOfCode": "def smooth_BCE(eps=0.1):  # https://github.com/ultralytics/yolov3/issues/238#issuecomment-598028441\n    # return positive, negative label smoothing BCE targets\n    return 1.0 - 0.5 * eps, 0.5 * eps\nclass BCEBlurWithLogitsLoss(nn.Module):\n    # BCEwithLogitLoss() with reduced missing label effects.\n    def __init__(self, alpha=0.05):\n        super().__init__()\n        self.loss_fcn = nn.BCEWithLogitsLoss(reduction='none')  # must be nn.BCEWithLogitsLoss()\n        self.alpha = alpha\n    def forward(self, pred, true):",
        "detail": "recommendation_assets.yolov5.utils.loss",
        "documentation": {}
    },
    {
        "label": "ConfusionMatrix",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.metrics",
        "description": "recommendation_assets.yolov5.utils.metrics",
        "peekOfCode": "class ConfusionMatrix:\n    # Updated version of https://github.com/kaanakan/object_detection_confusion_matrix\n    def __init__(self, nc, conf=0.25, iou_thres=0.45):\n        self.matrix = np.zeros((nc + 1, nc + 1))\n        self.nc = nc  # number of classes\n        self.conf = conf\n        self.iou_thres = iou_thres\n    def process_batch(self, detections, labels):\n        \"\"\"\n        Return intersection-over-union (Jaccard index) of boxes.",
        "detail": "recommendation_assets.yolov5.utils.metrics",
        "documentation": {}
    },
    {
        "label": "fitness",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.metrics",
        "description": "recommendation_assets.yolov5.utils.metrics",
        "peekOfCode": "def fitness(x):\n    # Model fitness as a weighted combination of metrics\n    w = [0.0, 0.0, 0.1, 0.9]  # weights for [P, R, mAP@0.5, mAP@0.5:0.95]\n    return (x[:, :4] * w).sum(1)\ndef smooth(y, f=0.05):\n    # Box filter of fraction f\n    nf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\n    p = np.ones(nf // 2)  # ones padding\n    yp = np.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\n    return np.convolve(yp, np.ones(nf) / nf, mode='valid')  # y-smoothed",
        "detail": "recommendation_assets.yolov5.utils.metrics",
        "documentation": {}
    },
    {
        "label": "smooth",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.metrics",
        "description": "recommendation_assets.yolov5.utils.metrics",
        "peekOfCode": "def smooth(y, f=0.05):\n    # Box filter of fraction f\n    nf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\n    p = np.ones(nf // 2)  # ones padding\n    yp = np.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\n    return np.convolve(yp, np.ones(nf) / nf, mode='valid')  # y-smoothed\ndef ap_per_class(tp, conf, pred_cls, target_cls, plot=False, save_dir='.', names=(), eps=1e-16, prefix=''):\n    \"\"\" Compute the average precision, given the recall and precision curves.\n    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n    # Arguments",
        "detail": "recommendation_assets.yolov5.utils.metrics",
        "documentation": {}
    },
    {
        "label": "ap_per_class",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.metrics",
        "description": "recommendation_assets.yolov5.utils.metrics",
        "peekOfCode": "def ap_per_class(tp, conf, pred_cls, target_cls, plot=False, save_dir='.', names=(), eps=1e-16, prefix=''):\n    \"\"\" Compute the average precision, given the recall and precision curves.\n    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n    # Arguments\n        tp:  True positives (nparray, nx1 or nx10).\n        conf:  Objectness value from 0-1 (nparray).\n        pred_cls:  Predicted object classes (nparray).\n        target_cls:  True object classes (nparray).\n        plot:  Plot precision-recall curve at mAP@0.5\n        save_dir:  Plot save directory",
        "detail": "recommendation_assets.yolov5.utils.metrics",
        "documentation": {}
    },
    {
        "label": "compute_ap",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.metrics",
        "description": "recommendation_assets.yolov5.utils.metrics",
        "peekOfCode": "def compute_ap(recall, precision):\n    \"\"\" Compute the average precision, given the recall and precision curves\n    # Arguments\n        recall:    The recall curve (list)\n        precision: The precision curve (list)\n    # Returns\n        Average precision, precision curve, recall curve\n    \"\"\"\n    # Append sentinel values to beginning and end\n    mrec = np.concatenate(([0.0], recall, [1.0]))",
        "detail": "recommendation_assets.yolov5.utils.metrics",
        "documentation": {}
    },
    {
        "label": "bbox_iou",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.metrics",
        "description": "recommendation_assets.yolov5.utils.metrics",
        "peekOfCode": "def bbox_iou(box1, box2, xywh=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-7):\n    # Returns Intersection over Union (IoU) of box1(1,4) to box2(n,4)\n    # Get the coordinates of bounding boxes\n    if xywh:  # transform from xywh to xyxy\n        (x1, y1, w1, h1), (x2, y2, w2, h2) = box1.chunk(4, -1), box2.chunk(4, -1)\n        w1_, h1_, w2_, h2_ = w1 / 2, h1 / 2, w2 / 2, h2 / 2\n        b1_x1, b1_x2, b1_y1, b1_y2 = x1 - w1_, x1 + w1_, y1 - h1_, y1 + h1_\n        b2_x1, b2_x2, b2_y1, b2_y2 = x2 - w2_, x2 + w2_, y2 - h2_, y2 + h2_\n    else:  # x1, y1, x2, y2 = box1\n        b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)",
        "detail": "recommendation_assets.yolov5.utils.metrics",
        "documentation": {}
    },
    {
        "label": "box_iou",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.metrics",
        "description": "recommendation_assets.yolov5.utils.metrics",
        "peekOfCode": "def box_iou(box1, box2, eps=1e-7):\n    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n    \"\"\"\n    Return intersection-over-union (Jaccard index) of boxes.\n    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n    Arguments:\n        box1 (Tensor[N, 4])\n        box2 (Tensor[M, 4])\n    Returns:\n        iou (Tensor[N, M]): the NxM matrix containing the pairwise",
        "detail": "recommendation_assets.yolov5.utils.metrics",
        "documentation": {}
    },
    {
        "label": "bbox_ioa",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.metrics",
        "description": "recommendation_assets.yolov5.utils.metrics",
        "peekOfCode": "def bbox_ioa(box1, box2, eps=1e-7):\n    \"\"\" Returns the intersection over box2 area given box1, box2. Boxes are x1y1x2y2\n    box1:       np.array of shape(4)\n    box2:       np.array of shape(nx4)\n    returns:    np.array of shape(n)\n    \"\"\"\n    # Get the coordinates of bounding boxes\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2.T\n    # Intersection area",
        "detail": "recommendation_assets.yolov5.utils.metrics",
        "documentation": {}
    },
    {
        "label": "wh_iou",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.metrics",
        "description": "recommendation_assets.yolov5.utils.metrics",
        "peekOfCode": "def wh_iou(wh1, wh2, eps=1e-7):\n    # Returns the nxm IoU matrix. wh1 is nx2, wh2 is mx2\n    wh1 = wh1[:, None]  # [N,1,2]\n    wh2 = wh2[None]  # [1,M,2]\n    inter = torch.min(wh1, wh2).prod(2)  # [N,M]\n    return inter / (wh1.prod(2) + wh2.prod(2) - inter + eps)  # iou = inter / (area1 + area2 - inter)\n# Plots ----------------------------------------------------------------------------------------------------------------\n@threaded\ndef plot_pr_curve(px, py, ap, save_dir=Path('pr_curve.png'), names=()):\n    # Precision-recall curve",
        "detail": "recommendation_assets.yolov5.utils.metrics",
        "documentation": {}
    },
    {
        "label": "plot_pr_curve",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.metrics",
        "description": "recommendation_assets.yolov5.utils.metrics",
        "peekOfCode": "def plot_pr_curve(px, py, ap, save_dir=Path('pr_curve.png'), names=()):\n    # Precision-recall curve\n    fig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)\n    py = np.stack(py, axis=1)\n    if 0 < len(names) < 21:  # display per-class legend if < 21 classes\n        for i, y in enumerate(py.T):\n            ax.plot(px, y, linewidth=1, label=f'{names[i]} {ap[i, 0]:.3f}')  # plot(recall, precision)\n    else:\n        ax.plot(px, py, linewidth=1, color='grey')  # plot(recall, precision)\n    ax.plot(px, py.mean(1), linewidth=3, color='blue', label='all classes %.3f mAP@0.5' % ap[:, 0].mean())",
        "detail": "recommendation_assets.yolov5.utils.metrics",
        "documentation": {}
    },
    {
        "label": "plot_mc_curve",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.metrics",
        "description": "recommendation_assets.yolov5.utils.metrics",
        "peekOfCode": "def plot_mc_curve(px, py, save_dir=Path('mc_curve.png'), names=(), xlabel='Confidence', ylabel='Metric'):\n    # Metric-confidence curve\n    fig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)\n    if 0 < len(names) < 21:  # display per-class legend if < 21 classes\n        for i, y in enumerate(py):\n            ax.plot(px, y, linewidth=1, label=f'{names[i]}')  # plot(confidence, metric)\n    else:\n        ax.plot(px, py.T, linewidth=1, color='grey')  # plot(confidence, metric)\n    y = smooth(py.mean(0), 0.05)\n    ax.plot(px, y, linewidth=3, color='blue', label=f'all classes {y.max():.2f} at {px[y.argmax()]:.3f}')",
        "detail": "recommendation_assets.yolov5.utils.metrics",
        "documentation": {}
    },
    {
        "label": "Colors",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.plots",
        "description": "recommendation_assets.yolov5.utils.plots",
        "peekOfCode": "class Colors:\n    # Ultralytics color palette https://ultralytics.com/\n    def __init__(self):\n        # hex = matplotlib.colors.TABLEAU_COLORS.values()\n        hexs = ('FF3838', 'FF9D97', 'FF701F', 'FFB21D', 'CFD231', '48F90A', '92CC17', '3DDB86', '1A9334', '00D4BB',\n                '2C99A8', '00C2FF', '344593', '6473FF', '0018EC', '8438FF', '520085', 'CB38FF', 'FF95C8', 'FF37C7')\n        self.palette = [self.hex2rgb(f'#{c}') for c in hexs]\n        self.n = len(self.palette)\n    def __call__(self, i, bgr=False):\n        c = self.palette[int(i) % self.n]",
        "detail": "recommendation_assets.yolov5.utils.plots",
        "documentation": {}
    },
    {
        "label": "feature_visualization",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.plots",
        "description": "recommendation_assets.yolov5.utils.plots",
        "peekOfCode": "def feature_visualization(x, module_type, stage, n=32, save_dir=Path('runs/detect/exp')):\n    \"\"\"\n    x:              Features to be visualized\n    module_type:    Module type\n    stage:          Module stage within model\n    n:              Maximum number of feature maps to plot\n    save_dir:       Directory to save results\n    \"\"\"\n    if 'Detect' not in module_type:\n        batch, channels, height, width = x.shape  # batch, channels, height, width",
        "detail": "recommendation_assets.yolov5.utils.plots",
        "documentation": {}
    },
    {
        "label": "hist2d",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.plots",
        "description": "recommendation_assets.yolov5.utils.plots",
        "peekOfCode": "def hist2d(x, y, n=100):\n    # 2d histogram used in labels.png and evolve.png\n    xedges, yedges = np.linspace(x.min(), x.max(), n), np.linspace(y.min(), y.max(), n)\n    hist, xedges, yedges = np.histogram2d(x, y, (xedges, yedges))\n    xidx = np.clip(np.digitize(x, xedges) - 1, 0, hist.shape[0] - 1)\n    yidx = np.clip(np.digitize(y, yedges) - 1, 0, hist.shape[1] - 1)\n    return np.log(hist[xidx, yidx])\ndef butter_lowpass_filtfilt(data, cutoff=1500, fs=50000, order=5):\n    from scipy.signal import butter, filtfilt\n    # https://stackoverflow.com/questions/28536191/how-to-filter-smooth-with-scipy-numpy",
        "detail": "recommendation_assets.yolov5.utils.plots",
        "documentation": {}
    },
    {
        "label": "butter_lowpass_filtfilt",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.plots",
        "description": "recommendation_assets.yolov5.utils.plots",
        "peekOfCode": "def butter_lowpass_filtfilt(data, cutoff=1500, fs=50000, order=5):\n    from scipy.signal import butter, filtfilt\n    # https://stackoverflow.com/questions/28536191/how-to-filter-smooth-with-scipy-numpy\n    def butter_lowpass(cutoff, fs, order):\n        nyq = 0.5 * fs\n        normal_cutoff = cutoff / nyq\n        return butter(order, normal_cutoff, btype='low', analog=False)\n    b, a = butter_lowpass(cutoff, fs, order=order)\n    return filtfilt(b, a, data)  # forward-backward filter\ndef output_to_target(output, max_det=300):",
        "detail": "recommendation_assets.yolov5.utils.plots",
        "documentation": {}
    },
    {
        "label": "output_to_target",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.plots",
        "description": "recommendation_assets.yolov5.utils.plots",
        "peekOfCode": "def output_to_target(output, max_det=300):\n    # Convert model output to target format [batch_id, class_id, x, y, w, h, conf] for plotting\n    targets = []\n    for i, o in enumerate(output):\n        box, conf, cls = o[:max_det, :6].cpu().split((4, 1, 1), 1)\n        j = torch.full((conf.shape[0], 1), i)\n        targets.append(torch.cat((j, cls, xyxy2xywh(box), conf), 1))\n    return torch.cat(targets, 0).numpy()\n@threaded\ndef plot_images(images, targets, paths=None, fname='images.jpg', names=None):",
        "detail": "recommendation_assets.yolov5.utils.plots",
        "documentation": {}
    },
    {
        "label": "plot_images",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.plots",
        "description": "recommendation_assets.yolov5.utils.plots",
        "peekOfCode": "def plot_images(images, targets, paths=None, fname='images.jpg', names=None):\n    # Plot image grid with labels\n    if isinstance(images, torch.Tensor):\n        images = images.cpu().float().numpy()\n    if isinstance(targets, torch.Tensor):\n        targets = targets.cpu().numpy()\n    max_size = 1920  # max image size\n    max_subplots = 16  # max image subplots, i.e. 4x4\n    bs, _, h, w = images.shape  # batch size, _, height, width\n    bs = min(bs, max_subplots)  # limit plot images",
        "detail": "recommendation_assets.yolov5.utils.plots",
        "documentation": {}
    },
    {
        "label": "plot_lr_scheduler",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.plots",
        "description": "recommendation_assets.yolov5.utils.plots",
        "peekOfCode": "def plot_lr_scheduler(optimizer, scheduler, epochs=300, save_dir=''):\n    # Plot LR simulating training for full epochs\n    optimizer, scheduler = copy(optimizer), copy(scheduler)  # do not modify originals\n    y = []\n    for _ in range(epochs):\n        scheduler.step()\n        y.append(optimizer.param_groups[0]['lr'])\n    plt.plot(y, '.-', label='LR')\n    plt.xlabel('epoch')\n    plt.ylabel('LR')",
        "detail": "recommendation_assets.yolov5.utils.plots",
        "documentation": {}
    },
    {
        "label": "plot_val_txt",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.plots",
        "description": "recommendation_assets.yolov5.utils.plots",
        "peekOfCode": "def plot_val_txt():  # from utils.plots import *; plot_val()\n    # Plot val.txt histograms\n    x = np.loadtxt('val.txt', dtype=np.float32)\n    box = xyxy2xywh(x[:, :4])\n    cx, cy = box[:, 0], box[:, 1]\n    fig, ax = plt.subplots(1, 1, figsize=(6, 6), tight_layout=True)\n    ax.hist2d(cx, cy, bins=600, cmax=10, cmin=0)\n    ax.set_aspect('equal')\n    plt.savefig('hist2d.png', dpi=300)\n    fig, ax = plt.subplots(1, 2, figsize=(12, 6), tight_layout=True)",
        "detail": "recommendation_assets.yolov5.utils.plots",
        "documentation": {}
    },
    {
        "label": "plot_targets_txt",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.plots",
        "description": "recommendation_assets.yolov5.utils.plots",
        "peekOfCode": "def plot_targets_txt():  # from utils.plots import *; plot_targets_txt()\n    # Plot targets.txt histograms\n    x = np.loadtxt('targets.txt', dtype=np.float32).T\n    s = ['x targets', 'y targets', 'width targets', 'height targets']\n    fig, ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)\n    ax = ax.ravel()\n    for i in range(4):\n        ax[i].hist(x[i], bins=100, label=f'{x[i].mean():.3g} +/- {x[i].std():.3g}')\n        ax[i].legend()\n        ax[i].set_title(s[i])",
        "detail": "recommendation_assets.yolov5.utils.plots",
        "documentation": {}
    },
    {
        "label": "plot_val_study",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.plots",
        "description": "recommendation_assets.yolov5.utils.plots",
        "peekOfCode": "def plot_val_study(file='', dir='', x=None):  # from utils.plots import *; plot_val_study()\n    # Plot file=study.txt generated by val.py (or plot all study*.txt in dir)\n    save_dir = Path(file).parent if file else Path(dir)\n    plot2 = False  # plot additional results\n    if plot2:\n        ax = plt.subplots(2, 4, figsize=(10, 6), tight_layout=True)[1].ravel()\n    fig2, ax2 = plt.subplots(1, 1, figsize=(8, 4), tight_layout=True)\n    # for f in [save_dir / f'study_coco_{x}.txt' for x in ['yolov5n6', 'yolov5s6', 'yolov5m6', 'yolov5l6', 'yolov5x6']]:\n    for f in sorted(save_dir.glob('study*.txt')):\n        y = np.loadtxt(f, dtype=np.float32, usecols=[0, 1, 2, 3, 7, 8, 9], ndmin=2).T",
        "detail": "recommendation_assets.yolov5.utils.plots",
        "documentation": {}
    },
    {
        "label": "plot_labels",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.plots",
        "description": "recommendation_assets.yolov5.utils.plots",
        "peekOfCode": "def plot_labels(labels, names=(), save_dir=Path('')):\n    # plot dataset labels\n    LOGGER.info(f\"Plotting labels to {save_dir / 'labels.jpg'}... \")\n    c, b = labels[:, 0], labels[:, 1:].transpose()  # classes, boxes\n    nc = int(c.max() + 1)  # number of classes\n    x = pd.DataFrame(b.transpose(), columns=['x', 'y', 'width', 'height'])\n    # seaborn correlogram\n    sn.pairplot(x, corner=True, diag_kind='auto', kind='hist', diag_kws=dict(bins=50), plot_kws=dict(pmax=0.9))\n    plt.savefig(save_dir / 'labels_correlogram.jpg', dpi=200)\n    plt.close()",
        "detail": "recommendation_assets.yolov5.utils.plots",
        "documentation": {}
    },
    {
        "label": "imshow_cls",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.plots",
        "description": "recommendation_assets.yolov5.utils.plots",
        "peekOfCode": "def imshow_cls(im, labels=None, pred=None, names=None, nmax=25, verbose=False, f=Path('images.jpg')):\n    # Show classification image grid with labels (optional) and predictions (optional)\n    from utils.augmentations import denormalize\n    names = names or [f'class{i}' for i in range(1000)]\n    blocks = torch.chunk(denormalize(im.clone()).cpu().float(), len(im),\n                         dim=0)  # select batch index 0, block by channels\n    n = min(len(blocks), nmax)  # number of plots\n    m = min(8, round(n ** 0.5))  # 8 x 8 default\n    fig, ax = plt.subplots(math.ceil(n / m), m)  # 8 rows x n/8 cols\n    ax = ax.ravel() if m > 1 else [ax]",
        "detail": "recommendation_assets.yolov5.utils.plots",
        "documentation": {}
    },
    {
        "label": "plot_evolve",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.plots",
        "description": "recommendation_assets.yolov5.utils.plots",
        "peekOfCode": "def plot_evolve(evolve_csv='path/to/evolve.csv'):  # from utils.plots import *; plot_evolve()\n    # Plot evolve.csv hyp evolution results\n    evolve_csv = Path(evolve_csv)\n    data = pd.read_csv(evolve_csv)\n    keys = [x.strip() for x in data.columns]\n    x = data.values\n    f = fitness(x)\n    j = np.argmax(f)  # max fitness index\n    plt.figure(figsize=(10, 12), tight_layout=True)\n    matplotlib.rc('font', **{'size': 8})",
        "detail": "recommendation_assets.yolov5.utils.plots",
        "documentation": {}
    },
    {
        "label": "plot_results",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.plots",
        "description": "recommendation_assets.yolov5.utils.plots",
        "peekOfCode": "def plot_results(file='path/to/results.csv', dir=''):\n    # Plot training results.csv. Usage: from utils.plots import *; plot_results('path/to/results.csv')\n    save_dir = Path(file).parent if file else Path(dir)\n    fig, ax = plt.subplots(2, 5, figsize=(12, 6), tight_layout=True)\n    ax = ax.ravel()\n    files = list(save_dir.glob('results*.csv'))\n    assert len(files), f'No results.csv files found in {save_dir.resolve()}, nothing to plot.'\n    for f in files:\n        try:\n            data = pd.read_csv(f)",
        "detail": "recommendation_assets.yolov5.utils.plots",
        "documentation": {}
    },
    {
        "label": "profile_idetection",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.plots",
        "description": "recommendation_assets.yolov5.utils.plots",
        "peekOfCode": "def profile_idetection(start=0, stop=0, labels=(), save_dir=''):\n    # Plot iDetection '*.txt' per-image logs. from utils.plots import *; profile_idetection()\n    ax = plt.subplots(2, 4, figsize=(12, 6), tight_layout=True)[1].ravel()\n    s = ['Images', 'Free Storage (GB)', 'RAM Usage (GB)', 'Battery', 'dt_raw (ms)', 'dt_smooth (ms)', 'real-world FPS']\n    files = list(Path(save_dir).glob('frames*.txt'))\n    for fi, f in enumerate(files):\n        try:\n            results = np.loadtxt(f, ndmin=2).T[:, 90:-30]  # clip first and last rows\n            n = results.shape[1]  # number of rows\n            x = np.arange(start, min(stop, n) if stop else n)",
        "detail": "recommendation_assets.yolov5.utils.plots",
        "documentation": {}
    },
    {
        "label": "save_one_box",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.plots",
        "description": "recommendation_assets.yolov5.utils.plots",
        "peekOfCode": "def save_one_box(xyxy, im, file=Path('im.jpg'), gain=1.02, pad=10, square=False, BGR=False, save=True):\n    # Save image crop as {file} with crop size multiple {gain} and {pad} pixels. Save and/or return crop\n    xyxy = torch.tensor(xyxy).view(-1, 4)\n    b = xyxy2xywh(xyxy)  # boxes\n    if square:\n        b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # attempt rectangle to square\n    b[:, 2:] = b[:, 2:] * gain + pad  # box wh * gain + pad\n    xyxy = xywh2xyxy(b).long()\n    clip_boxes(xyxy, im.shape)\n    crop = im[int(xyxy[0, 1]):int(xyxy[0, 3]), int(xyxy[0, 0]):int(xyxy[0, 2]), ::(1 if BGR else -1)]",
        "detail": "recommendation_assets.yolov5.utils.plots",
        "documentation": {}
    },
    {
        "label": "RANK",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.plots",
        "description": "recommendation_assets.yolov5.utils.plots",
        "peekOfCode": "RANK = int(os.getenv('RANK', -1))\nmatplotlib.rc('font', **{'size': 11})\nmatplotlib.use('Agg')  # for writing to files only\nclass Colors:\n    # Ultralytics color palette https://ultralytics.com/\n    def __init__(self):\n        # hex = matplotlib.colors.TABLEAU_COLORS.values()\n        hexs = ('FF3838', 'FF9D97', 'FF701F', 'FFB21D', 'CFD231', '48F90A', '92CC17', '3DDB86', '1A9334', '00D4BB',\n                '2C99A8', '00C2FF', '344593', '6473FF', '0018EC', '8438FF', '520085', 'CB38FF', 'FF95C8', 'FF37C7')\n        self.palette = [self.hex2rgb(f'#{c}') for c in hexs]",
        "detail": "recommendation_assets.yolov5.utils.plots",
        "documentation": {}
    },
    {
        "label": "colors",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.plots",
        "description": "recommendation_assets.yolov5.utils.plots",
        "peekOfCode": "colors = Colors()  # create instance for 'from utils.plots import colors'\ndef feature_visualization(x, module_type, stage, n=32, save_dir=Path('runs/detect/exp')):\n    \"\"\"\n    x:              Features to be visualized\n    module_type:    Module type\n    stage:          Module stage within model\n    n:              Maximum number of feature maps to plot\n    save_dir:       Directory to save results\n    \"\"\"\n    if 'Detect' not in module_type:",
        "detail": "recommendation_assets.yolov5.utils.plots",
        "documentation": {}
    },
    {
        "label": "EarlyStopping",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "class EarlyStopping:\n    # YOLOv5 simple early stopper\n    def __init__(self, patience=30):\n        self.best_fitness = 0.0  # i.e. mAP\n        self.best_epoch = 0\n        self.patience = patience or float('inf')  # epochs to wait after fitness stops improving to stop\n        self.possible_stop = False  # possible stop may occur next epoch\n    def __call__(self, epoch, fitness):\n        if fitness >= self.best_fitness:  # >= 0 to allow for early zero-fitness stage of training\n            self.best_epoch = epoch",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "ModelEMA",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "class ModelEMA:\n    \"\"\" Updated Exponential Moving Average (EMA) from https://github.com/rwightman/pytorch-image-models\n    Keeps a moving average of everything in the model state_dict (parameters and buffers)\n    For EMA details see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n    \"\"\"\n    def __init__(self, model, decay=0.9999, tau=2000, updates=0):\n        # Create EMA\n        self.ema = deepcopy(de_parallel(model)).eval()  # FP32 EMA\n        self.updates = updates  # number of EMA updates\n        self.decay = lambda x: decay * (1 - math.exp(-x / tau))  # decay exponential ramp (to help early epochs)",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_inference_mode",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def smart_inference_mode(torch_1_9=check_version(torch.__version__, '1.9.0')):\n    # Applies torch.inference_mode() decorator if torch>=1.9.0 else torch.no_grad() decorator\n    def decorate(fn):\n        return (torch.inference_mode if torch_1_9 else torch.no_grad)()(fn)\n    return decorate\ndef smartCrossEntropyLoss(label_smoothing=0.0):\n    # Returns nn.CrossEntropyLoss with label smoothing enabled for torch>=1.10.0\n    if check_version(torch.__version__, '1.10.0'):\n        return nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n    if label_smoothing > 0:",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smartCrossEntropyLoss",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def smartCrossEntropyLoss(label_smoothing=0.0):\n    # Returns nn.CrossEntropyLoss with label smoothing enabled for torch>=1.10.0\n    if check_version(torch.__version__, '1.10.0'):\n        return nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n    if label_smoothing > 0:\n        LOGGER.warning(f'WARNING  label smoothing {label_smoothing} requires torch>=1.10.0')\n    return nn.CrossEntropyLoss()\ndef smart_DDP(model):\n    # Model DDP creation with checks\n    assert not check_version(torch.__version__, '1.12.0', pinned=True), \\",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_DDP",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def smart_DDP(model):\n    # Model DDP creation with checks\n    assert not check_version(torch.__version__, '1.12.0', pinned=True), \\\n        'torch==1.12.0 torchvision==0.13.0 DDP training is not supported due to a known issue. ' \\\n        'Please upgrade or downgrade torch to use DDP. See https://github.com/ultralytics/yolov5/issues/8395'\n    if check_version(torch.__version__, '1.11.0'):\n        return DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK, static_graph=True)\n    else:\n        return DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK)\ndef reshape_classifier_output(model, n=1000):",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "reshape_classifier_output",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def reshape_classifier_output(model, n=1000):\n    # Update a TorchVision classification model to class count 'n' if required\n    from models.common import Classify\n    name, m = list((model.model if hasattr(model, 'model') else model).named_children())[-1]  # last module\n    if isinstance(m, Classify):  # YOLOv5 Classify() head\n        if m.linear.out_features != n:\n            m.linear = nn.Linear(m.linear.in_features, n)\n    elif isinstance(m, nn.Linear):  # ResNet, EfficientNet\n        if m.out_features != n:\n            setattr(model, name, nn.Linear(m.in_features, n))",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "torch_distributed_zero_first",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def torch_distributed_zero_first(local_rank: int):\n    # Decorator to make all processes in distributed training wait for each local_master to do something\n    if local_rank not in [-1, 0]:\n        dist.barrier(device_ids=[local_rank])\n    yield\n    if local_rank == 0:\n        dist.barrier(device_ids=[0])\ndef device_count():\n    # Returns number of CUDA devices available. Safe version of torch.cuda.device_count(). Supports Linux and Windows\n    assert platform.system() in ('Linux', 'Windows'), 'device_count() only supported on Linux or Windows'",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "device_count",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def device_count():\n    # Returns number of CUDA devices available. Safe version of torch.cuda.device_count(). Supports Linux and Windows\n    assert platform.system() in ('Linux', 'Windows'), 'device_count() only supported on Linux or Windows'\n    try:\n        cmd = 'nvidia-smi -L | wc -l' if platform.system() == 'Linux' else 'nvidia-smi -L | find /c /v \"\"'  # Windows\n        return int(subprocess.run(cmd, shell=True, capture_output=True, check=True).stdout.decode().split()[-1])\n    except Exception:\n        return 0\ndef select_device(device='', batch_size=0, newline=True):\n    # device = None or 'cpu' or 0 or '0' or '0,1,2,3'",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "select_device",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def select_device(device='', batch_size=0, newline=True):\n    # device = None or 'cpu' or 0 or '0' or '0,1,2,3'\n    s = f'YOLOv5  {git_describe() or file_date()} Python-{platform.python_version()} torch-{torch.__version__} '\n    device = str(device).strip().lower().replace('cuda:', '').replace('none', '')  # to string, 'cuda:0' to '0'\n    cpu = device == 'cpu'\n    mps = device == 'mps'  # Apple Metal Performance Shaders (MPS)\n    if cpu or mps:\n        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force torch.cuda.is_available() = False\n    elif device:  # non-cpu device requested\n        os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable - must be before assert is_available()",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "time_sync",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def time_sync():\n    # PyTorch-accurate time\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    return time.time()\ndef profile(input, ops, n=10, device=None):\n    \"\"\" YOLOv5 speed/memory/FLOPs profiler\n    Usage:\n        input = torch.randn(16, 3, 640, 640)\n        m1 = lambda x: x * torch.sigmoid(x)",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "profile",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def profile(input, ops, n=10, device=None):\n    \"\"\" YOLOv5 speed/memory/FLOPs profiler\n    Usage:\n        input = torch.randn(16, 3, 640, 640)\n        m1 = lambda x: x * torch.sigmoid(x)\n        m2 = nn.SiLU()\n        profile(input, [m1, m2], n=100)  # profile over 100 iterations\n    \"\"\"\n    results = []\n    if not isinstance(device, torch.device):",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "is_parallel",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def is_parallel(model):\n    # Returns True if model is of type DP or DDP\n    return type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)\ndef de_parallel(model):\n    # De-parallelize a model: returns single-GPU model if model is of type DP or DDP\n    return model.module if is_parallel(model) else model\ndef initialize_weights(model):\n    for m in model.modules():\n        t = type(m)\n        if t is nn.Conv2d:",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "de_parallel",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def de_parallel(model):\n    # De-parallelize a model: returns single-GPU model if model is of type DP or DDP\n    return model.module if is_parallel(model) else model\ndef initialize_weights(model):\n    for m in model.modules():\n        t = type(m)\n        if t is nn.Conv2d:\n            pass  # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n        elif t is nn.BatchNorm2d:\n            m.eps = 1e-3",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "initialize_weights",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def initialize_weights(model):\n    for m in model.modules():\n        t = type(m)\n        if t is nn.Conv2d:\n            pass  # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n        elif t is nn.BatchNorm2d:\n            m.eps = 1e-3\n            m.momentum = 0.03\n        elif t in [nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU]:\n            m.inplace = True",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "find_modules",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def find_modules(model, mclass=nn.Conv2d):\n    # Finds layer indices matching module class 'mclass'\n    return [i for i, m in enumerate(model.module_list) if isinstance(m, mclass)]\ndef sparsity(model):\n    # Return global model sparsity\n    a, b = 0, 0\n    for p in model.parameters():\n        a += p.numel()\n        b += (p == 0).sum()\n    return b / a",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "sparsity",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def sparsity(model):\n    # Return global model sparsity\n    a, b = 0, 0\n    for p in model.parameters():\n        a += p.numel()\n        b += (p == 0).sum()\n    return b / a\ndef prune(model, amount=0.3):\n    # Prune model to requested global sparsity\n    import torch.nn.utils.prune as prune",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "prune",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def prune(model, amount=0.3):\n    # Prune model to requested global sparsity\n    import torch.nn.utils.prune as prune\n    for name, m in model.named_modules():\n        if isinstance(m, nn.Conv2d):\n            prune.l1_unstructured(m, name='weight', amount=amount)  # prune\n            prune.remove(m, 'weight')  # make permanent\n    LOGGER.info(f'Model pruned to {sparsity(model):.3g} global sparsity')\ndef fuse_conv_and_bn(conv, bn):\n    # Fuse Conv2d() and BatchNorm2d() layers https://tehnokv.com/posts/fusing-batchnorm-and-conv/",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "fuse_conv_and_bn",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def fuse_conv_and_bn(conv, bn):\n    # Fuse Conv2d() and BatchNorm2d() layers https://tehnokv.com/posts/fusing-batchnorm-and-conv/\n    fusedconv = nn.Conv2d(conv.in_channels,\n                          conv.out_channels,\n                          kernel_size=conv.kernel_size,\n                          stride=conv.stride,\n                          padding=conv.padding,\n                          dilation=conv.dilation,\n                          groups=conv.groups,\n                          bias=True).requires_grad_(False).to(conv.weight.device)",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "model_info",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def model_info(model, verbose=False, imgsz=640):\n    # Model information. img_size may be int or list, i.e. img_size=640 or img_size=[640, 320]\n    n_p = sum(x.numel() for x in model.parameters())  # number parameters\n    n_g = sum(x.numel() for x in model.parameters() if x.requires_grad)  # number gradients\n    if verbose:\n        print(f\"{'layer':>5} {'name':>40} {'gradient':>9} {'parameters':>12} {'shape':>20} {'mu':>10} {'sigma':>10}\")\n        for i, (name, p) in enumerate(model.named_parameters()):\n            name = name.replace('module_list.', '')\n            print('%5g %40s %9s %12g %20s %10.3g %10.3g' %\n                  (i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std()))",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "scale_img",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def scale_img(img, ratio=1.0, same_shape=False, gs=32):  # img(16,3,256,416)\n    # Scales img(bs,3,y,x) by ratio constrained to gs-multiple\n    if ratio == 1.0:\n        return img\n    h, w = img.shape[2:]\n    s = (int(h * ratio), int(w * ratio))  # new size\n    img = F.interpolate(img, size=s, mode='bilinear', align_corners=False)  # resize\n    if not same_shape:  # pad/crop img\n        h, w = (math.ceil(x * ratio / gs) * gs for x in (h, w))\n    return F.pad(img, [0, w - s[1], 0, h - s[0]], value=0.447)  # value = imagenet mean",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "copy_attr",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def copy_attr(a, b, include=(), exclude=()):\n    # Copy attributes from b to a, options to only include [...] and to exclude [...]\n    for k, v in b.__dict__.items():\n        if (len(include) and k not in include) or k.startswith('_') or k in exclude:\n            continue\n        else:\n            setattr(a, k, v)\ndef smart_optimizer(model, name='Adam', lr=0.001, momentum=0.9, decay=1e-5):\n    # YOLOv5 3-param group optimizer: 0) weights with decay, 1) weights no decay, 2) biases no decay\n    g = [], [], []  # optimizer parameter groups",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_optimizer",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def smart_optimizer(model, name='Adam', lr=0.001, momentum=0.9, decay=1e-5):\n    # YOLOv5 3-param group optimizer: 0) weights with decay, 1) weights no decay, 2) biases no decay\n    g = [], [], []  # optimizer parameter groups\n    bn = tuple(v for k, v in nn.__dict__.items() if 'Norm' in k)  # normalization layers, i.e. BatchNorm2d()\n    for v in model.modules():\n        for p_name, p in v.named_parameters(recurse=0):\n            if p_name == 'bias':  # bias (no decay)\n                g[2].append(p)\n            elif p_name == 'weight' and isinstance(v, bn):  # weight (no decay)\n                g[1].append(p)",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_hub_load",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def smart_hub_load(repo='ultralytics/yolov5', model='yolov5s', **kwargs):\n    # YOLOv5 torch.hub.load() wrapper with smart error/issue handling\n    if check_version(torch.__version__, '1.9.1'):\n        kwargs['skip_validation'] = True  # validation causes GitHub API rate limit errors\n    if check_version(torch.__version__, '1.12.0'):\n        kwargs['trust_repo'] = True  # argument required starting in torch 0.12\n    try:\n        return torch.hub.load(repo, model, **kwargs)\n    except Exception:\n        return torch.hub.load(repo, model, force_reload=True, **kwargs)",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "smart_resume",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "def smart_resume(ckpt, optimizer, ema=None, weights='yolov5s.pt', epochs=300, resume=True):\n    # Resume training from a partially trained checkpoint\n    best_fitness = 0.0\n    start_epoch = ckpt['epoch'] + 1\n    if ckpt['optimizer'] is not None:\n        optimizer.load_state_dict(ckpt['optimizer'])  # optimizer\n        best_fitness = ckpt['best_fitness']\n    if ema and ckpt.get('ema'):\n        ema.ema.load_state_dict(ckpt['ema'].float().state_dict())  # EMA\n        ema.updates = ckpt['updates']",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "LOCAL_RANK",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html\nRANK = int(os.getenv('RANK', -1))\nWORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))\ntry:\n    import thop  # for FLOPs computation\nexcept ImportError:\n    thop = None\n# Suppress PyTorch warnings\nwarnings.filterwarnings('ignore', message='User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\nwarnings.filterwarnings('ignore', category=UserWarning)",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "RANK",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "RANK = int(os.getenv('RANK', -1))\nWORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))\ntry:\n    import thop  # for FLOPs computation\nexcept ImportError:\n    thop = None\n# Suppress PyTorch warnings\nwarnings.filterwarnings('ignore', message='User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\nwarnings.filterwarnings('ignore', category=UserWarning)\ndef smart_inference_mode(torch_1_9=check_version(torch.__version__, '1.9.0')):",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "WORLD_SIZE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.utils.torch_utils",
        "description": "recommendation_assets.yolov5.utils.torch_utils",
        "peekOfCode": "WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))\ntry:\n    import thop  # for FLOPs computation\nexcept ImportError:\n    thop = None\n# Suppress PyTorch warnings\nwarnings.filterwarnings('ignore', message='User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\nwarnings.filterwarnings('ignore', category=UserWarning)\ndef smart_inference_mode(torch_1_9=check_version(torch.__version__, '1.9.0')):\n    # Applies torch.inference_mode() decorator if torch>=1.9.0 else torch.no_grad() decorator",
        "detail": "recommendation_assets.yolov5.utils.torch_utils",
        "documentation": {}
    },
    {
        "label": "TritonRemoteModel",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.utils.triton",
        "description": "recommendation_assets.yolov5.utils.triton",
        "peekOfCode": "class TritonRemoteModel:\n    \"\"\" A wrapper over a model served by the Triton Inference Server. It can\n    be configured to communicate over GRPC or HTTP. It accepts Torch Tensors\n    as input and returns them as outputs.\n    \"\"\"\n    def __init__(self, url: str):\n        \"\"\"\n        Keyword arguments:\n        url: Fully qualified address of the Triton server - for e.g. grpc://localhost:8000\n        \"\"\"",
        "detail": "recommendation_assets.yolov5.utils.triton",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.benchmarks",
        "description": "recommendation_assets.yolov5.benchmarks",
        "peekOfCode": "def run(\n        weights=ROOT / 'yolov5s.pt',  # weights path\n        imgsz=640,  # inference size (pixels)\n        batch_size=1,  # batch size\n        data=ROOT / 'data/coco128.yaml',  # dataset.yaml path\n        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n        half=False,  # use FP16 half-precision inference\n        test=False,  # test exports only\n        pt_only=False,  # test PyTorch only\n        hard_fail=False,  # throw error on benchmark failure",
        "detail": "recommendation_assets.yolov5.benchmarks",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.benchmarks",
        "description": "recommendation_assets.yolov5.benchmarks",
        "peekOfCode": "def test(\n        weights=ROOT / 'yolov5s.pt',  # weights path\n        imgsz=640,  # inference size (pixels)\n        batch_size=1,  # batch size\n        data=ROOT / 'data/coco128.yaml',  # dataset.yaml path\n        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n        half=False,  # use FP16 half-precision inference\n        test=False,  # test exports only\n        pt_only=False,  # test PyTorch only\n        hard_fail=False,  # throw error on benchmark failure",
        "detail": "recommendation_assets.yolov5.benchmarks",
        "documentation": {}
    },
    {
        "label": "parse_opt",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.benchmarks",
        "description": "recommendation_assets.yolov5.benchmarks",
        "peekOfCode": "def parse_opt():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--weights', type=str, default=ROOT / 'yolov5s.pt', help='weights path')\n    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='inference size (pixels)')\n    parser.add_argument('--batch-size', type=int, default=1, help='batch size')\n    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')\n    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n    parser.add_argument('--test', action='store_true', help='test exports only')\n    parser.add_argument('--pt-only', action='store_true', help='test PyTorch only')",
        "detail": "recommendation_assets.yolov5.benchmarks",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.benchmarks",
        "description": "recommendation_assets.yolov5.benchmarks",
        "peekOfCode": "def main(opt):\n    test(**vars(opt)) if opt.test else run(**vars(opt))\nif __name__ == '__main__':\n    opt = parse_opt()\n    main(opt)",
        "detail": "recommendation_assets.yolov5.benchmarks",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.benchmarks",
        "description": "recommendation_assets.yolov5.benchmarks",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\n# ROOT = ROOT.relative_to(Path.cwd())  # relative\nimport export\nfrom models.experimental import attempt_load\nfrom models.yolo import SegmentationModel\nfrom segment.val import run as val_seg\nfrom utils import notebook_init",
        "detail": "recommendation_assets.yolov5.benchmarks",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.benchmarks",
        "description": "recommendation_assets.yolov5.benchmarks",
        "peekOfCode": "ROOT = FILE.parents[0]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\n# ROOT = ROOT.relative_to(Path.cwd())  # relative\nimport export\nfrom models.experimental import attempt_load\nfrom models.yolo import SegmentationModel\nfrom segment.val import run as val_seg\nfrom utils import notebook_init\nfrom utils.general import LOGGER, check_yaml, file_size, print_args",
        "detail": "recommendation_assets.yolov5.benchmarks",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.detect",
        "description": "recommendation_assets.yolov5.detect",
        "peekOfCode": "def run(\n        weights=ROOT / r'C:\\Users\\Dell\\Downloads\\yolov5\\runs\\train\\weights\\exp3\\last.pt',  # model path or triton URL\n        source=ROOT / 'data/images',  # file/dir/URL/glob/screen/0(webcam)\n        data=ROOT / 'data/fashion.yaml',  # dataset.yaml path\n        imgsz=(640, 640),  # inference size (height, width)\n        conf_thres=0.25,  # confidence threshold\n        iou_thres=0.45,  # NMS IOU threshold\n        max_det=1000,  # maximum detections per image\n        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n        view_img=False,  # show results",
        "detail": "recommendation_assets.yolov5.detect",
        "documentation": {}
    },
    {
        "label": "generate_bbox",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.detect",
        "description": "recommendation_assets.yolov5.detect",
        "peekOfCode": "def generate_bbox(img_path, conf_thres=0.5):\n    # Load model\n    weights = r'C:\\Users\\Dell\\Downloads\\yolov5\\runs\\train\\weights\\exp3\\last.pt'\n    model = torch.hub.load('ultralytics/yolov5', 'custom', path=weights)\n    # Convert to tensor and add batch dimension\n    results = model(img_path)\n    return results.xyxy[0].cpu().numpy()  # Return the numpy array of predictions\ndef parse_opt():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path or triton URL')",
        "detail": "recommendation_assets.yolov5.detect",
        "documentation": {}
    },
    {
        "label": "parse_opt",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.detect",
        "description": "recommendation_assets.yolov5.detect",
        "peekOfCode": "def parse_opt():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path or triton URL')\n    parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob/screen/0(webcam)')\n    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) dataset.yaml path')\n    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')\n    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')\n    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')\n    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')\n    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')",
        "detail": "recommendation_assets.yolov5.detect",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.detect",
        "description": "recommendation_assets.yolov5.detect",
        "peekOfCode": "def main(opt):\n    check_requirements(ROOT / 'requirements.txt', exclude=('tensorboard', 'thop'))\n    img_path =r'C:\\Users\\Dell\\Downloads\\images\\465.jpg'  # Update with your image path\n    results = generate_bbox(img_path, conf_thres=0.5)\n    # Process the results and extract clothes\n    # Call the plot_clothes function to visualize the results\n    run(**vars(opt))\nif __name__ == '__main__':\n    opt = parse_opt()\n    main(opt)",
        "detail": "recommendation_assets.yolov5.detect",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.detect",
        "description": "recommendation_assets.yolov5.detect",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom ultralytics.utils.plotting import Annotator, colors, save_one_box\nfrom models.common import DetectMultiBackend\nfrom utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\nfrom utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)",
        "detail": "recommendation_assets.yolov5.detect",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.detect",
        "description": "recommendation_assets.yolov5.detect",
        "peekOfCode": "ROOT = FILE.parents[0]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom ultralytics.utils.plotting import Annotator, colors, save_one_box\nfrom models.common import DetectMultiBackend\nfrom utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\nfrom utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)\nfrom utils.torch_utils import select_device, smart_inference_mode",
        "detail": "recommendation_assets.yolov5.detect",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.detect",
        "description": "recommendation_assets.yolov5.detect",
        "peekOfCode": "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom ultralytics.utils.plotting import Annotator, colors, save_one_box\nfrom models.common import DetectMultiBackend\nfrom utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\nfrom utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)\nfrom utils.torch_utils import select_device, smart_inference_mode\n# Paths to your custom YOLOv5 model weights, input image, and dataset YAML\nweights_path =r'C:\\Users\\Dell\\Downloads\\yolov5\\runs\\train\\weights\\exp3\\last.pt'\nsource_path = r'C:\\Users\\Dell\\Downloads\\images\\465.jpg'",
        "detail": "recommendation_assets.yolov5.detect",
        "documentation": {}
    },
    {
        "label": "source_path",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.detect",
        "description": "recommendation_assets.yolov5.detect",
        "peekOfCode": "source_path = r'C:\\Users\\Dell\\Downloads\\images\\465.jpg'\ndata_yaml_path =r'C:\\Users\\Dell\\Downloads\\yolov5\\data\\fashion.yaml'\n# Other settings\nimage_size = (640, 640)\n@smart_inference_mode()\ndef run(\n        weights=ROOT / r'C:\\Users\\Dell\\Downloads\\yolov5\\runs\\train\\weights\\exp3\\last.pt',  # model path or triton URL\n        source=ROOT / 'data/images',  # file/dir/URL/glob/screen/0(webcam)\n        data=ROOT / 'data/fashion.yaml',  # dataset.yaml path\n        imgsz=(640, 640),  # inference size (height, width)",
        "detail": "recommendation_assets.yolov5.detect",
        "documentation": {}
    },
    {
        "label": "image_size",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.detect",
        "description": "recommendation_assets.yolov5.detect",
        "peekOfCode": "image_size = (640, 640)\n@smart_inference_mode()\ndef run(\n        weights=ROOT / r'C:\\Users\\Dell\\Downloads\\yolov5\\runs\\train\\weights\\exp3\\last.pt',  # model path or triton URL\n        source=ROOT / 'data/images',  # file/dir/URL/glob/screen/0(webcam)\n        data=ROOT / 'data/fashion.yaml',  # dataset.yaml path\n        imgsz=(640, 640),  # inference size (height, width)\n        conf_thres=0.25,  # confidence threshold\n        iou_thres=0.45,  # NMS IOU threshold\n        max_det=1000,  # maximum detections per image",
        "detail": "recommendation_assets.yolov5.detect",
        "documentation": {}
    },
    {
        "label": "iOSModel",
        "kind": 6,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "class iOSModel(torch.nn.Module):\n    def __init__(self, model, im):\n        super().__init__()\n        b, c, h, w = im.shape  # batch, channel, height, width\n        self.model = model\n        self.nc = model.nc  # number of classes\n        if w == h:\n            self.normalize = 1. / w\n        else:\n            self.normalize = torch.tensor([1. / w, 1. / h, 1. / w, 1. / h])  # broadcast (slower, smaller)",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "export_formats",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "def export_formats():\n    # YOLOv5 export formats\n    x = [\n        ['PyTorch', '-', '.pt', True, True],\n        ['TorchScript', 'torchscript', '.torchscript', True, True],\n        ['ONNX', 'onnx', '.onnx', True, True],\n        ['OpenVINO', 'openvino', '_openvino_model', True, False],\n        ['TensorRT', 'engine', '.engine', False, True],\n        ['CoreML', 'coreml', '.mlmodel', True, False],\n        ['TensorFlow SavedModel', 'saved_model', '_saved_model', True, True],",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "try_export",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "def try_export(inner_func):\n    # YOLOv5 export decorator, i..e @try_export\n    inner_args = get_default_args(inner_func)\n    def outer_func(*args, **kwargs):\n        prefix = inner_args['prefix']\n        try:\n            with Profile() as dt:\n                f, model = inner_func(*args, **kwargs)\n            LOGGER.info(f'{prefix} export success  {dt.t:.1f}s, saved as {f} ({file_size(f):.1f} MB)')\n            return f, model",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "export_torchscript",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "def export_torchscript(model, im, file, optimize, prefix=colorstr('TorchScript:')):\n    # YOLOv5 TorchScript model export\n    LOGGER.info(f'\\n{prefix} starting export with torch {torch.__version__}...')\n    f = file.with_suffix('.torchscript')\n    ts = torch.jit.trace(model, im, strict=False)\n    d = {'shape': im.shape, 'stride': int(max(model.stride)), 'names': model.names}\n    extra_files = {'config.txt': json.dumps(d)}  # torch._C.ExtraFilesMap()\n    if optimize:  # https://pytorch.org/tutorials/recipes/mobile_interpreter.html\n        optimize_for_mobile(ts)._save_for_lite_interpreter(str(f), _extra_files=extra_files)\n    else:",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "export_onnx",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "def export_onnx(model, im, file, opset, dynamic, simplify, prefix=colorstr('ONNX:')):\n    # YOLOv5 ONNX export\n    check_requirements('onnx>=1.12.0')\n    import onnx\n    LOGGER.info(f'\\n{prefix} starting export with onnx {onnx.__version__}...')\n    f = file.with_suffix('.onnx')\n    output_names = ['output0', 'output1'] if isinstance(model, SegmentationModel) else ['output0']\n    if dynamic:\n        dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)\n        if isinstance(model, SegmentationModel):",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "export_openvino",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "def export_openvino(file, metadata, half, int8, data, prefix=colorstr('OpenVINO:')):\n    # YOLOv5 OpenVINO export\n    check_requirements('openvino-dev>=2023.0')  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n    import openvino.runtime as ov  # noqa\n    from openvino.tools import mo  # noqa\n    LOGGER.info(f'\\n{prefix} starting export with openvino {ov.__version__}...')\n    f = str(file).replace(file.suffix, f'_openvino_model{os.sep}')\n    f_onnx = file.with_suffix('.onnx')\n    f_ov = str(Path(f) / file.with_suffix('.xml').name)\n    if int8:",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "export_paddle",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "def export_paddle(model, im, file, metadata, prefix=colorstr('PaddlePaddle:')):\n    # YOLOv5 Paddle export\n    check_requirements(('paddlepaddle', 'x2paddle'))\n    import x2paddle\n    from x2paddle.convert import pytorch2paddle\n    LOGGER.info(f'\\n{prefix} starting export with X2Paddle {x2paddle.__version__}...')\n    f = str(file).replace('.pt', f'_paddle_model{os.sep}')\n    pytorch2paddle(module=model, save_dir=f, jit_type='trace', input_examples=[im])  # export\n    yaml_save(Path(f) / file.with_suffix('.yaml').name, metadata)  # add metadata.yaml\n    return f, None",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "export_coreml",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr('CoreML:')):\n    # YOLOv5 CoreML export\n    check_requirements('coremltools')\n    import coremltools as ct\n    LOGGER.info(f'\\n{prefix} starting export with coremltools {ct.__version__}...')\n    f = file.with_suffix('.mlmodel')\n    if nms:\n        model = iOSModel(model, im)\n    ts = torch.jit.trace(model, im, strict=False)  # TorchScript model\n    ct_model = ct.convert(ts, inputs=[ct.ImageType('image', shape=im.shape, scale=1 / 255, bias=[0, 0, 0])])",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "export_engine",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "def export_engine(model, im, file, half, dynamic, simplify, workspace=4, verbose=False, prefix=colorstr('TensorRT:')):\n    # YOLOv5 TensorRT export https://developer.nvidia.com/tensorrt\n    assert im.device.type != 'cpu', 'export running on CPU but must be on GPU, i.e. `python export.py --device 0`'\n    try:\n        import tensorrt as trt\n    except Exception:\n        if platform.system() == 'Linux':\n            check_requirements('nvidia-tensorrt', cmds='-U --index-url https://pypi.ngc.nvidia.com')\n        import tensorrt as trt\n    if trt.__version__[0] == '7':  # TensorRT 7 handling https://github.com/ultralytics/yolov5/issues/6012",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "export_saved_model",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "def export_saved_model(model,\n                       im,\n                       file,\n                       dynamic,\n                       tf_nms=False,\n                       agnostic_nms=False,\n                       topk_per_class=100,\n                       topk_all=100,\n                       iou_thres=0.45,\n                       conf_thres=0.25,",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "export_pb",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "def export_pb(keras_model, file, prefix=colorstr('TensorFlow GraphDef:')):\n    # YOLOv5 TensorFlow GraphDef *.pb export https://github.com/leimao/Frozen_Graph_TensorFlow\n    import tensorflow as tf\n    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n    LOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\n    f = file.with_suffix('.pb')\n    m = tf.function(lambda x: keras_model(x))  # full model\n    m = m.get_concrete_function(tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype))\n    frozen_func = convert_variables_to_constants_v2(m)\n    frozen_func.graph.as_graph_def()",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "export_tflite",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "def export_tflite(keras_model, im, file, int8, data, nms, agnostic_nms, prefix=colorstr('TensorFlow Lite:')):\n    # YOLOv5 TensorFlow Lite export\n    import tensorflow as tf\n    LOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\n    batch_size, ch, *imgsz = list(im.shape)  # BCHW\n    f = str(file).replace('.pt', '-fp16.tflite')\n    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n    converter.target_spec.supported_types = [tf.float16]\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "export_edgetpu",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "def export_edgetpu(file, prefix=colorstr('Edge TPU:')):\n    # YOLOv5 Edge TPU export https://coral.ai/docs/edgetpu/models-intro/\n    cmd = 'edgetpu_compiler --version'\n    help_url = 'https://coral.ai/docs/edgetpu/compiler/'\n    assert platform.system() == 'Linux', f'export only supported on Linux. See {help_url}'\n    if subprocess.run(f'{cmd} > /dev/null 2>&1', shell=True).returncode != 0:\n        LOGGER.info(f'\\n{prefix} export requires Edge TPU compiler. Attempting install from {help_url}')\n        sudo = subprocess.run('sudo --version >/dev/null', shell=True).returncode == 0  # sudo installed on system\n        for c in (\n                'curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -',",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "export_tfjs",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "def export_tfjs(file, int8, prefix=colorstr('TensorFlow.js:')):\n    # YOLOv5 TensorFlow.js export\n    check_requirements('tensorflowjs')\n    import tensorflowjs as tfjs\n    LOGGER.info(f'\\n{prefix} starting export with tensorflowjs {tfjs.__version__}...')\n    f = str(file).replace('.pt', '_web_model')  # js dir\n    f_pb = file.with_suffix('.pb')  # *.pb path\n    f_json = f'{f}/model.json'  # *.json path\n    args = [\n        'tensorflowjs_converter',",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "add_tflite_metadata",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "def add_tflite_metadata(file, metadata, num_outputs):\n    # Add metadata to *.tflite models per https://www.tensorflow.org/lite/models/convert/metadata\n    with contextlib.suppress(ImportError):\n        # check_requirements('tflite_support')\n        from tflite_support import flatbuffers\n        from tflite_support import metadata as _metadata\n        from tflite_support import metadata_schema_py_generated as _metadata_fb\n        tmp_file = Path('/tmp/meta.txt')\n        with open(tmp_file, 'w') as meta_f:\n            meta_f.write(str(metadata))",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "pipeline_coreml",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "def pipeline_coreml(model, im, file, names, y, prefix=colorstr('CoreML Pipeline:')):\n    # YOLOv5 CoreML pipeline\n    import coremltools as ct\n    from PIL import Image\n    print(f'{prefix} starting pipeline with coremltools {ct.__version__}...')\n    batch_size, ch, h, w = list(im.shape)  # BCHW\n    t = time.time()\n    # YOLOv5 Output shapes\n    spec = model.get_spec()\n    out0, out1 = iter(spec.description.output)",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "def run(\n        data=ROOT / 'data/coco128.yaml',  # 'dataset.yaml path'\n        weights=ROOT / 'yolov5s.pt',  # weights path\n        imgsz=(640, 640),  # image (height, width)\n        batch_size=1,  # batch size\n        device='cpu',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n        include=('torchscript', 'onnx'),  # include formats\n        half=False,  # FP16 half-precision export\n        inplace=False,  # set YOLOv5 Detect() inplace=True\n        keras=False,  # use Keras",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "parse_opt",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "def parse_opt(known=False):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')\n    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model.pt path(s)')\n    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640, 640], help='image (h, w)')\n    parser.add_argument('--batch-size', type=int, default=1, help='batch size')\n    parser.add_argument('--device', default='cpu', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n    parser.add_argument('--half', action='store_true', help='FP16 half-precision export')\n    parser.add_argument('--inplace', action='store_true', help='set YOLOv5 Detect() inplace=True')\n    parser.add_argument('--keras', action='store_true', help='TF: use Keras')",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "def main(opt):\n    for opt.weights in (opt.weights if isinstance(opt.weights, list) else [opt.weights]):\n        run(**vars(opt))\nif __name__ == '__main__':\n    opt = parse_opt()\n    main(opt)",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif platform.system() != 'Windows':\n    ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom models.experimental import attempt_load\nfrom models.yolo import ClassificationModel, Detect, DetectionModel, SegmentationModel\nfrom utils.dataloaders import LoadImages\nfrom utils.general import (LOGGER, Profile, check_dataset, check_img_size, check_requirements, check_version,",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "ROOT = FILE.parents[0]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif platform.system() != 'Windows':\n    ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom models.experimental import attempt_load\nfrom models.yolo import ClassificationModel, Detect, DetectionModel, SegmentationModel\nfrom utils.dataloaders import LoadImages\nfrom utils.general import (LOGGER, Profile, check_dataset, check_img_size, check_requirements, check_version,\n                           check_yaml, colorstr, file_size, get_default_args, print_args, url2file, yaml_save)",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "MACOS",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.export",
        "description": "recommendation_assets.yolov5.export",
        "peekOfCode": "MACOS = platform.system() == 'Darwin'  # macOS environment\nclass iOSModel(torch.nn.Module):\n    def __init__(self, model, im):\n        super().__init__()\n        b, c, h, w = im.shape  # batch, channel, height, width\n        self.model = model\n        self.nc = model.nc  # number of classes\n        if w == h:\n            self.normalize = 1. / w\n        else:",
        "detail": "recommendation_assets.yolov5.export",
        "documentation": {}
    },
    {
        "label": "custom",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.hubconf",
        "description": "recommendation_assets.yolov5.hubconf",
        "peekOfCode": "def custom(path='path/to/model.pt', autoshape=True, _verbose=True, device=None):\n    # YOLOv5 custom or local model\n    return _create(path, autoshape=autoshape, verbose=_verbose, device=device)\ndef yolov5n(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-nano model https://github.com/ultralytics/yolov5\n    return _create('yolov5n', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5s(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-small model https://github.com/ultralytics/yolov5\n    return _create('yolov5s', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5m(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):",
        "detail": "recommendation_assets.yolov5.hubconf",
        "documentation": {}
    },
    {
        "label": "yolov5n",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.hubconf",
        "description": "recommendation_assets.yolov5.hubconf",
        "peekOfCode": "def yolov5n(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-nano model https://github.com/ultralytics/yolov5\n    return _create('yolov5n', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5s(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-small model https://github.com/ultralytics/yolov5\n    return _create('yolov5s', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5m(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-medium model https://github.com/ultralytics/yolov5\n    return _create('yolov5m', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5l(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):",
        "detail": "recommendation_assets.yolov5.hubconf",
        "documentation": {}
    },
    {
        "label": "yolov5s",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.hubconf",
        "description": "recommendation_assets.yolov5.hubconf",
        "peekOfCode": "def yolov5s(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-small model https://github.com/ultralytics/yolov5\n    return _create('yolov5s', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5m(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-medium model https://github.com/ultralytics/yolov5\n    return _create('yolov5m', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5l(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-large model https://github.com/ultralytics/yolov5\n    return _create('yolov5l', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5x(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):",
        "detail": "recommendation_assets.yolov5.hubconf",
        "documentation": {}
    },
    {
        "label": "yolov5m",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.hubconf",
        "description": "recommendation_assets.yolov5.hubconf",
        "peekOfCode": "def yolov5m(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-medium model https://github.com/ultralytics/yolov5\n    return _create('yolov5m', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5l(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-large model https://github.com/ultralytics/yolov5\n    return _create('yolov5l', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5x(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-xlarge model https://github.com/ultralytics/yolov5\n    return _create('yolov5x', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5n6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):",
        "detail": "recommendation_assets.yolov5.hubconf",
        "documentation": {}
    },
    {
        "label": "yolov5l",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.hubconf",
        "description": "recommendation_assets.yolov5.hubconf",
        "peekOfCode": "def yolov5l(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-large model https://github.com/ultralytics/yolov5\n    return _create('yolov5l', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5x(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-xlarge model https://github.com/ultralytics/yolov5\n    return _create('yolov5x', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5n6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-nano-P6 model https://github.com/ultralytics/yolov5\n    return _create('yolov5n6', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5s6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):",
        "detail": "recommendation_assets.yolov5.hubconf",
        "documentation": {}
    },
    {
        "label": "yolov5x",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.hubconf",
        "description": "recommendation_assets.yolov5.hubconf",
        "peekOfCode": "def yolov5x(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-xlarge model https://github.com/ultralytics/yolov5\n    return _create('yolov5x', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5n6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-nano-P6 model https://github.com/ultralytics/yolov5\n    return _create('yolov5n6', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5s6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-small-P6 model https://github.com/ultralytics/yolov5\n    return _create('yolov5s6', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5m6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):",
        "detail": "recommendation_assets.yolov5.hubconf",
        "documentation": {}
    },
    {
        "label": "yolov5n6",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.hubconf",
        "description": "recommendation_assets.yolov5.hubconf",
        "peekOfCode": "def yolov5n6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-nano-P6 model https://github.com/ultralytics/yolov5\n    return _create('yolov5n6', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5s6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-small-P6 model https://github.com/ultralytics/yolov5\n    return _create('yolov5s6', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5m6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-medium-P6 model https://github.com/ultralytics/yolov5\n    return _create('yolov5m6', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5l6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):",
        "detail": "recommendation_assets.yolov5.hubconf",
        "documentation": {}
    },
    {
        "label": "yolov5s6",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.hubconf",
        "description": "recommendation_assets.yolov5.hubconf",
        "peekOfCode": "def yolov5s6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-small-P6 model https://github.com/ultralytics/yolov5\n    return _create('yolov5s6', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5m6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-medium-P6 model https://github.com/ultralytics/yolov5\n    return _create('yolov5m6', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5l6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-large-P6 model https://github.com/ultralytics/yolov5\n    return _create('yolov5l6', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5x6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):",
        "detail": "recommendation_assets.yolov5.hubconf",
        "documentation": {}
    },
    {
        "label": "yolov5m6",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.hubconf",
        "description": "recommendation_assets.yolov5.hubconf",
        "peekOfCode": "def yolov5m6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-medium-P6 model https://github.com/ultralytics/yolov5\n    return _create('yolov5m6', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5l6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-large-P6 model https://github.com/ultralytics/yolov5\n    return _create('yolov5l6', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5x6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-xlarge-P6 model https://github.com/ultralytics/yolov5\n    return _create('yolov5x6', pretrained, channels, classes, autoshape, _verbose, device)\nif __name__ == '__main__':",
        "detail": "recommendation_assets.yolov5.hubconf",
        "documentation": {}
    },
    {
        "label": "yolov5l6",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.hubconf",
        "description": "recommendation_assets.yolov5.hubconf",
        "peekOfCode": "def yolov5l6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-large-P6 model https://github.com/ultralytics/yolov5\n    return _create('yolov5l6', pretrained, channels, classes, autoshape, _verbose, device)\ndef yolov5x6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-xlarge-P6 model https://github.com/ultralytics/yolov5\n    return _create('yolov5x6', pretrained, channels, classes, autoshape, _verbose, device)\nif __name__ == '__main__':\n    import argparse\n    from pathlib import Path\n    import numpy as np",
        "detail": "recommendation_assets.yolov5.hubconf",
        "documentation": {}
    },
    {
        "label": "yolov5x6",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.hubconf",
        "description": "recommendation_assets.yolov5.hubconf",
        "peekOfCode": "def yolov5x6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    # YOLOv5-xlarge-P6 model https://github.com/ultralytics/yolov5\n    return _create('yolov5x6', pretrained, channels, classes, autoshape, _verbose, device)\nif __name__ == '__main__':\n    import argparse\n    from pathlib import Path\n    import numpy as np\n    from PIL import Image\n    from utils.general import cv2, print_args\n    # Argparser",
        "detail": "recommendation_assets.yolov5.hubconf",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.train",
        "description": "recommendation_assets.yolov5.train",
        "peekOfCode": "def train(hyp, opt, device, callbacks):  # hyp is path/to/hyp.yaml or hyp dictionary\n    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze = \\\n        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \\\n        opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze\n    callbacks.run('on_pretrain_routine_start')\n    # Directories\n    w = save_dir / 'weights'  # weights dir\n    (w.parent if evolve else w).mkdir(parents=True, exist_ok=True)  # make dir\n    last, best = w / 'last.pt', w / 'best.pt'\n    # Hyperparameters",
        "detail": "recommendation_assets.yolov5.train",
        "documentation": {}
    },
    {
        "label": "parse_opt",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.train",
        "description": "recommendation_assets.yolov5.train",
        "peekOfCode": "def parse_opt(known=False):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--weights', type=str, default=ROOT / 'yolov5s.pt', help='initial weights path')\n    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')\n    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')\n    parser.add_argument('--hyp', type=str, default=ROOT / 'data/hyps/hyp.scratch-low.yaml', help='hyperparameters path')\n    parser.add_argument('--epochs', type=int, default=100, help='total training epochs')\n    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs, -1 for autobatch')\n    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')\n    parser.add_argument('--rect', action='store_true', help='rectangular training')",
        "detail": "recommendation_assets.yolov5.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.train",
        "description": "recommendation_assets.yolov5.train",
        "peekOfCode": "def main(opt, callbacks=Callbacks()):\n    # Checks\n    if RANK in {-1, 0}:\n        print_args(vars(opt))\n        check_git_status()\n        check_requirements(ROOT / 'requirements.txt')\n    # Resume (from specified or most recent last.pt)\n    if opt.resume and not check_comet_resume(opt) and not opt.evolve:\n        last = Path(check_file(opt.resume) if isinstance(opt.resume, str) else get_latest_run())\n        opt_yaml = last.parent.parent / 'opt.yaml'  # train options yaml",
        "detail": "recommendation_assets.yolov5.train",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.train",
        "description": "recommendation_assets.yolov5.train",
        "peekOfCode": "def run(**kwargs):\n    # Usage: import train; train.run(data='coco128.yaml', imgsz=320, weights='yolov5m.pt')\n    opt = parse_opt(True)\n    for k, v in kwargs.items():\n        setattr(opt, k, v)\n    main(opt)\n    return opt\nif __name__ == '__main__':\n    opt = parse_opt()\n    main(opt)",
        "detail": "recommendation_assets.yolov5.train",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.train",
        "description": "recommendation_assets.yolov5.train",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nimport val as validate  # for end-of-epoch mAP\nfrom models.experimental import attempt_load\nfrom models.yolo import Model\nfrom utils.autoanchor import check_anchors\nfrom utils.autobatch import check_train_batch_size",
        "detail": "recommendation_assets.yolov5.train",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.train",
        "description": "recommendation_assets.yolov5.train",
        "peekOfCode": "ROOT = FILE.parents[0]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nimport val as validate  # for end-of-epoch mAP\nfrom models.experimental import attempt_load\nfrom models.yolo import Model\nfrom utils.autoanchor import check_anchors\nfrom utils.autobatch import check_train_batch_size\nfrom utils.callbacks import Callbacks",
        "detail": "recommendation_assets.yolov5.train",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.train",
        "description": "recommendation_assets.yolov5.train",
        "peekOfCode": "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nimport val as validate  # for end-of-epoch mAP\nfrom models.experimental import attempt_load\nfrom models.yolo import Model\nfrom utils.autoanchor import check_anchors\nfrom utils.autobatch import check_train_batch_size\nfrom utils.callbacks import Callbacks\nfrom utils.dataloaders import create_dataloader\nfrom utils.downloads import attempt_download, is_url\nfrom utils.general import (LOGGER, TQDM_BAR_FORMAT, check_amp, check_dataset, check_file, check_git_info,",
        "detail": "recommendation_assets.yolov5.train",
        "documentation": {}
    },
    {
        "label": "LOCAL_RANK",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.train",
        "description": "recommendation_assets.yolov5.train",
        "peekOfCode": "LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html\nRANK = int(os.getenv('RANK', -1))\nWORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))\nGIT_INFO = check_git_info()\ndef train(hyp, opt, device, callbacks):  # hyp is path/to/hyp.yaml or hyp dictionary\n    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze = \\\n        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \\\n        opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze\n    callbacks.run('on_pretrain_routine_start')\n    # Directories",
        "detail": "recommendation_assets.yolov5.train",
        "documentation": {}
    },
    {
        "label": "RANK",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.train",
        "description": "recommendation_assets.yolov5.train",
        "peekOfCode": "RANK = int(os.getenv('RANK', -1))\nWORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))\nGIT_INFO = check_git_info()\ndef train(hyp, opt, device, callbacks):  # hyp is path/to/hyp.yaml or hyp dictionary\n    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze = \\\n        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \\\n        opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze\n    callbacks.run('on_pretrain_routine_start')\n    # Directories\n    w = save_dir / 'weights'  # weights dir",
        "detail": "recommendation_assets.yolov5.train",
        "documentation": {}
    },
    {
        "label": "WORLD_SIZE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.train",
        "description": "recommendation_assets.yolov5.train",
        "peekOfCode": "WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))\nGIT_INFO = check_git_info()\ndef train(hyp, opt, device, callbacks):  # hyp is path/to/hyp.yaml or hyp dictionary\n    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze = \\\n        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \\\n        opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze\n    callbacks.run('on_pretrain_routine_start')\n    # Directories\n    w = save_dir / 'weights'  # weights dir\n    (w.parent if evolve else w).mkdir(parents=True, exist_ok=True)  # make dir",
        "detail": "recommendation_assets.yolov5.train",
        "documentation": {}
    },
    {
        "label": "GIT_INFO",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.train",
        "description": "recommendation_assets.yolov5.train",
        "peekOfCode": "GIT_INFO = check_git_info()\ndef train(hyp, opt, device, callbacks):  # hyp is path/to/hyp.yaml or hyp dictionary\n    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze = \\\n        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \\\n        opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze\n    callbacks.run('on_pretrain_routine_start')\n    # Directories\n    w = save_dir / 'weights'  # weights dir\n    (w.parent if evolve else w).mkdir(parents=True, exist_ok=True)  # make dir\n    last, best = w / 'last.pt', w / 'best.pt'",
        "detail": "recommendation_assets.yolov5.train",
        "documentation": {}
    },
    {
        "label": "save_one_txt",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.val",
        "description": "recommendation_assets.yolov5.val",
        "peekOfCode": "def save_one_txt(predn, save_conf, shape, file):\n    # Save one txt result\n    gn = torch.tensor(shape)[[1, 0, 1, 0]]  # normalization gain whwh\n    for *xyxy, conf, cls in predn.tolist():\n        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n        with open(file, 'a') as f:\n            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\ndef save_one_json(predn, jdict, path, class_map):\n    # Save one JSON result {\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}",
        "detail": "recommendation_assets.yolov5.val",
        "documentation": {}
    },
    {
        "label": "save_one_json",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.val",
        "description": "recommendation_assets.yolov5.val",
        "peekOfCode": "def save_one_json(predn, jdict, path, class_map):\n    # Save one JSON result {\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}\n    image_id = int(path.stem) if path.stem.isnumeric() else path.stem\n    box = xyxy2xywh(predn[:, :4])  # xywh\n    box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\n    for p, b in zip(predn.tolist(), box.tolist()):\n        jdict.append({\n            'image_id': image_id,\n            'category_id': class_map[int(p[5])],\n            'bbox': [round(x, 3) for x in b],",
        "detail": "recommendation_assets.yolov5.val",
        "documentation": {}
    },
    {
        "label": "process_batch",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.val",
        "description": "recommendation_assets.yolov5.val",
        "peekOfCode": "def process_batch(detections, labels, iouv):\n    \"\"\"\n    Return correct prediction matrix\n    Arguments:\n        detections (array[N, 6]), x1, y1, x2, y2, conf, class\n        labels (array[M, 5]), class, x1, y1, x2, y2\n    Returns:\n        correct (array[N, 10]), for 10 IoU levels\n    \"\"\"\n    correct = np.zeros((detections.shape[0], iouv.shape[0])).astype(bool)",
        "detail": "recommendation_assets.yolov5.val",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.val",
        "description": "recommendation_assets.yolov5.val",
        "peekOfCode": "def run(\n        data,\n        weights=None,  # model.pt path(s)\n        batch_size=32,  # batch size\n        imgsz=640,  # inference size (pixels)\n        conf_thres=0.001,  # confidence threshold\n        iou_thres=0.6,  # NMS IoU threshold\n        max_det=300,  # maximum detections per image\n        task='val',  # train, val, test, speed or study\n        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu",
        "detail": "recommendation_assets.yolov5.val",
        "documentation": {}
    },
    {
        "label": "parse_opt",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.val",
        "description": "recommendation_assets.yolov5.val",
        "peekOfCode": "def parse_opt():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')\n    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)')\n    parser.add_argument('--batch-size', type=int, default=32, help='batch size')\n    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='inference size (pixels)')\n    parser.add_argument('--conf-thres', type=float, default=0.001, help='confidence threshold')\n    parser.add_argument('--iou-thres', type=float, default=0.6, help='NMS IoU threshold')\n    parser.add_argument('--max-det', type=int, default=300, help='maximum detections per image')\n    parser.add_argument('--task', default='val', help='train, val, test, speed or study')",
        "detail": "recommendation_assets.yolov5.val",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "recommendation_assets.yolov5.val",
        "description": "recommendation_assets.yolov5.val",
        "peekOfCode": "def main(opt):\n    check_requirements(ROOT / 'requirements.txt', exclude=('tensorboard', 'thop'))\n    if opt.task in ('train', 'val', 'test'):  # run normally\n        if opt.conf_thres > 0.001:  # https://github.com/ultralytics/yolov5/issues/1466\n            LOGGER.info(f'WARNING  confidence threshold {opt.conf_thres} > 0.001 produces invalid results')\n        if opt.save_hybrid:\n            LOGGER.info('WARNING  --save-hybrid will return high mAP from hybrid labels, not from predictions alone')\n        run(**vars(opt))\n    else:\n        weights = opt.weights if isinstance(opt.weights, list) else [opt.weights]",
        "detail": "recommendation_assets.yolov5.val",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.val",
        "description": "recommendation_assets.yolov5.val",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom models.common import DetectMultiBackend\nfrom utils.callbacks import Callbacks\nfrom utils.dataloaders import create_dataloader\nfrom utils.general import (LOGGER, TQDM_BAR_FORMAT, Profile, check_dataset, check_img_size, check_requirements,\n                           check_yaml, coco80_to_coco91_class, colorstr, increment_path, non_max_suppression,",
        "detail": "recommendation_assets.yolov5.val",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.val",
        "description": "recommendation_assets.yolov5.val",
        "peekOfCode": "ROOT = FILE.parents[0]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom models.common import DetectMultiBackend\nfrom utils.callbacks import Callbacks\nfrom utils.dataloaders import create_dataloader\nfrom utils.general import (LOGGER, TQDM_BAR_FORMAT, Profile, check_dataset, check_img_size, check_requirements,\n                           check_yaml, coco80_to_coco91_class, colorstr, increment_path, non_max_suppression,\n                           print_args, scale_boxes, xywh2xyxy, xyxy2xywh)",
        "detail": "recommendation_assets.yolov5.val",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "recommendation_assets.yolov5.val",
        "description": "recommendation_assets.yolov5.val",
        "peekOfCode": "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom models.common import DetectMultiBackend\nfrom utils.callbacks import Callbacks\nfrom utils.dataloaders import create_dataloader\nfrom utils.general import (LOGGER, TQDM_BAR_FORMAT, Profile, check_dataset, check_img_size, check_requirements,\n                           check_yaml, coco80_to_coco91_class, colorstr, increment_path, non_max_suppression,\n                           print_args, scale_boxes, xywh2xyxy, xyxy2xywh)\nfrom utils.metrics import ConfusionMatrix, ap_per_class, box_iou\nfrom utils.plots import output_to_target, plot_images, plot_val_study\nfrom utils.torch_utils import select_device, smart_inference_mode",
        "detail": "recommendation_assets.yolov5.val",
        "documentation": {}
    },
    {
        "label": "generate_bbox",
        "kind": 2,
        "importPath": "finalpipeline",
        "description": "finalpipeline",
        "peekOfCode": "def generate_bbox(img_path, conf_thres=0.5):\n    # Load model\n    weights = r'./recommendation_assets/last.pt'\n    model = torch.hub.load('ultralytics/yolov5', 'custom', path=weights)\n    # Convert to tensor and add batch dimension\n    results = model(img_path)\n    return results.xyxy[0].cpu().numpy()\n    # Return the numpy array of predictions\ndef preprocess_image1(filename):\n    \"\"\"",
        "detail": "finalpipeline",
        "documentation": {}
    },
    {
        "label": "preprocess_image1",
        "kind": 2,
        "importPath": "finalpipeline",
        "description": "finalpipeline",
        "peekOfCode": "def preprocess_image1(filename):\n    \"\"\"\n    Load the specified file as a JPEG image, preprocess it and\n    resize it to the target shape for Gender Classification\n    \"\"\"\n    image_string = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    image = tf.image.resize(image, TARGET_SHAPE[:2])\n    return tf.expand_dims(image, axis=0)\ndef preprocess_image2(filename):",
        "detail": "finalpipeline",
        "documentation": {}
    },
    {
        "label": "preprocess_image2",
        "kind": 2,
        "importPath": "finalpipeline",
        "description": "finalpipeline",
        "peekOfCode": "def preprocess_image2(filename):\n    \"\"\"\n    Load the specified file as a JPEG image, preprocess it and\n    resize it to the target shape for Embedding Generation\n    \"\"\"\n    image_string = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, TARGET_SHAPE[:2])\n    image = resnet.preprocess_input(image)",
        "detail": "finalpipeline",
        "documentation": {}
    },
    {
        "label": "load_models",
        "kind": 2,
        "importPath": "finalpipeline",
        "description": "finalpipeline",
        "peekOfCode": "def load_models():\n    \"\"\"\n    Load Various Models for Recommendation Engine Pipeline\n    \"\"\"\n    if len(LOADED_MODELS) == 5:\n        return True\n    else:\n        print(\"Loading Models...\")\n        try:\n            GENDER_CLASSIFIER = load_model(r\"./recommendation_assets/embedding/gender_classification_modell.h5\")        ",
        "detail": "finalpipeline",
        "documentation": {}
    },
    {
        "label": "load_CSVs",
        "kind": 2,
        "importPath": "finalpipeline",
        "description": "finalpipeline",
        "peekOfCode": "def load_CSVs():\n    \"\"\"\n    Load Various CSV files for Embedding Generation Pipeline\n    \"\"\"\n    if len(LOADED_CSVS) == 7:\n        return True\n    else:\n        print(\"Loading CSVs...\")\n        MENS_TOPWEAR_CSV = pd.read_csv(r\"./recommendation_assets/CSVS/mens_topwear_embedding.csv\")\n        MENS_BOTTOMWEAR_CSV = pd.read_csv(r\"./recommendation_assets/CSVS/mens_bottomwear_embedding.csv\")",
        "detail": "finalpipeline",
        "documentation": {}
    },
    {
        "label": "extract_clothes",
        "kind": 2,
        "importPath": "finalpipeline",
        "description": "finalpipeline",
        "peekOfCode": "def extract_clothes(image):\n    '''\n    Extract Topwear, Bottomwear, Footwear, and Bag from a given Image\n    Note: In case there are multiple items of the same class, the item with the highest confidence score is returned.\n    Args:\n        image: Path to the image file that you want to extract clothes from.\n    Returns:\n        outputs: Dict[numpy.ndarray], Dictionary containing detected Object Class as Keys and the image slice as Values. Also contains the original image.\n    '''\n    temporary_image_storage = False",
        "detail": "finalpipeline",
        "documentation": {}
    },
    {
        "label": "plot_clothes",
        "kind": 2,
        "importPath": "finalpipeline",
        "description": "finalpipeline",
        "peekOfCode": "def plot_clothes(**images):\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.axis(\"off\")\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()\ndef generate_embedding(outputs, detected_objects):",
        "detail": "finalpipeline",
        "documentation": {}
    },
    {
        "label": "generate_embedding",
        "kind": 2,
        "importPath": "finalpipeline",
        "description": "finalpipeline",
        "peekOfCode": "def generate_embedding(outputs, detected_objects):\n    \"\"\"\n    Generate Embeddings for Cropped Outputs from the Object Detection Module\n    Args:\n        outputs: Dict[] objects containing Image slices for cropped detections\n        detected_objects: List[] names of the detected objects like topwear, bottomwear and footwear\n    Returns:\n        outputs: Outputs with embeddings for each detected objects.\n    \"\"\"\n    for output in detected_objects:",
        "detail": "finalpipeline",
        "documentation": {}
    },
    {
        "label": "get_results",
        "kind": 2,
        "importPath": "finalpipeline",
        "description": "finalpipeline",
        "peekOfCode": "def get_results(outputs, gender, detected_objects):\n    \"\"\"\n    Get Similar products for a given input containing query product.\n    \"\"\"\n    dict_results = dict()\n    for output in detected_objects:\n        csv_file = gender + \"_\" + output\n        csv_file = LOADED_CSVS[csv_file].copy(deep=True)\n        query = outputs[output+\"_embedding\"]\n        dict_results[output] = __get__(csv_file, query)",
        "detail": "finalpipeline",
        "documentation": {}
    },
    {
        "label": "final",
        "kind": 2,
        "importPath": "finalpipeline",
        "description": "finalpipeline",
        "peekOfCode": "def final(images):\n    \"\"\"\n    The complete pipeline for recommending similar fashion products based on a query image.\n    \"\"\"\n    if isinstance(images, str):\n        images = [images]\n    if load_models() and load_CSVs():\n        for img_path in images:\n            inputs = preprocess_image1(img_path)\n            gender_score = LOADED_MODELS[\"gender_classifier\"](inputs)[0][0].numpy()",
        "detail": "finalpipeline",
        "documentation": {}
    },
    {
        "label": "LOADED_MODELS",
        "kind": 5,
        "importPath": "finalpipeline",
        "description": "finalpipeline",
        "peekOfCode": "LOADED_MODELS = dict()\nLOADED_CSVS = dict()\nTARGET_SHAPE = (224,224,3)\n# Commented out IPython magic to ensure Python compatibility.\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom PIL import Image\n# from detect import generate_bbox  # Import the generate_bbox function",
        "detail": "finalpipeline",
        "documentation": {}
    },
    {
        "label": "LOADED_CSVS",
        "kind": 5,
        "importPath": "finalpipeline",
        "description": "finalpipeline",
        "peekOfCode": "LOADED_CSVS = dict()\nTARGET_SHAPE = (224,224,3)\n# Commented out IPython magic to ensure Python compatibility.\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom PIL import Image\n# from detect import generate_bbox  # Import the generate_bbox function\n# %matplotlib inline",
        "detail": "finalpipeline",
        "documentation": {}
    },
    {
        "label": "TARGET_SHAPE",
        "kind": 5,
        "importPath": "finalpipeline",
        "description": "finalpipeline",
        "peekOfCode": "TARGET_SHAPE = (224,224,3)\n# Commented out IPython magic to ensure Python compatibility.\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom PIL import Image\n# from detect import generate_bbox  # Import the generate_bbox function\n# %matplotlib inline\ndef generate_bbox(img_path, conf_thres=0.5):",
        "detail": "finalpipeline",
        "documentation": {}
    },
    {
        "label": "img_path",
        "kind": 5,
        "importPath": "finalpipeline",
        "description": "finalpipeline",
        "peekOfCode": "img_path = r\"./recommendation_assets/1702073747521-105.JPG\"\nfinal(img_path)",
        "detail": "finalpipeline",
        "documentation": {}
    },
    {
        "label": "generate_bbox",
        "kind": 2,
        "importPath": "recommendation",
        "description": "recommendation",
        "peekOfCode": "def generate_bbox(img_path, conf_thres=0.5):\n    # Load model\n    weights = r'./recommendation_assets/last.pt'\n    model = torch.hub.load('ultralytics/yolov5', 'custom', path=weights)\n    # Convert to tensor and add batch dimension\n    results = model(img_path)\n    return results.xyxy[0].cpu().numpy()\n    # Return the numpy array of predictions\ndef preprocess_image1(filename):\n    \"\"\"",
        "detail": "recommendation",
        "documentation": {}
    },
    {
        "label": "preprocess_image1",
        "kind": 2,
        "importPath": "recommendation",
        "description": "recommendation",
        "peekOfCode": "def preprocess_image1(filename):\n    \"\"\"\n    Load the specified file as a JPEG image, preprocess it and\n    resize it to the target shape for Gender Classification\n    \"\"\"\n    image_string = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    image = tf.image.resize(image, TARGET_SHAPE[:2])\n    return tf.expand_dims(image, axis=0)\ndef preprocess_image2(filename):",
        "detail": "recommendation",
        "documentation": {}
    },
    {
        "label": "preprocess_image2",
        "kind": 2,
        "importPath": "recommendation",
        "description": "recommendation",
        "peekOfCode": "def preprocess_image2(filename):\n    \"\"\"\n    Load the specified file as a JPEG image, preprocess it and\n    resize it to the target shape for Embedding Generation\n    \"\"\"\n    image_string = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, TARGET_SHAPE[:2])\n    image = resnet.preprocess_input(image)",
        "detail": "recommendation",
        "documentation": {}
    },
    {
        "label": "load_models",
        "kind": 2,
        "importPath": "recommendation",
        "description": "recommendation",
        "peekOfCode": "def load_models():\n    \"\"\"\n    Load Various Models for Recommendation Engine Pipeline\n    \"\"\"\n    if len(LOADED_MODELS) == 5:\n        return True\n    else:\n        print(\"Loading Models...\")\n        try:\n            GENDER_CLASSIFIER = load_model(r\"./recommendation_assets/embedding/gender_classification_modell.h5\")        ",
        "detail": "recommendation",
        "documentation": {}
    },
    {
        "label": "load_CSVs",
        "kind": 2,
        "importPath": "recommendation",
        "description": "recommendation",
        "peekOfCode": "def load_CSVs():\n    \"\"\"\n    Load Various CSV files for Embedding Generation Pipeline\n    \"\"\"\n    if len(LOADED_CSVS) == 7:\n        return True\n    else:\n        print(\"Loading CSVs...\")\n        MENS_TOPWEAR_CSV = pd.read_csv(r\"./recommendation_assets/CSVS/mens_topwear_embedding.csv\")\n        MENS_BOTTOMWEAR_CSV = pd.read_csv(r\"./recommendation_assets/mens_bottomwear_embedding.csv\")",
        "detail": "recommendation",
        "documentation": {}
    },
    {
        "label": "extract_clothes",
        "kind": 2,
        "importPath": "recommendation",
        "description": "recommendation",
        "peekOfCode": "def extract_clothes(image):\n    '''\n    Extract Topwear, Bottomwear, Footwear, and Bag from a given Image\n    Note: In case there are multiple items of the same class, the item with the highest confidence score is returned.\n    Args:\n        image: Path to the image file that you want to extract clothes from.\n    Returns:\n        outputs: Dict[numpy.ndarray], Dictionary containing detected Object Class as Keys and the image slice as Values. Also contains the original image.\n    '''\n    temporary_image_storage = False",
        "detail": "recommendation",
        "documentation": {}
    },
    {
        "label": "plot_clothes",
        "kind": 2,
        "importPath": "recommendation",
        "description": "recommendation",
        "peekOfCode": "def plot_clothes(**images):\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.axis(\"off\")\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()\ndef generate_embedding(outputs, detected_objects):",
        "detail": "recommendation",
        "documentation": {}
    },
    {
        "label": "generate_embedding",
        "kind": 2,
        "importPath": "recommendation",
        "description": "recommendation",
        "peekOfCode": "def generate_embedding(outputs, detected_objects):\n    \"\"\"\n    Generate Embeddings for Cropped Outputs from the Object Detection Module\n    Args:\n        outputs: Dict[] objects containing Image slices for cropped detections\n        detected_objects: List[] names of the detected objects like topwear, bottomwear and footwear\n    Returns:\n        outputs: Outputs with embeddings for each detected objects.\n    \"\"\"\n    for output in detected_objects:",
        "detail": "recommendation",
        "documentation": {}
    },
    {
        "label": "get_results",
        "kind": 2,
        "importPath": "recommendation",
        "description": "recommendation",
        "peekOfCode": "def get_results(outputs, gender, detected_objects):\n    \"\"\"\n    Get Similar products for a given input containing query product.\n    \"\"\"\n    dict_results = dict()\n    for output in detected_objects:\n        csv_file = gender + \"_\" + output\n        csv_file = LOADED_CSVS[csv_file].copy(deep=True)\n        query = outputs[output+\"_embedding\"]\n        dict_results[output] = __get__(csv_file, query)",
        "detail": "recommendation",
        "documentation": {}
    },
    {
        "label": "final",
        "kind": 2,
        "importPath": "recommendation",
        "description": "recommendation",
        "peekOfCode": "def final(images):\n    \"\"\"\n    The complete pipeline for recommending similar fashion products based on a query image.\n    \"\"\"\n    if isinstance(images, str):\n        images = [images]\n    if load_models() and load_CSVs():\n        for img_path in images:\n            inputs = preprocess_image1(img_path)\n            gender_score = LOADED_MODELS[\"gender_classifier\"](inputs)[0][0].numpy()",
        "detail": "recommendation",
        "documentation": {}
    },
    {
        "label": "LOADED_MODELS",
        "kind": 5,
        "importPath": "recommendation",
        "description": "recommendation",
        "peekOfCode": "LOADED_MODELS = dict()\nLOADED_CSVS = dict()\nTARGET_SHAPE = (224,224,3)\n# cell#3\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom PIL import Image\n# from detect import generate_bbox  # Import the generate_bbox function",
        "detail": "recommendation",
        "documentation": {}
    },
    {
        "label": "LOADED_CSVS",
        "kind": 5,
        "importPath": "recommendation",
        "description": "recommendation",
        "peekOfCode": "LOADED_CSVS = dict()\nTARGET_SHAPE = (224,224,3)\n# cell#3\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom PIL import Image\n# from detect import generate_bbox  # Import the generate_bbox function\n# %matplotlib inline",
        "detail": "recommendation",
        "documentation": {}
    },
    {
        "label": "TARGET_SHAPE",
        "kind": 5,
        "importPath": "recommendation",
        "description": "recommendation",
        "peekOfCode": "TARGET_SHAPE = (224,224,3)\n# cell#3\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom PIL import Image\n# from detect import generate_bbox  # Import the generate_bbox function\n# %matplotlib inline\ndef generate_bbox(img_path, conf_thres=0.5):",
        "detail": "recommendation",
        "documentation": {}
    },
    {
        "label": "img_path",
        "kind": 5,
        "importPath": "recommendation",
        "description": "recommendation",
        "peekOfCode": "img_path = r\"./recommendation_assets/1702073747521-105.JPG\"\nfinal(img_path)",
        "detail": "recommendation",
        "documentation": {}
    }
]